title         | ADAPTIVE MODEL REDUCTION TO ACCELERATE OPTIMIZATION
              | PROBLEMS GOVERNED BY PARTIAL DIFFERENTIAL EQUATIONS
blank         | 
              | 
              | 
              | 
text          |                    A DISSERTATION
              |            SUBMITTED TO THE INSTITUTE FOR
              |    COMPUTATIONAL AND MATHEMATICAL ENGINEERING
              |       AND THE COMMITTEE ON GRADUATE STUDIES
              |               OF STANFORD UNIVERSITY
              |      IN PARTIAL FULFILLMENT OF THE REQUIREMENTS
              |                  FOR THE DEGREE OF
              |                DOCTOR OF PHILOSOPHY
blank         | 
              | 
              | 
              | 
text          |                    Matthew Joseph Zahr
              |                        August 2016
              |                 ¬© 2016 by Matthew Joseph Zahr. All Rights Reserved.
              |          Re-distributed by Stanford University under license with the author.
blank         | 
              | 
              | 
text          |                         This work is licensed under a Creative Commons Attribution-
              |                         Noncommercial 3.0 United States License.
              |                         http://creativecommons.org/licenses/by-nc/3.0/us/
blank         | 
              | 
              | 
              | 
text          | This dissertation is online at: http://purl.stanford.edu/bh989ww6442
blank         | 
              | 
              | 
              | 
meta          |                                           ii
text          | I certify that I have read this dissertation and that, in my opinion, it is fully adequate
              | in scope and quality as a dissertation for the degree of Doctor of Philosophy.
blank         | 
text          |                                                           Charbel Farhat, Primary Adviser
blank         | 
              | 
              | 
text          | I certify that I have read this dissertation and that, in my opinion, it is fully adequate
              | in scope and quality as a dissertation for the degree of Doctor of Philosophy.
blank         | 
text          |                                                                             Michael Saunders
blank         | 
              | 
              | 
text          | I certify that I have read this dissertation and that, in my opinion, it is fully adequate
              | in scope and quality as a dissertation for the degree of Doctor of Philosophy.
blank         | 
text          |                                                                              Per-Olof Persson
blank         | 
              | 
              | 
              | 
text          | Approved for the Stanford University Committee on Graduate Studies.
              |                              Patricia J. Gumport, Vice Provost for Graduate Education
blank         | 
              | 
              | 
              | 
text          | This signature page was generated electronically upon submission of this dissertation in
              | electronic format. An original signed hard copy of the signature page is on file in
              | University Archives.
blank         | 
              | 
              | 
              | 
meta          |                                           iii
title         | Abstract
blank         | 
text          | Optimization problems constrained by Partial Differential Equations (PDEs) are ubiquitous in mod-
              | ern science and engineering. They play a central role in optimal design and control of multiphysics
              | systems, as well as nondestructive evaluation and detection, and inverse problems. Methods to
              | solve these optimization problems rely on potentially many numerical solutions of the underlying
              | equations. For complicated physical interactions taking place on complex domains, these solutions
              | will be computationally expensive‚Äîin terms of both time and resources‚Äîto obtain, rendering the
              | optimization procedure difficult or intractable.
              |    This dissertation introduces a globally convergent, error-aware trust region algorithm for lever-
              | aging inexpensive approximation models to greatly reduce the cost of solving PDE-constrained
              | optimization problems in increasingly complex scenarios. While the trust region theory is general,
              | in that it is agnostic to the particular form of the approximation model, provided it possesses certain
              | properties, this work employs reduced-order models based on the method of snapshots and Proper
              | Orthogonal Decomposition (POD). The trust region algorithm proceeds by progressively refining
              | the fidelity of the reduced-order model while converging to the optimal solution. Thus, the reduced-
              | order model is trained exactly along the optimization trajectory, circumventing the task of training
              | in a potentially high-dimensional parameter space. The proposed method is shown to find the opti-
              | mal aerodynamic shape of a full aircraft configuration in about half the time required by accepted
              | methods.
              |    The proposed error-aware trust region algorithm is extended to handle the case where uncertain-
              | ties are present in the governing equations. In such situations, the goal is to find a design or control
              | that is risk-averse with respect to some quantity of interest. The objective function and constraints
              | in these problems usually correspond to integrals of quantities of interest over the stochastic space,
              | which will inevitably require many solutions of the underlying partial differential equation. For
              | this reason, dimension-adaptive sparse grids are combined with reduced-order models to define an
              | inexpensive approximation model, which is wrapped in the error-aware trust region framework to
              | ensure convergence to the optimal risk-averse solution. This framework is demonstrated on a model
              | problem from computational mechanics and shown to be several orders of magnitude faster than
              | existing methods.
blank         | 
              | 
              | 
              | 
meta          |                                                    iv
title         | Acknowledgments
blank         | 
text          | First and foremost, I would like to thank my advisor, Professor Charbel Farhat, for his advice
              | and guidance during the past five years. From the time you recruited me out of the AHPCRC
              | Summer Institute as an undergraduate through the completion of my PhD and job search, you
              | have been a professional role model and trusted mentor. I am also grateful for the perfect balance
              | between independence and supervision that you have provided as it has given me the opportunity
              | to explore other areas of computational mathematics and form external collaborations. I am very
              | proud to be able to call myself your student. I also want to thank Professor Per-Olof Persson ‚Äì
              | my co-author, practicum advisor, thesis reader, coding buddy, and guide into the DG world ‚Äì for
              | serving so many voluntary roles throughout my PhD. I would also like to thank Professor Sanjay
              | Govindjee and Professor Tarek Zohdi ‚Äì my trusted mentors since my undergraduate days at UC
              | Berkeley ‚Äì who have provided crucial support and guidance throughout my PhD and job search. I
              | am also very grateful to rest of my thesis committee ‚Äì Professor Michael Saunders, Professor Walter
              | Murray, and Professor Louis Durlofsky ‚Äì and my main source of funding ‚Äì the Department of Energy
              | Computational Science Graduate Fellowship (DOE CSGF).
              |    I have had the pleasure to be in research lab with many talented and entertaining individuals.
              | I am very thankful to my FRG predecessor Kevin Carlberg who provided invaluable guidance and
              | assistance while I was an AHPCRC undergraduate intern and new graduate student, and remains a
              | trusted mentor and close collaborator to this day. I also want to sincerely thank my FRG labmates
              | ‚Äì Dr Kyle Washabaugh, Dr Alex Main, Todd Chapman, and Raunak Borker ‚Äì that made Durand
              | 028 not only an intellectually stimulating place, but also an entertaining and fun one; I will always
              | consider you among my closest friends and collaborators. Finally, I would like to acknowledge the
              | constant support and assistance I received from a number of other FRG-ers: Grace Fontanilla,
              | Tatiana Wilson, William Law, and Dr Philip Avery.
              |    Finally, I would like to thank my family and friends for their love, support, and patience, without
              | which, none of this would be possible. To my sweet dove and soon-to-be wife, Theresa Yates. To my
              | dad, Michael J. Zahr: my closest friend, most trusted mentor, and eternal role model. To my mom,
              | Tamara Bradley: the sweetest, most caring and supportive mother imaginable. To my grandpa and
              | grandma, Robert and Marlene Boranian: you are a constant source of love, encouragement, and sup-
              | port in life and the earliest investors in my education. To my sister, Emily Bradley, and stepfather,
              | Robert Bradley: you are, and always will be, a constant source of enjoyment and entertainment in
blank         | 
              | 
              | 
meta          |                                                   v
text          | my life. To my ski and camping buddies, Michael Gardner and Devon Laduzinsky. To my beloved
              | late uncle John ‚ÄúJack‚Äù Zahr: you are a shining example of success in all phases of life ‚Äì you will
              | forever be missed and remembered. To my second mother Sharee Eisenga; my soon-to-be in-laws,
              | Michael, Christine, and Rebecca Yates; and the entire Zahr, Hoffmann, and Bradley family.
blank         | 
text          | I dedicate this thesis to my future wife, Theresa Yates; parents, Michael Zahr and Tamara Bradley;
              |  grandparents, Robert and Marlene Boranian; step-father, Robert Bradley; sister, Emily Bradley;
              |                               and my late uncle, John ‚ÄúJack‚Äù Zahr
blank         | 
              | 
              | 
              | 
meta          |                                                 vi
title         | Contents
blank         | 
text          | Abstract                                                                                                  iv
blank         | 
text          | Acknowledgments                                                                                           v
blank         | 
text          | 1 Introduction                                                                                             1
              |   1.1   Motivation    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    1
              |   1.2   Strategy and Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        3
              |   1.3   Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        3
              |         1.3.1   PDE-Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . .           3
              |         1.3.2   Trust Region Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .         5
              |         1.3.3   Projection-Based Model Reduction . . . . . . . . . . . . . . . . . . . . . . . .           8
              |         1.3.4   Surrogate Methods for PDE-Constrained Optimization . . . . . . . . . . . . .               9
              |   1.4   Thesis Accomplishments and Outline . . . . . . . . . . . . . . . . . . . . . . . . . . .          13
blank         | 
text          | 2 PDE-Constrained Optimization                                                                            16
              |   2.1   Parametrized Partial Differential Equations . . . . . . . . . . . . . . . . . . . . . . .         16
              |         2.1.1   Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      17
              |         2.1.2   Discretization: Parametrization . . . . . . . . . . . . . . . . . . . . . . . . . .       22
              |         2.1.3   Discretization: Governing Equations . . . . . . . . . . . . . . . . . . . . . . .         26
              |         2.1.4   Discretization: Quantities of Interest . . . . . . . . . . . . . . . . . . . . . . .      32
              |   2.2   Parametrized Stochastic Partial Differential Equations . . . . . . . . . . . . . . . . .          34
              |         2.2.1   Risk Measures of Quantities of Interest       . . . . . . . . . . . . . . . . . . . . .   35
              |         2.2.2   Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      36
              |         2.2.3   Finite-Dimensional Approximation . . . . . . . . . . . . . . . . . . . . . . . .          37
              |   2.3   PDE-Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          38
              |         2.3.1   Continuous vs. Discrete Formulation . . . . . . . . . . . . . . . . . . . . . . .         40
              |         2.3.2   Full Space vs. Reduced Space Approach . . . . . . . . . . . . . . . . . . . . .           43
              |         2.3.3   Sensitivity Method for Computing Gradients . . . . . . . . . . . . . . . . . .            44
              |         2.3.4   Adjoint Method for Computing Gradients . . . . . . . . . . . . . . . . . . . .            45
              |         2.3.5   Optimization Problems with Side Constraints . . . . . . . . . . . . . . . . . .           48
blank         | 
              | 
              | 
meta          |                                                    vii
text          | 3 Generalized Multifidelity Trust Region Method                                                        50
              |   3.1   Unconstrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      51
              |         3.1.1   Error-Aware Multifidelity Trust Region Method . . . . . . . . . . . . . . . . .         51
              |         3.1.2   Interior-Point Method for Trust Region Subproblem . . . . . . . . . . . . . .           62
              |         3.1.3   Numerical Experiment: Contrived . . . . . . . . . . . . . . . . . . . . . . . .         63
              |   3.2   Nonlinearly Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . .        70
              |         3.2.1   Error-Aware Augmented Lagrangian Multifidelity Trust Region Method . . .                71
              |         3.2.2   Numerical Experiment: Contrived . . . . . . . . . . . . . . . . . . . . . . . .         72
blank         | 
text          | 4 Projection-Based Model Reduction                                                                     77
              |   4.1   Global Reduced-Order Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       78
              |         4.1.1   Primal Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      78
              |         4.1.2   Exact and Minimum-Residual Sensitivity Formulation . . . . . . . . . . . . .            82
              |         4.1.3   Exact and Minimum-Residual Adjoint Formulation . . . . . . . . . . . . . . .            87
              |   4.2   Global Hyperreduced Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        92
              |         4.2.1   Precomputation for Polynomial Nonlinearities . . . . . . . . . . . . . . . . . .        93
              |         4.2.2   Mask and Sample Mesh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        95
              |         4.2.3   Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    96
              |         4.2.4   Minimum-Residual Primal Formulation . . . . . . . . . . . . . . . . . . . . .           98
              |         4.2.5   Exact and Minimum-Residual Sensitivity Formulation . . . . . . . . . . . . . 100
              |         4.2.6   Adjoint Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
              |   4.3   Construction of Reduced-Order Basis and Residual Mask . . . . . . . . . . . . . . . 106
              |   4.4   Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
blank         | 
text          | 5 Optimization via Model Reduction and Residual-Based Trust Regions                                    116
              |   5.1   Residual-Based Trust Region Method . . . . . . . . . . . . . . . . . . . . . . . . . . 117
              |         5.1.1   Multifidelity Trust Region Ingredients . . . . . . . . . . . . . . . . . . . . . . 117
              |         5.1.2   Basis Construction via Proper Orthogonal Decomposition and the Method of
              |                 Snapshots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
              |   5.2   Snapshots from Partially Converged Solutions . . . . . . . . . . . . . . . . . . . . . . 128
              |   5.3   Efficient Trust Region Assessment with Partially Converged Solutions . . . . . . . . 130
              |   5.4   Extension to Hyperreduced Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
              |   5.5   Numerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
              |         5.5.1   Optimal Control of 1D Inviscid Burgers‚Äô Equation . . . . . . . . . . . . . . . 135
              |         5.5.2   Optimal Control of 1D Viscous Burgers‚Äô Equation . . . . . . . . . . . . . . . 144
              |         5.5.3   Shape Optimization of Airfoil in Inviscid, Subsonic Flow . . . . . . . . . . . . 150
              |         5.5.4   Shape Optimization of the Common Research Model in Viscous, Turbulent Flow158
blank         | 
              | 
              | 
              | 
meta          |                                                   viii
text          | 6 Model Reduction and Sparse Grids for Efficient Stochastic Optimization                             170
              |   6.1   Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
              |         6.1.1   Stochastic High-Dimensional Model . . . . . . . . . . . . . . . . . . . . . . . 171
              |         6.1.2   Stochastic Reduced-Order Model . . . . . . . . . . . . . . . . . . . . . . . . . 173
              |         6.1.3   Anisotropic Sparse Grids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
              |   6.2   Two Levels of Approximation of Risk-Averse Measures . . . . . . . . . . . . . . . . . 180
              |   6.3   Multifidelity Trust Region Method Based on Two-Level Approximation . . . . . . . 183
              |         6.3.1   Trust Region Ingredients     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
              |         6.3.2   Greedy Construction of Sparse Grid and Reduced Basis . . . . . . . . . . . . 187
              |         6.3.3   Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
              |   6.4   Numerical Experiment: Optimal Control of the Viscous Burgers‚Äô Equation with Un-
              |         certain Coefficients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
blank         | 
text          | 7 Conclusions                                                                                        205
              |   7.1   Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
              |   7.2   Prospective Future Work      . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
blank         | 
text          | A Global Convergence Proof: Error-Aware Trust Region Method                                          209
blank         | 
text          | B Residual-Based Error Bounds                                                                        215
blank         | 
text          | C Adaptive State and Parameter Space Reduction for Large-Scale Optimization 224
              |   C.1 Two-Level Nested Reduction of Parametrized Partial Differential Equations . . . . . 226
              |         C.1.1 Outer Layer of Reduction: Restriction of Parameter Space . . . . . . . . . . . 226
              |         C.1.2 Inner Layer of Reduction: Projection-Based Model Reduction . . . . . . . . . 229
              |   C.2 Globally Convergent Multifidelity Trust Region Method . . . . . . . . . . . . . . . . 231
              |         C.2.1 Outer Iteration: Globally Convergent Parameter Space Adaptation . . . . . . 231
              |         C.2.2 Inner Iteration: Multifidelity Optimization with Reduced-Order Models . . . 234
blank         | 
text          | D Time-Dependent PDE-Constrained Optimization under Periodicity Constraints240
              |   D.1 Governing Equations and Discretization . . . . . . . . . . . . . . . . . . . . . . . . . 240
              |         D.1.1 System of Conservation Laws on Deforming Domain: Arbitrary Lagrangian-
              |                 Eulerian Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
              |         D.1.2 Arbitrary Lagrangian-Eulerian Discontinuous Galerkin Method . . . . . . . . 242
              |   D.2 Fully Discrete, Time-Dependent Adjoint Equations . . . . . . . . . . . . . . . . . . . 245
              |         D.2.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
              |         D.2.2 Parametrization of Initial Condition . . . . . . . . . . . . . . . . . . . . . . . 248
              |         D.2.3 Benefits of Fully Discrete Framework . . . . . . . . . . . . . . . . . . . . . . . 249
              |         D.2.4 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
              |         D.2.5 Numerical Experiment: Energetically Optimal Trajectory of 2D Airfoil in
              |                 Compressible, Viscous Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
blank         | 
              | 
meta          |                                                    ix
text          |        D.2.6 Numerical Experiment: Energetically Optimal Shape and Flapping Motion of
              |                2D Airfoil at Constant Impulse . . . . . . . . . . . . . . . . . . . . . . . . . . 263
              |   D.3 Computing Time-Periodic Solutions of Partial Differential Equations . . . . . . . . . 272
              |        D.3.1 Numerical Solvers: Shooting Methods . . . . . . . . . . . . . . . . . . . . . . 273
              |        D.3.2 Stability of Periodic Orbits of Fully Discrete Partial Differential Equations . 276
              |   D.4 Fully Discrete, Time-Periodic Adjoint Method . . . . . . . . . . . . . . . . . . . . . . 278
              |        D.4.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
              |        D.4.2 Numerical Solver: Matrix-Free Krylov Method . . . . . . . . . . . . . . . . . 280
              |        D.4.3 Generalized Reduced-Gradient Method for PDE Optimization with Time-
              |                Periodicity Constraints    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
              |        D.4.4 Numerical Experiment: Time-Periodic Solutions of the Compressible Navier-
              |                Stokes Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
              |        D.4.5 Numerical Experiment: Energetically Optimal Flapping with Thrust and Time-
              |                Periodicity Constraints    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
              |   D.5 Conclusion     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
              |   D.6 Existence and Uniqueness of Solutions of the Adjoint Equations of the Fully Discrete,
              |        Time-Periodically Constrained Partial Differential Equations . . . . . . . . . . . . . 299
blank         | 
text          | Bibliography                                                                                         301
blank         | 
              | 
              | 
              | 
meta          |                                                   x
title         | List of Tables
blank         | 
text          |  2.1   Butcher Tableau for s-stage diagonally implicit Runge-Kutta scheme . . . . . . . . .                 31
blank         | 
text          |  3.1   Convergence history of Algorithm 1 applied to the Rosenbrock problem. . . . . . . .                  69
              |  3.2   Convergence history of Algorithm 1 applied to the constrained problem (3.49). Iter-
              |        ations 0 ‚àí 9: œÑ0 = 10‚àí4 , iterations 10 ‚àí 19: œÑ1 = 10‚àí5 , iterations 20 ‚àí 29: œÑ2 = 10‚àí6 .
              |        The norm of the gradient of LœÑj (¬µ), for fixed œÑj , decreases 3 ‚àí 4 orders of magnitude
              |        throughout the iterations despite the values of LœÑj (¬µk ) and mk (¬µk ) or LœÑj (¬µÃÇk ) and
              |        mk (¬µÃÇk ) not being close until near convergence. . . . . . . . . . . . . . . . . . . . . .          76
blank         | 
text          |  5.1   Variants of the multifidelity trust region method based on projection-based reduced-
              |        order models introduced in Algorithms 11 and 12. The first three methods are not
              |        guaranteed to be globally convergent since they do not necessarily satisfy the gradient
              |        condition (3.15). The methods that employ the traditional trust region employ two
              |        trust region subproblem solvers: an exact solver based on the interior point method
              |        in Algorithm 3 and the inexact Steihaug-Toint CG solver. The methods that employ
              |        the residual-based trust region rely on the exact interior point solver in Algorithm 3.
              |        The interior point solver considered in this section uses Newton-CG to solve the un-
              |        constrained subproblem (instead of BFGS) for fair comparison with the second-order
              |        Steihaug-Toint CG. The snapshot matrices Uk , Wk , Zk consist of state, sensitiv-
              |        ity, and adjoint snapshots, respectively, of the high-dimensional model at all previous
              |        trust region centers, i.e., ¬µ0 , . . . , ¬µk‚àí1 .    . . . . . . . . . . . . . . . . . . . . . . . . . 141
              |  5.2   Convergence history of Algorithm 11 applied to optimal control of the inviscid Burg-
              |        ers‚Äô equation using method ‚Äòsens-etr-intpt‚Äô using reduced-order models based on a
              |        Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
              |  5.3   Convergence history of Algorithm 12 applied to optimal control of the inviscid Burg-
              |        ers‚Äô equation using method ‚Äòsens-etr-intpt‚Äô using reduced-order models based on a
              |        Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
              |  5.4   Convergence history of Algorithm 11 applied to optimal control of the inviscid Burg-
              |        ers‚Äô equation using method ‚Äòsens-ctr-stcg‚Äô using reduced-order models based on a
              |        Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
blank         | 
              | 
              | 
meta          |                                                          xi
text          | 5.5   Convergence history of Algorithm 12 applied to optimal control of the inviscid Burg-
              |       ers‚Äô equation using method ‚Äòsens-ctr-stcg‚Äô using reduced-order models based on a
              |       Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
              | 5.6   Convergence history of Algorithm 11 applied to optimal control of the viscous Burg-
              |       ers‚Äô equation using method ‚Äòdual-etr-intpt‚Äô using reduced-order models based on a
              |       Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
              | 5.7   Convergence history of Algorithm 11 applied to optimal control of the viscous Burg-
              |       ers‚Äô equation using method ‚Äòdual-ctr-intpt‚Äô using reduced-order models based on a
              |       Galerkin projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
              | 5.8   Performance of the HDM- and ROM-based optimization methods. . . . . . . . . . . 169
blank         | 
text          | 6.1   Convergence history of Algorithm 15 applied to the optimal control of the stochastic
              |       Burgers‚Äô equation in (6.67). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
              | 6.2   Convergence history of Algorithm 16 applied to the optimal control of the stochastic
              |       Burgers‚Äô equation in (6.67). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
blank         | 
text          | D.1 Butcher Tableau for 3-stage, 3rd order DIRK scheme [3]
              |                                          2
              |                                                              6Œ±2 ‚àí20Œ±+5
              |       Œ± = 0.435866521508459, Œ≥ = ‚àí 6Œ±        ‚àí16Œ±+1
              |                                               4     ,   œâ=        4     .   . . . . . . . . . . . . . . . 254
              | D.2 Summary of parametrizations considered in Section D.2.5. The number of clamped
              |       cubic spline knots used to discretize x(t), y(t), and Œ∏(t) are mx + 1, my + 1, and
              |       mŒ∏ , respectively. PI freezes the rigid body translation (mx = my = 0) and optimizes
              |       over only the rotation (mŒ∏ 6= 0). PII optimizes over all rigid body degrees of freedom
              |       (mx = my = mŒ∏ 6= 0). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
              | D.3 Table summarizing integrated quantities of interest at optimal solution of (D.44) for
              |       each parametrization (PI, PII) for each level of refinement. The total work monoton-
              |       ically increases as N¬µ increases for a given parametrization, which is expected due to
              |       the nested search spaces. For a fixed ID, the optimal total work for parametrization
              |       PII is larger than that for PI since the search space for PI is a subset of that of
              |       PII. The other integrated quantities are included for completeness, but do not exhibit
              |       trends (except for converging to a fixed value as N¬µ increases) since they were not
              |       included in the optimization problem. . . . . . . . . . . . . . . . . . . . . . . . . . . 261
              | D.4 Summary of parametrizations considered in Section D.2.6. The number of periodic
              |       cubic spline knots used to discretize y(t), Œ∏(t), and ¬∏(t) are my + 1, mŒ∏ + 1, and
              |       mc +1, respectively. FI freezes the airfoil shape and considers only rigid body motions
              |       (my = mŒ∏ 6= 0, mc = 0). FII parametrizes both shape and kinematic motion (my =
              |       mŒ∏ = mc 6= 0). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
blank         | 
              | 
              | 
              | 
meta          |                                                   xii
text          | D.5 Table summarizing integrated quantities of interest at optimal solution of each op-
              |      timization problem for each impulse level. In all cases, the desired value of Jx is
              |      achieved to greater than 4 digits of accuracy. The optimal solution for larger values
              |      of the impulse constraint require more total work to complete flapping motion, i.e.,
              |      work monotonically increases in magnitude as value of impulse constraint increases.
              |      Smaller values of total work are achievable if airfoil is allowed to morph its shape
              |      in addition its rigid body motion. The other integrated quantities are included for
              |      completeness, but do not exhibit trends since they were not in the optimization problem.268
              | D.6 Table summarizing performance of numerical solvers for fully discrete time-periodic
              |      partial differential equations, considering nonlinear preconditioning via m fixed point
              |      iterations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
              | D.7 Comparison of non-zero derivatives of total energy, W , and x-impulse, Jx , computed
              |      with the adjoint method and a second-order finite difference approximation with step
              |      size œÑ = 10‚àí6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
blank         | 
              | 
              | 
              | 
meta          |                                                 xiii
title         | List of Figures
blank         | 
text          |  1.1   The adaptive approach to accelerate PDE-constrained optimization with projection-
              |        based reduced-order models. Top left: block schematic of the workflow where few
              |        High-Dimensional Model (HDM) samples are compressed to build the Reduced-Order
              |        Basis (ROB) and the resulting Reduced-Order Model (ROM) is used in the optimiza-
              |        tion procedure, as long as it maintains accuracy. When the accuracy degrades, an
              |        additional sample of the HDM is taken at the new point in the parameter space and
              |        the ROB is enriched. Top right: schematic of parameter space (¬µ-space) where the
              |        black dot and star are the initial guess and solution of the optimization problem,
              |        respectively, the red circles indicate HDM samples, the gray regions are the ‚Äútrust
              |        regions‚Äù for the ROM constructed at each iteration, the blue line is the trajectory
              |        of the ROM optimization procedure, and the blue star is the optimal solution found
              |        by the ROM optimization. Bottom: schematic of the computational cost where the
              |        expensive (HDM evaluations and ROB construction) and inexpensive components
              |        are intermixed throughout the algorithm. These methods are usually equipped with
              |        global convergence theory that guarantee convergence to a local optimum of the PDE-
              |        constrained optimization problem, as indicated in the top right plot. . . . . . . . . .   11
blank         | 
              | 
              | 
              | 
meta          |                                                xiv
text          | 1.2   The offline-online approach to accelerate PDE-constrained optimization with projection-
              |       based reduced-order models. Top left: block schematic of the workflow where a
              |       number of High-Dimensional Model (HDM) samples are compressed to build the
              |       Reduced-Order Basis (ROB) in an offline phase; the resulting inexpensive Reduced-
              |       Order Model (ROM) is repeatedly queried in the online optimization phase. Top
              |       right: schematic of parameter space (¬µ-space) where the black dot and star are the
              |       initial guess and solution of the optimization problem, respectively, the red circles
              |       indicate HDM samples, the blue line is the trajectory of the ROM optimization pro-
              |       cedure, and the blue star is the optimal solution found by the ROM optimization.
              |       Bottom: schematic of the computational cost where there is a clear distinction be-
              |       tween the expensive components (HDM evaluations and ROB construction) that are
              |       done once-and-for-all in the offline phase and the inexpensive components (ROM eval-
              |       uations) that are repeatedly queried in the online phase. In general, these methods are
              |       not guaranteed to converge to a local optimum of the PDE-constrained optimization
              |       problem, as indicated in the top right plot. . . . . . . . . . . . . . . . . . . . . . . .       12
              | 1.3   Organization of thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     15
blank         | 
text          | 2.1   Left: Undeformed NACA0012 airfoil and surrounding triangular mesh. Right: Defor-
              |       mation of R2 according to mapping œï in (2.33) that deforms the NACA0012 geometry
              |       and surrounding mesh. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      25
              | 2.2   Top left: Undeformed geometry of a circle (blue) and a FFD lattice (gray). Top
              |       center : Perturbation of FFD control nodes according to an x-directed elongation
              |       mode and resulting shape of the circle. Top right: Perturbation of FFD control
              |       nodes according to a bending mode and resulting shape of the circle. Bottom: Local
              |       perturbations to individual FFD control nodes in the y direction and the resulting
              |       shape of the circle. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   26
              | 2.3   Free form deformation lattices and Volkswagen Passat geometry: (left) undeformed
              |       configuration, (top right) deformed configuration with lowered roof, and (bottom right)
              |       deformed configuration with steeply tapered trunk. . . . . . . . . . . . . . . . . . . .         27
              | 2.4   Free form deformation lattice and Common Research Model (CRM) geometry: (left)
              |       undeformed configuration and (right) deformed configuration with positive dihedral.              27
              | 2.5   Shape parametrization of a NACA0012 airfoil using a cubic design element. Blue
              |       nodes and lines designate the undeformed design element and shape and black nodes
              |       and lines designate the deformed design element and shape. . . . . . . . . . . . . . .           28
              |                                                       2
              | 2.6   Left: Quadrilateral mesh of a subset of R corresponding to a rectangle (160 √ó 100
              |       elements) whose topology is parametrized by a density-based method. Right: An
              |       example of an admissible topology of the density-based topological parametrization‚Äî
              |       an optimized cantilever designed to maximize the global stiffness of the structure
              |       under a vertical load at the right end. . . . . . . . . . . . . . . . . . . . . . . . . . .      29
blank         | 
              | 
              | 
meta          |                                                  xv
text          | 2.7   Left: Quadrilateral mesh of a subset of R2 corresponding to a rectangle (160√ó100 ele-
              |       ments) with a hole whose topology is parametrized by a density-based method. Right:
              |       An example of an admissible topology of the density-based topological parametrization‚Äî
              |       a Michell structure [37, 94] designed to maximize the global stiffness of the structure
              |       under a vertical load at the right end. . . . . . . . . . . . . . . . . . . . . . . . . . .         29
              |                                                  3
              | 2.8   Left: Hexahedral mesh of a subset of R corresponding to a cube (35√ó35√ó35 elements)
              |       whose topology is parametrized by a density-based method. Right: An example of
              |       an admissible topology of the density-based topological parametrization‚Äîa trestle
              |       designed to maximize the global stiffness of the structure under a vertical load. . . .             29
              | 2.9   Left: Tetrahedral mesh of a subset of R3 corresponding to an unoptimized lacrosse
              |       head (475, 666 elements) whose topology is parametrized by a density-based method.
              |       Right: An example of an admissible topology of the density-based topological parametrization‚Äî
              |       an unconverged maximum stiffness topology. The entire object is included in the top
              |       row and the bottom row is a slice to show internal voids in the optimized shape. . . .              30
blank         | 
text          | 3.1   Geometry of trust region constraint in special case where œëk = kAk (¬µ ‚àí ¬µk )k2 =
              |       k¬µ ‚àí ¬µk kAT Ak . The eigenvalue decomposition of ATk Ak is ATk Ak = Qk Œõk QTk with
              |                    k
blank         | 
text          |       eigenvectors qi = Qk ei and eigenvalues Œªi = eTi Œõk ei . . . . . . . . . . . . . . . . . . .        53
              |                                                                               4     3
              | 3.2   Logarithmic barrier function (3.27) corresponding to mk (x) = x ‚àí x (                ), œëk (x) =
              |        2
              |       x , ‚àÜk = 1 with Œ≥ = 0.1 (        ) and Œ≥ = 0.0001 (        ).   . . . . . . . . . . . . . . . . .   62
              | 3.3   Trajectory of Algorithm 1 as applied to the Rosenbrock problem (3.33). The contours
              |       represent the true function F (¬µ), the red dots indicate trust region centers, and the
              |       blue line is the trajectory of the trust region subproblem. . . . . . . . . . . . . . . .           66
              | 3.4   Trajectory of Algorithm 1 as applied to the Rosenbrock problem (3.33); iterations
              |       proceed from left to right then top to bottom. The contours represent the true
              |       function F (¬µ), the red dots indicate trust region centers ¬µk , the blue dots are the
              |       candidate for the next trust region center ¬µÃÇk , and the green region indicates the
              |       feasible set for the trust region subproblem. . . . . . . . . . . . . . . . . . . . . . . .         67
              | 3.5   Convergence history of the objective quantities using Algorithm 1: F (¬µk ) (                  ),
              |       F (¬µÃÇk ) (       ), mk (¬µk ) (   ), mk (¬µÃÇk ) (      ). Steady progress is made toward the
              |       optimal solution, despite the objective and model only agreeing at iteration 0. . . . .             68
              | 3.6   Convergence history of gradient quantities using Algorithm 1: k‚àáF (¬µk )k (                    ),
              |       k‚àáF (¬µÃÇk )k (        ), k‚àámk (¬µk )k (     ). The gradient of the true objective function
              |       decreases 6 orders of magnitude. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          68
              | 3.7   Trajectory of Algorithm 1 as applied to the constrained problem (3.49). The contours
              |       represent the true function F (¬µ), the red dots indicate trust region centers, and the
              |       blue line is the trajectory of the trust region subproblem. . . . . . . . . . . . . . . .           73
blank         | 
              | 
              | 
              | 
meta          |                                                      xvi
text          | 3.8   Trajectory of Algorithm 1 as applied to the constrained problem (3.49) embedded
              |       in the augmented Lagrangian framework. The contours represent the true function
              |       F (¬µ), the red dots indicate trust region centers ¬µk , the blue dots are the candidate
              |       for the next trust region center ¬µÃÇk , and the green region indicates the feasible set for
              |       the trust region subproblem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   74
              | 3.9   Convergence history of the augmented Lagrangian objective quantities using Algo-
              |       rithm 1: LœÑj (¬µk ) (      ), LœÑj (¬µÃÇk ) (   ), mk (¬µk ) (   ), mk (¬µÃÇk ) (   ). The three
              |       augmented Lagrangian iterations are separated by a vertical dashed line with the
              |       following penalty parameters: œÑ0 = 10‚àí4 (iterations 0 ‚àí 9), œÑ1 = 10‚àí5 (iterations
              |       10 ‚àí 19), œÑ2 = 10‚àí6 (iterations 20 ‚àí 29). . . . . . . . . . . . . . . . . . . . . . . . . .    75
              | 3.10 Convergence history of the augmented Lagrangian gradient quantities using Algo-
              |       rithm 1: k‚àáLœÑj (¬µk )k (       ), k‚àáLœÑj (¬µÃÇk )k (    ), k‚àámk (¬µk )k (     ). The three aug-
              |       mented Lagrangian iterations are separated by a vertical dashed line with the follow-
              |       ing penalty parameters: œÑ0 = 10‚àí4 (iterations 0 ‚àí 9), œÑ1 = 10‚àí5 (iterations 10 ‚àí 19),
              |       œÑ2 = 10‚àí6 (iterations 20‚àí29). For each augmented Lagrangian iteration, the gradient
              |       of the true augmented Lagrangian (for fixed œÑj ) decreases 3 ‚àí 4 orders of magnitude.          75
blank         | 
text          | 5.1   Control (left) and corresponding solution (right) of the inviscid Burgers‚Äô equation
              |       in (5.56) at: the initial condition ¬µ = (1.0, 1.0, 0.0) (      ), the target solution ¬µ =
              |       (2.5, 0.02, 0.0425) (   ), and solution of the baseline optimization method (         ). . . 136
              | 5.2   Contours of the objective function f (u(¬µ), ¬µ) in (5.55) in the ¬µ1 ‚àí ¬µ2 plane corre-
              |       sponding to a slice at ¬µ3 = 0.0. The initial condition for the optimization problem
              |       and target solution are shown with a red circle and blue square, respectively. . . . . 138
              | 5.3   Contour of the reduced objective function f (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) in (5.55) in the ¬µ1 ‚àí¬µ2
              |       plane corresponding to a slice at ¬µ3 = 0.0. The reduced-order model employs a
              |       Galerkin projection and the trial basis is constructed from: (top) the primal solution
              |       at ¬µ0 , i.e., col(Œ¶) = span{u(¬µ0 )}; (middle) the primal and adjoint solution at ¬µ0 , i.e.,
              |       col(Œ¶) = span{u(¬µ
              |                       0 ), Œª(¬µ0 )}; (bottom)
              |                                              the primal and sensitivity solution at ¬µ0 , i.e.,
              |                               ‚àÇu
              |       col(Œ¶) = span u(¬µ0 ),       (¬µ ) . The green shaded region indicates the areas where:
              |                               ‚àÇ¬µ 0
              |       (left) the Euclidean ball is bounded by 0.5, i.e., k¬µ ‚àí ¬µ0 k ‚â§ 0.5, (center) the error
              |       between the true and reduced objective function is bounded by 100, i.e., |f (u(¬µ), ¬µ)‚àí
              |       f (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)| ‚â§ 100, and (right) the residual norm of the reconstructed ROM
              |       solution is bounded by 10, i.e., kr(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)k ‚â§ 10. The initial condition for
              |       the optimization problem and target solution are shown with a red circle and blue
              |       square, respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
blank         | 
              | 
              | 
              | 
meta          |                                                   xvii
text          | 5.4   Contour of the reduced objective function f (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) in (5.55) in the ¬µ1 ‚àí¬µ2
              |       plane corresponding to a slice at ¬µ3 = 0.0. The reduced-order model employs a
              |       LSPG projection and the trial basis is constructed from: (top) the primal solution at
              |       ¬µ0 , i.e., col(Œ¶) = span{u(¬µ0 )}; (middle) the primal and adjoint solution at ¬µ0 , i.e.,
              |       col(Œ¶) = span{u(¬µ
              |                       0 ), Œª(¬µ0 )}; (bottom)
              |                                              the primal and sensitivity solution at ¬µ0 , i.e.,
              |                               ‚àÇu
              |       col(Œ¶) = span u(¬µ0 ),       (¬µ ) . The green shaded region indicates the areas where:
              |                               ‚àÇ¬µ 0
              |       (left) the Euclidean ball is bounded by 0.5, i.e., k¬µ ‚àí ¬µ0 k ‚â§ 0.5, (center) the error
              |       between the true and reduced objective function is bounded by 100, i.e., |f (u(¬µ), ¬µ)‚àí
              |       f (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)| ‚â§ 100, and (right) the residual norm of the reconstructed ROM
              |       solution is bounded by 10, i.e., kr(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)k ‚â§ 10. The initial condition for
              |       the optimization problem and target solution are shown with a red circle and blue
              |       square, respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
              | 5.5   Convergence history of various optimization solvers for optimal control of the invis-
              |       cid Burgers‚Äô equation when Galerkin reduced-order model defines the approximation
              |       model. Optimization solvers considered: L-BFGS solver with only HDM evaluations
              |       (    ), prim-etr-intpt (      ), prim-ctr-intpt (      ), prim-ctr-stcg (      ), sens-etr-intpt
              |       (    ), sens-ctr-intpt (       ), sens-ctr-stcg (      ), adj-etr-intpt (       ), adj-ctr-intpt
              |       (    ), adj-ctr-stcg (      ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
              | 5.6   Convergence history of various optimization solvers for optimal control of the inviscid
              |       Burgers‚Äô equation when LSPG reduced-order model defines the approximation model.
              |       Optimization solvers considered: L-BFGS solver with only HDM evaluations (                      ),
              |       prim-etr-intpt (      ), prim-ctr-intpt (      ), prim-ctr-stcg (      ), sens-etr-intpt (      ),
              |       sens-ctr-intpt (      ), sens-ctr-stcg (    ), adj-etr-intpt (      ), adj-ctr-intpt (   ), adj-
              |       ctr-stcg (      ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
              | 5.7   Left: Cumulative number of primal ROM queries as a function of major iteration in
              |       the trust region algorithm based on reduced-order models (Algorithm 11) as applied
              |       to optimal control of the inviscid Burgers‚Äô equation. Right: Histogram of the number
              |       of primal ROM queries at a given basis size. Data separated into the top and bottom
              |       rows to deal with the disparate x-scales. All reduced-order models use a Galerkin pro-
              |       jection. Optimization solvers considered: prim-etr-intpt (            ), prim-ctr-intpt (       ),
              |       prim-ctr-stcg (       ), sens-etr-intpt (     ), sens-ctr-intpt (       ), sens-ctr-stcg (      ),
              |       adj-etr-intpt (      ), adj-ctr-intpt (     ), adj-ctr-stcg (     ). . . . . . . . . . . . . . . 143
              | 5.8   Convergence of the objective function (left) and gradient (right) as a function of
              |       the cost metric in (5.58) for several values of the speedup factor of the reduced-
              |       order model: œÑ = 20 (top row), œÑ = 50 (middle row), œÑ = ‚àû (bottom row) for
              |       optimal control of the inviscid Burgers‚Äô equation. All reduced-order models use a
              |       Galerkin projection. Optimization solvers considered: L-BFGS solver with only HDM
              |       evaluations (      ), sens-etr-intpt (      ), sens-ctr-intpt (     ), sens-ctr-stcg (       ). . . 145
blank         | 
              | 
              | 
              | 
meta          |                                                   xviii
text          | 5.9   Convergence history of the objective quantities for optimal control of the inviscid
              |       Burgers‚Äô equation using Algorithm 11 (left ‚Äì fully converged solutions as snapshots and
              |       in the evaluation of trust region steps) and Algorithm 12 (right ‚Äì partially converged
              |       solutions as snapshots and in the evaluation of trust region steps): F (¬µk ) (                        ),
              |       F (¬µÃÇk ) (      ), mk (¬µk ) (       ), mk (¬µÃÇk ) (         ). The variant ‚Äòsens-etr-intpt‚Äô (Table 5.1)
              |       of the multifidelity trust region algorithm with Galerkin-based reduced-order models
              |       is used. Since the approximation model in the left plot is first-order consistent at
              |       trust region centers, mk (¬µk ) is omitted. . . . . . . . . . . . . . . . . . . . . . . . . . 146
              | 5.10 Convergence history of the gradient quantities for optimal control of the inviscid
              |       Burgers‚Äô equation using Algorithm 11 (left ‚Äì fully converged solutions as snapshots and
              |       in the evaluation of trust region steps) and Algorithm 12 (right ‚Äì partially converged
              |       solutions as snapshots and in the evaluation of trust region steps): k‚àáF (¬µk )k (                     ),
              |       k‚àáF (¬µÃÇk )k (       ), k‚àámk (¬µk )k (         ), k‚àámk (¬µÃÇk )k (         ). The variant ‚Äòsens-etr-intpt‚Äô
              |       (Table 5.1) of the multifidelity trust region algorithm with Galerkin-based reduced-
              |       order models is used. Since the approximation model in the left plot is first-order
              |       consistent at trust region centers, k‚àámk (¬µk )k is omitted. . . . . . . . . . . . . . . . 146
              | 5.11 Convergence history of the constraint quantities for optimal control of the inviscid
              |       Burgers‚Äô equation using Algorithm 11 (left ‚Äì fully converged solutions as snapshots and
              |       in the evaluation of trust region steps) and Algorithm 12 (right ‚Äì partially converged
              |       solutions as snapshots and in the evaluation of trust region steps): œëk (¬µk ) (                       ),
              |       œëk (¬µÃÇk ) (     ), ‚àÜk (         ). The variant ‚Äòsens-etr-intpt‚Äô (Table 5.1) of the multifidelity
              |       trust region algorithm with Galerkin-based reduced-order models is used. . . . . . . 147
              | 5.12 Control (left) and corresponding solution (right) of the viscous Burgers‚Äô equation
              |       in (5.60) at: the initial guess for the optimization problem (                      ) and the optimal
              |       solution of (5.59) (        ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
              | 5.13 Convergence history of various optimization solvers for optimal control of the vis-
              |       cous Burgers‚Äô equation when Galerkin reduced-order model defines the approximation
              |       model. Optimization solvers considered: L-BFGS solver with only HDM evaluations
              |       (     ), adj-etr-intpt (         ), adj-ctr-intpt (          ), adj-ctr-stcg (     ). . . . . . . . . . . 148
              | 5.14 Left: Cumulative number of primal ROM queries as a function of major iteration in
              |       the trust region algorithm based on reduced-order models (Algorithm 11) as applied
              |       to optimal control of the viscous Burgers‚Äô equation. Right: Histogram of the number
              |       of primal ROM queries at a given basis size. Data separated into the top and bottom
              |       rows to deal with the disparate x-scales. All reduced-order models use a Galerkin
              |       projection. Optimization solvers considered: adj-etr-intpt (                     ), adj-ctr-intpt (   ),
              |       adj-ctr-stcg (       ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
blank         | 
              | 
              | 
              | 
meta          |                                                            xix
text          | 5.15 Convergence of the objective function (left) and gradient (right) as a function of
              |      the cost metric in (5.61) for several values of the speedup factor of the reduced-
              |      order model: œÑ = 50 (top row), œÑ = 100 (middle row), œÑ = ‚àû (bottom row) for
              |      optimal control of the viscous Burgers‚Äô equation. All reduced-order models use a
              |      Galerkin projection. Optimization solvers considered: L-BFGS solver with only HDM
              |      evaluations (      ), adj-etr-intpt (    ), adj-ctr-intpt (    ), adj-ctr-stcg (    ).   . . . 151
              | 5.16 Convergence history of the objective (left) and gradient (right) quantities for optimal
              |      control of the viscous Burgers‚Äô equation using Algorithm 11 (fully converged solutions
              |      as snapshots and in the evaluation of trust region steps). Left: |F (¬µk ) ‚àí F (¬µ‚àó )|
              |      (    ), |F (¬µÃÇk ) ‚àí F (¬µ‚àó )| (   ), |mk (¬µÃÇk ) ‚àí F (¬µ‚àó )| (   ). Right: k‚àáF (¬µk )k (       ),
              |      k‚àáF (¬µÃÇk )k (     ), k‚àámk (¬µÃÇk )k (     ). The variant ‚Äòadj-etr-intpt‚Äô (Table 5.1) of the
              |      multifidelity trust region algorithm with Galerkin-based reduced-order models is used.
              |      Since the approximation model is first-order consistent at trust region centers mk (¬µk )
              |      and k‚àámk (¬µk )k are omitted. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
              | 5.17 Shape parametrization of a NACA0012 airfoil using a cubic design element (the no-
              |      tation ¬µi designates the i-th component of the vector ¬µ which refers to the i-th
              |      displacement degree of freedom of the shape parametrization) . . . . . . . . . . . . . 153
              | 5.18 NACA0012 mesh and pressure distribution at Mach 0.5 and zero angle of attack. . . 154
              | 5.19 Cub-RAE2822 mesh and pressure isolines computed at Mach 0.5 and zero angle of
              |      attack. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
              | 5.20 Progression of the objective function during the HDM-based optimization. The initial
              |      guess is defined as the 0th optimization iteration. . . . . . . . . . . . . . . . . . . . . 156
              | 5.21 Subsonic inverse design of the airfoil Cub-RAE2822: initial shape (NACA0012) and
              |      associated Cp function, and final shape (Cub-RAE2822) and associated Cp functions
              |      delivered by the HDM- and ROM-based optimizations, respectively. . . . . . . . . . 157
              | 5.22 Objective function versus number of queries to the HDM: ROM-based optimization
              |      (red) and HDM-based optimization (black). . . . . . . . . . . . . . . . . . . . . . . . 158
              | 5.23 Progression of reduced objective function: dashed line indicates an HDM sample and
              |      a subsequent update of the ROB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
              | 5.24 Progression of HDM residual: dashed line indicates an HDM sample and a subsequent
              |      update of the ROB. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
              | 5.25 Parametrization of CRM. Left: Undeformed CRM configuration. Right: Deformed
              |      CRM configuration with positive perturbation to the wingspan ¬µ1 (top row), localized
              |      sweep ¬µ2 (second row), twist ¬µ3 (third row), and localized dihedral ¬µ4 (bottom row). 161
              | 5.26 Two different views of the initial guess (gray) and solution (red) of the optimization
              |      problem in (5.64). The displacement from the undeformed configuration to the opti-
              |      mal solution (red) is magnified by 2√ó. There is a 2.2 drag count reduction from the
              |      initial to optimized shape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
blank         | 
              | 
              | 
              | 
meta          |                                                  xx
text          | 5.27 Left: Initial guess for optimization problem in (5.64). Right: Solution of optimization
              |       problem in (5.64). Both plots are colored by the coefficient of pressure Cp . There is
              |       a 2.2 drag count reduction from the initial to optimized shape. . . . . . . . . . . . . 162
              | 5.28 Convergence history of the baseline PDE-constrained optimization solver without
              |       model reduction (        ) and proposed trust region method based on hyperreduced
              |       approximation models (        ). A yellow square ( ) indicates an augmented Lagrangian
              |       update. The reduction in drag count is taken as the performance metric and the
              |       number of primal HDM queries is the cost model. With respect to this cost metric,
              |       the ROM-based optimization solver converges 2√ó faster than the HDM-based solver. 163
              | 5.29 Convergence history of the baseline PDE-constrained optimization solver without
              |       model reduction (        ) and proposed trust region method based on hyperreduced
              |       approximation models (        ). A yellow square ( ) indicates an augmented Lagrangian
              |       update. The reduction in drag count is taken as the performance metric and the total
              |       wall time of the optimization procedure (normalized by the wall time of a single
              |       primal HDM solve) is the cost model. With respect to this cost metric, the ROM-
              |       based optimization solver converges 1.6√ó faster than the HDM-based solver. . . . . . 164
              | 5.30 The sample mesh (72√ó103 nodes) used at an intermediate iteration of the trust region
              |       method based on hyperreduced (collocation) approximation models. . . . . . . . . . 164
blank         | 
text          | 6.1   Full tensor product based on Clenshaw-Curtis (levels 1, 3, 5) . . . . . . . . . . . . . 178
              | 6.2   Isotropic sparse grid based on Clenshaw-Curtis (levels 1, 3, 5) . . . . . . . . . . . . . 178
              | 6.3   Anisotropic sparse grid based on Clenshaw-Curtis (levels 1, 3, 5) . . . . . . . . . . . 179
              | 6.4   Anisotropic sparse grid based on Clenshaw-Curtis with all (including non-admissible)
              |       forward neighbors (levels 1, 3, 5) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
              | 6.5   Left: the control defining the initial guess for the optimization problem (              ), the
              |       solution of the deterministic optimal control problem, i.e., with the stochastic variables
              |       fixed at their mean value y = 0 (            ), and the solution of the stochastic optimal
              |       control problem (       ). Right: the mean solution of the viscous Burgers‚Äô equation
              |       in (6.68) at the initial control (       ), optimal deterministic control (        ), and the
              |       optimal stochastic control. One (         ) and two (      ) standard deviations about the
              |       mean solution corresponding to the optimal stochastic control are also included.             . . 199
              | 6.6   Convergence history of the objective error quantities using MI (left) and MII (right):
              |       |F (¬µk ) ‚àí F (¬µ‚àó )| (   ), |F (¬µÃÇk ) ‚àí F (¬µ‚àó )| (   ), |mk (¬µk ) ‚àí F (¬µ‚àó )| (    ), |mk (¬µÃÇk ) ‚àí
              |            ‚àó
              |       F (¬µ )| (      ). Rapid progress is made toward the optimal solution, despite poor
              |       agreement between the objective and model at early iterations. . . . . . . . . . . . . 200
              | 6.7   Convergence history of the gradient quantities using MI (left) and MII (right):
              |       k‚àáF (¬µk )k (      ), k‚àáF (¬µÃÇk )k (     ), k‚àámk (¬µk )k (      ), k‚àámk (¬µÃÇk )k (      ). . . . . . 201
              | 6.8   Cumulative number of HDM primal and adjoint evaluations as the major iterations
              |       in the various trust region algorithms progress: BII (           ), BIII (       ), MI (      ),
              |       MII (       ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
blank         | 
              | 
meta          |                                                    xxi
text          | 6.9   Left: Cumulative number of primal and adjoint ROM evaluations as the major iter-
              |       ations in the various trust region algorithms progress. Right: Number of primal and
              |       adjoint ROM queries organized according to the size of the reduced-order basis (ku ).
              |       Trust region methods considered: MI (           ), MII (    ). . . . . . . . . . . . . . . . . 202
              | 6.10 Convergence of the objective function (left) and gradient (right) as a function of the
              |       cost metric in (6.69) for method MIII for several values of the speedup factor of the
              |       reduced-order model: œÑ = 1 (       ), œÑ = 10 (       ), œÑ = 100 (    ), œÑ = ‚àû (     ). The
              |       baseline methods used for comparison: BI (           ) and BIII (     ). . . . . . . . . . . 203
blank         | 
text          | C.1 Schematic of restriction of parameter space RN¬µ to affine subspace A(¬µÃÑ, Œ•) of di-
              |       mension k¬µ , in the special case where N¬µ = 2 and k¬µ = 1. The optimal solution ¬µ‚àó
              |       in the parameter space, as well as the optimal solution over A(¬µÃÑ, Œ•) are also depicted.227
blank         | 
text          | D.1 Time-dependent mapping between reference and physical domains. . . . . . . . . . . 241
              | D.2 Airfoil kinematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
              | D.3 Verification of adjoint-based gradient with fourth-order centered finite difference ap-
              |       proximation, for a range of finite intervals, œÑ , for the total work W ‚Äîthe objective
              |       function in (D.44)‚Äîfor parametrization PII (Table D.2). The computed gradient
              |       match the finite difference approximation to about 10 digits of accuracy before round-
              |       off errors degrade the accuracy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
              | D.4 Trajectories of x(t), y(t), and Œ∏(t) at initial guess (         ), solution of (D.44) under
              |       parametrization PI (      ), and solution of (D.44) under parametrization PII (           )
              |       for ID = 7. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
              | D.5 Time history of instantaneous quantities of interest (x-directed force ‚Äì Fxh (u, ¬µ, t),
              |       y-directed force ‚Äì Fyh (u, ¬µ, t), total power ‚Äì P h (u, ¬µ, t), x-translational power ‚Äì
              |       Pxh (u, ¬µ, t), y-translational power ‚Äì Pyh (u, ¬µ, t), rotational power ‚Äì PŒ∏h (u, ¬µ, t)) at
              |       initial guess (    ), solution of (D.44) under parametrization PI (         ), and solution
              |       of (D.44) under parametrization PII (       ) for ID = 7. . . . . . . . . . . . . . . . . . 259
              | D.6 Left: Convergence of total work W with optimization iteration for parametrization PI
              |       (    ) and PII (     ) for ID = 7. Both optimization problems converge to a motion
              |       with significantly lower required total work; PII finds a better motion than PI (in
              |       terms of total work) due to the enlarged search space, at the cost of additional iter-
              |       ations. Each optimization iteration requires a primal flow computation‚Äîto evaluate
              |       the quantities of interest‚Äîand its corresponding adjoint‚Äîto evaluate the gradient of
              |       the quantity of interest. Right: Convergence of optimal value of total work W as
              |       parameter space is refined for parametrization PI (           ) and PII (     ). This im-
              |       plies convergence to an optimal, smooth trajectory that is not polluted by its discrete
              |       parametrization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
blank         | 
              | 
              | 
              | 
meta          |                                                xxii
text          | D.7 Flow vorticity around airfoil undergoing motion corresponding to initial guess for
              |      optimization, i.e., pure heaving (       ). Flow separation off leading edge implies a
              |      large amount of work required to complete mission. Snapshots taken at times t =
              |      0.0, 0.8, 1.6, 2.4, 3.2, 4.0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
              | D.8 Flow vorticity around airfoil undergoing motion corresponding to optimal pitching
              |      motion for fixed translational motion, i.e., solution of (D.44) under parametrization
              |      PI (       ). The pitching motion greatly reduces the degree of flow separation and
              |      vortex shedding compared to the initial guess, and requires less work to complete the
              |      mission. Snapshots taken at times t = 0.0, 0.8, 1.6, 2.4, 3.2, 4.0. . . . . . . . . . . . 262
              | D.9 Flow vorticity around airfoil undergoing motion corresponding to optimal rigid body
              |      motion, i.e., solution of (D.44) under parametrization PII (            ). This rigid body
              |      motion further reduces the degree of flow separation and required work to complete
              |      the mission. This motion differs from the solution of PI as it has a larger pitch
              |      amplitude and slightly overshoots the final vertical position before settling to the
              |      required position. Snapshots taken at times t = 0.0, 0.8, 1.6, 2.4, 3.2, 4.0. . . . . . 263
              | D.10 Airfoil kinematics and deformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
              | D.11 Trajectories of y(t), Œ∏(t), and c(t) at initial guess (         ), solution of (D.49) under
              |      parametrization FI (q = 0.0:          , q = 1.0:       , q = 2.5:        ), and solution of
              |      (D.49) under parametrization FII (q = 0.0:           , q = 1.0:       , q = 2.5:     ) from
              |      Table D.4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
              | D.12 Time history of total power, P h (u, ¬µ, t), and x-directed force, Fxh (u, ¬µ, t), imparted
              |      onto foil by fluid at initial guess (      ), solution of (D.49) under parametrization
              |      FI (q = 0.0:         , q = 1.0:       , q = 2.5:        ), and solution of (D.49) under
              |      parametrization FII (q = 0.0:        , q = 1.0:     , q = 2.5:       ) from Table D.4.   . . 266
              | D.13 Convergence of quantities of interest, W and Jx , with optimization iteration for
              |      parametrization FI (q = 0.0:       , q = 1.0:      , q = 2.5:      ) and FII (q = 0.0:    ,
              |      q = 1.0:       , q = 2.5:    ) from Table D.4. Each optimization iteration requires the
              |      a primal flow computation‚Äîto evaluate quantities of interest‚Äîand its corresponding
              |      adjoint‚Äîto evaluate the gradient of quantities of interest. . . . . . . . . . . . . . . . 267
              | D.14 Flow vorticity around flapping airfoil undergoing motion corresponding to initial guess
              |      for optimization problem (D.49), i.e., pure heaving (       ). Flow separation off leading
              |      edge implies a large amount of work required for flapping motion. Snapshots taken
              |      at times t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0. . . . . . . . . . . . . . . . . . . . . 269
              | D.15 Flow vorticity around flapping airfoil undergoing optimal rigid body motion corre-
              |      sponding to the solution of (D.49) under parametrization FI. The x-directed im-
              |      pulse is Jx = 2.5. The pitching motion greatly reduces the degree of flow separation
              |      and vortex shedding compared to the initial guess, and requires less work to com-
              |      plete the flapping motion and generate desired impulse. Snapshots taken at times
              |      t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0. . . . . . . . . . . . . . . . . . . . . . . . . . 270
blank         | 
              | 
              | 
meta          |                                               xxiii
text          | D.16 Flow vorticity around flapping airfoil undergoing optimal deformation and kinematic
              |      motion, corresponding to the solution of (D.49) under parametrization FII. The x-
              |      directed impulse is Jx = 2.5. The morphing further reduces the flow separation
              |      and work required to complete the flapping motion and generate desired impulse.
              |      Snapshots taken at times t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0.                . . . . . . . . . . 271
              | D.17 Trajectories of h(¬µ, t) and Œ∏(¬µ, t) that define the motion of the airfoil in Figure D.27
              |      and will be used to study primal and dual time-periodic solvers. . . . . . . . . . . . . 285
              | D.18 Flow vorticity around heaving/pitching airfoil for simulation initialized from steady
              |      state flow. Non-physical transients are introduced at the beginning of the time interval
              |      that result in non-trivial errors in integrated quantities of interests. Snapshots taken
              |      at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0. . . . . . . . . . . . . . . . . . . . . . . . . . 286
              | D.19 Time-periodic flow vorticity around heaving/pitching airfoil, i.e., initialized from pe-
              |      riodic initial condition. The time-periodic initial condition ensures transients are not
              |      introduced at the beginning of the simulation; the result is a seamless transition be-
              |      tween periods, as would be experienced in-flight, and trusted integrated quantities of
              |      interest. Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0. . . . . . . . . . . . 286
              | D.20 Convergence comparison for numerical solvers for fully discrete time-periodically con-
              |      strained partial differential equations (D.52), (D.54), nonlinearly preconditioned with
              |      m fixed point iterations. Left: m = 0, middle: m = 1, right: m = 5. Solvers:
              |      fixed point iteration (     ), steepest decent (               ), L-BFGS (    ), Newton-GMRES:
              |              ‚àí2                   ‚àí3                       ‚àí4
              |      ‚àÜ = 10       (   ), ‚àÜ = 10        (    ), ‚àÜ = 10           (      ), where ‚àÜ is the GMRES con-
              |      vergence tolerance. The optimization algorithms (steepest decent and L-BFGS) were
              |      not included in the m = 0 study due to lack of convergence issues. . . . . . . . . . . 287
              | D.21 Linear and nonlinear convergence of Newton-GMRES method for determining fully
              |      discrete time-periodic solutions with various linear system tolerances, ‚àÜ, i.e., kJ x ‚àí Rk <
              |      ‚àÜ, where r and J are defined in (D.61) and (D.62). Tolerances considered: ‚àÜ = 10‚àí2
              |      (    ), ‚àÜ = 10‚àí3 (        ), ‚àÜ = 10‚àí4 (         ). . . . . . . . . . . . . . . . . . . . . . . . . 289
              | D.22 Time history of power, Fxh (u, ¬µ, t), and x-directed force, P h (u, ¬µ, t), after k Newton-
              |      GMRES iterations (linear system convergence tolerance ‚àÜ = 10‚àí2 ) starting from
              |      steady-state. Values of k: 0 (        ), 1 (      ), and 8 (         ). . . . . . . . . . . . . . . . 289
              | D.23 Convergence of fully discrete quantities of interest to their values at the time-periodic
              |      solution, W ‚àó and Jx‚àó , for various solvers, without nonlinear preconditioning. Solvers:
              |      Newton-GMRES: ‚àÜ = 10‚àí2 (               ), ‚àÜ = 10‚àí3 (              ), ‚àÜ = 10‚àí4 (     ), where ‚àÜ is
              |      the GMRES convergence tolerance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
              |                                     ‚àÇu(Nt )
              | D.24 First 200 eigenvalues ( ) of    ‚àÇu0 ‚Äîevaluated         at periodic solution‚Äîwith largest mag-
              |      nitude. All eigenvalues lie in unit circle, thus the periodic orbit is stable. . . . . . . . 290
blank         | 
              | 
              | 
              | 
meta          |                                                     xxiv
text          | D.25 GMRES convergence for determining solution of adjoint equations corresponding to
              |      fully discrete time-periodic partial differential equation, i.e., a linear two-point bound-
              |                                                          ‚àÇW                   ‚àÇJx
              |      ary value problem. A defined in (D.81), b1 =               , and b2 =           from (D.80),
              |                                                         ‚àÇu(Nt )              ‚àÇu(Nt )
              |      where W is fully discrete approximation of the total work done by fluid on airfoil
              |      and Jx is the x-directed impulse. Solvers: fixed point iteration (         ) and GMRES
              |      (    ). The linearization is performed about the time-periodic solution obtained with
              |      Newton-Krylov (‚àÜ = 10‚àí4 ) method. . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
              | D.26 Verification of periodic adjoint-based gradient with second-order centered finite differ-
              |      ence approximation, for a range of finite intervals, œÑ . The computed gradient match
              |      the finite difference approximation to nearly 7 digits before round-off errors degrade
              |      the accuracy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
              | D.27 Kinematic description of body under consideration, NACA0012 airfoil (right). . . . . 293
              | D.28 Trajectories of h(¬µ, t) and Œ∏(¬µ, t) at initial guess (      ) and optimal solution (       )
              |      for optimization problem in (D.93). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
              | D.29 Time history of the power, P h (u, ¬µ, t), and x-directed force, Fxh (u, ¬µ, t), imparted
              |      onto foil by fluid at initial guess (    ) and optimal solution (       ) for optimization
              |      problem in (D.93). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
              | D.30 Convergence of quantities of interest, W and Jx , with optimization iteration. Each
              |      optimization iteration requires a periodic flow computation and its corresponding
              |      adjoint to evaluate the quantities of interest and their gradients. . . . . . . . . . . . 297
              | D.31 Trajectory of airfoil and flow vorticity at initial guess for optimization (pure heaving
              |      motion, see Figure D.28). Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0. . 297
              | D.32 Trajectory of airfoil and flow vorticity at energetically optimal, zero-impulse flapping
              |      motion (see Figure D.28). Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0. . 298
blank         | 
              | 
              | 
              | 
meta          |                                                xxv
title         | Chapter 1
blank         | 
title         | Introduction
blank         | 
title         | 1.1     Motivation
text          | Optimization problems governed by partial differential equations, or PDE-constrained optimiza-
              | tion problems, arise in nearly every branch of engineering and science. The most classical PDE-
              | constrained optimization problems arise in the context of design and control of engineering systems.
              | The solutions of these problems promise to deliver engineering systems with superior performance
              | (in some chosen metric) than otherwise possible, and will have the greatest impact in highly complex
              | situations where intuition breaks down and prototyping and experimentation are expensive, difficult,
              | or dangerous. Topological optimization can lead to lightweight, highly optimized designs intended to
              | operate in volatile multiphysics environments [180], which can be realized using 3D printing or addi-
              | tive manufacturing technology [111, 203]. Topology optimization also promises to have widespread
              | impact in medicine, specifically with regard to medical implants, since optimized, patient-specific
              | implants can be realized [213]. Shape optimization has been used to design aircraft and automobiles
              | with superior aerodynamic performance [164, 163, 123, 55] and reduced environmental and noise
              | [50, 73] impact. Shape optimization has also been used in biological applications, e.g., to design the
              | shape of the incoming branch of the aorto-coronaric bypass [160, 171]. Boundary and volumetric
              | control have been used to drive the state of an engineering system toward some desired using source
              | terms, e.g., diverting heat from a microprocessor using a fan.
              |    PDE-constrained optimization problems also arise in the context of material and initial condition
              | inversion. In material inversion problems, an unknown material distribution must be inferred from
              | the response of a system to known inputs. Nondestructive evaluation [39, 72] seeks to determine
              | the material distribution of a solid object in situ, i.e., without extracting a sample and performing
              | laboratory tests, from the response measured from structural and acoustic inputs to detect structural
              | defects prior to operation. A similar PDE-constrained optimization problem underlies the emerging
              | technology of full waveform inversion [195] where the material properties of the Earth‚Äôs crust are
              | sought in order to detect the location and size of oil reservoirs. In initial condition inversion, the
blank         | 
              | 
              | 
meta          |                                                   1
              | CHAPTER 1. INTRODUCTION                                                                              2
blank         | 
              | 
              | 
text          | initial state of a system must be inferred from measurements at later times. An important instance
              | of such a PDE-constrained optimization problem occurs in the determination of the source of a
              | contaminant given its current configuration.
              |    The problems considered to this point have been posed in the ideal setting of certain knowl-
              | edge of the data defining the coefficients (material properties) and boundary conditions (loads) of
              | the partial differential equation describing the physical system of interest. This is not realistic as
              | all physical systems, particularly those characterized by a high degree of volatility, are plagued
              | with uncertainties. An important consideration in any discipline in science and engineering is the
              | robustness of a particular system with respect to these uncertainties. PDE-constrained optimiza-
              | tion problems also arise when attempting to quantify the uncertainty in quantities of interest of
              | PDEs as a result of uncertain data or input. For example, in the Bayesian framework, locating the
              | Maximum A Posteriori (MAP) point is a required step in importance sampling [134], i.e., where
              | samples are efficiently drawn from the posterior distribution of the uncertain PDE, and amounts to
              | a PDE-constrained optimization problem. Beyond simply using PDE-constrained optimization to
              | facilitate uncertainty quantification, it is important to incorporate uncertainty quantification into
              | the optimization problem to obtain designs and controls that are risk-averse with respect to the un-
              | certainties. This leads to PDE-constrained optimization under uncertainty‚Äîoptimization problems
              | constrained by stochastic partial differential equations with objective and constraints defined as risk
              | or hazard measures of its quantities of interests. These hazard measures usually penalize variance
              | from the mean or rare, catastrophic events.
              |    While the potential benefit of widespread adoption of PDE-constrained optimization in engi-
              | neering and scientific practice are profound, a number of factors prevent this, most notably the
              | large computational cost, in terms of computing time and resources, associated with these problems.
              | Often, particularly in relevant 3D applications, partial differential equations require a massive dis-
              | cretization to accurately resolve the underlying physics and the solution of the resulting (sequence
              | of) nonlinear equations requires significant computing time on a supercomputer. PDE-constrained
              | optimization problems require potentially many queries to the underlying discretized primal and dual
              | PDE to iteratively progress toward the optimal solution. Since even a single PDE solve constitutes a
              | significant investment in computational resources, the many-query setting of PDE-constrained opti-
              | mization exacerbates this problem and, in some cases, can be prohibitively expensive. This situation
              | is further complicated from the presence of uncertainty that exists in every physical setting, particu-
              | larly those with a high degree of volatility. In reality, the boundary conditions, material properties,
              | and sources terms of a system are not known with certainty and cannot be modeled as such if one
              | wishes to discover solutions that are robust with respect to these uncertainties. Depending on the
              | number of stochastic parameters incorporated into the PDE, the quantification of uncertainty in an
              | optimization problem will increase the computational cost by potentially many orders of magnitude.
              | This effectively makes these problems infeasible for all except the smallest academic problems.
meta          | CHAPTER 1. INTRODUCTION                                                                                  3
blank         | 
              | 
              | 
title         | 1.2      Strategy and Objectives
text          | The primary objective of this thesis is to develop a series of optimization methods to solve large-scale
              | deterministic and stochastic PDE-constrained optimization problems that can largely circumvent the
              | prohibitive cost of repeatedly running computational physics simulations without compromising the
              | quality of the resulting optimum. Focus will be placed on methods that apply to complex, nonlinear
              | partial differential equations that do not possess significant structure, i.e., linearity, ellipticity, and
              | coercivity, that can be used to develop inexpensive, computable error bounds. The strategy taken
              | to accomplish this objective is modular in the sense that two independent technologies will be
              | developed and later combined and specialized to the context of deterministic and stochastic PDE-
              | constrained optimization. The two foundational technologies that are developed for this purpose
              | are: (1) a globally convergent, generalized trust region method for the management of approximation
              | models in the context of optimization and (2) minimum-residual, projection-based reduced-order and
              | hyperreduced models as low-dimensional approximations of the discretized PDE.
              |    In the context of deterministic PDE-constrained optimization, these technologies, along with
              | the concept of a partially converged PDE solution, will be combined to yield an efficient, globally
              | convergent optimization procedure. In the context of stochastic PDE-constrained optimization,
              | projection-based reduced-order models and dimension-adaptive sparse grids will define an efficient
              | approximation model based on two levels of inexactness. This two-level approximation will be nested
              | in the generalized trust region method to produce an efficient, globally convergent optimization
              | procedure.
blank         | 
              | 
title         | 1.3      Literature Review
text          | This work builds on several foundational technologies including PDE-constrained optimization, trust
              | region methods, projection-based model reduction, and surrogate methods for PDE-constrained
              | optimization. This section presents a brief literature review of each technology and outlines the
              | contributions of this thesis to the state-of-the-art.
blank         | 
              | 
title         | 1.3.1     PDE-Constrained Optimization
text          | This work is primarily concerned with the efficient solution of optimization problems governed by
              | partial differential equations, in both the deterministic and stochastic setting. This section provides
              | a brief literature review of deterministic and stochastic PDE-constrained optimization with an ex-
              | tensive mathematical formulation of PDE-constrained optimization problems provided in Chapter 2.
              |    PDE-constrained optimization has been extensively studied in the case where the underlying
              | partial differential equation is deterministic. A thorough review of the topic is provided in the
              | references [78, 96]. The PDE-constrained optimization problem is naturally posed in a continuous
              | setting [100, 135], that is, the PDE itself is a constraint of the optimization problem and the ob-
              | jective function and ‚Äúside‚Äù constraints are defined by integrating the PDE solution over (portions)
meta          | CHAPTER 1. INTRODUCTION                                                                             4
blank         | 
              | 
              | 
text          | of the spatio-temporal domain. The corresponding optimality conditions are a system of partial
              | differential equations that must be discretized to be solvable in a computational setting. A more
              | common and practical approach, particularly in large-scale implementations, defines an optimization
              | problem constrained by the discretized PDE, resulting in an optimality system consisting of a system
              | of discrete nonlinear equations [138, 187, 130, 140]. Once the optimization setting has been chosen,
              | the optimization problem can be solved using a full space [147, 69, 91, 116, 2] or reduced space
              | [100, 197, 138, 108, 109, 210] approach. The reduced space approach uses a PDE solver to eliminate
              | the PDE constraint from the optimization problem while the full space approach considers the PDE
              | as a constraint and directly solves the complete optimization problem. If the reduced space method
              | is employed and a gradient-based optimizer is used, the sensitivity [80, 138, 129, 127] or adjoint
              | [100, 197, 187, 130, 123] method are required to compute the required gradients of the optimiza-
              | tion functionals. The relative efficiency of these methods depends on the number of optimization
              | variables and constraints: the sensitivity method is more efficient if there are more constraints than
              | variables and the adjoint method is more efficient in the opposite case. These concepts regarding
              | the continuous versus discrete formulation, full space versus reduced space approach, and sensitiv-
              | ity versus adjoint method for computing gradients apply whether the partial differential equation
              | under consideration is static or transient [88, 116, 136, 124, 205, 140, 55, 132, 211, 212]. The case
              | where the PDE is unsteady represents a significant increase in computational expense as there are
              | substantially more optimization variables in the full space approach (one for each spatial degree of
              | freedom at each timestep) or the full transient PDE must be resolved at each optimization iteration
              | in the reduced space approach. This work will solely consider the discrete formulation of the PDE-
              | constrained optimization problem, which will be solved using the reduced space approach. Both the
              | sensitivity and adjoint methods will be used to compute gradients of quantities of interest.
              |    In addition to the many considerations involved in the formulation and solution of deterministic
              | PDE-constrained optimization problems, the case where the underlying PDE depends on random
              | data [13, 141, 142, 199, 204, 12] involves an additional component‚Äîtreatment of the stochastic
              | variables. Stochastic Galerkin [13] and collocation [12] are popular techniques for discretizing the
              | stochastic space associated with the PDE. This work will solely consider the non-intrusive approach
              | of stochastic collocation [24, 23, 22, 178, 188, 107] and the collocation nodes will be defined using
              | sparse grids [184, 144, 66, 145, 156, 18, 157, 67, 29, 146]. While there has been work considering
              | random optimization variables [24, 30], this work will consider the optimization variables to be
              | deterministic quantities, with the data underlying the PDE (boundary conditions, coefficients) as
              | uncertain. These instances of PDE-constrained optimization problems under uncertainty can be
              | many orders of magnitude more expensive than the deterministic counterpart since the PDE solution
              | must be resolved over the stochastic space, which may be high dimensional. While these problems
              | have been solved in a number of relevant applications [30, 49, 178], they are impractically expensive
              | for many important engineering and science applications.
meta          | CHAPTER 1. INTRODUCTION                                                                                    5
blank         | 
              | 
              | 
title         | 1.3.2     Trust Region Methods
text          | One of the foundational technologies that this thesis builds upon and extensively utilizes are trust
              | region methods for numerical optimization. Trust region methods are a popular and robust globaliza-
              | tion strategy for numerical optimization solvers, that is, a framework for ensuring a local minimum is
              | obtained, regardless of the starting point. While they are not usually considered as efficient as line-
              | search methods [71, 143], they are popular due to their robustness and flexibility. Let F : RN¬µ ‚Üí R
              | define a function to be minimized and suppose evaluations of F (¬µ) and ‚àáF (¬µ) are expensive. Early
              | trust region methods replaced the potentially expensive optimization problem
blank         | 
text          |                                               minimize F (¬µ)
              |                                                ¬µ‚ààRN¬µ
blank         | 
              | 
text          | with the inexpensive quadratic program
blank         | 
text          |                                                                  1
              |              minimize     mk (¬µ) := F (¬µk ) + ‚àáF (¬µk )(¬µ ‚àí ¬µk ) + (¬µ ‚àí ¬µk )T Bk (¬µ ‚àí ¬µk )
              |               ¬µ‚ààRN¬µ                                              2
              |              subject to    k¬µ ‚àí ¬µk k ‚â§ ‚àÜk ,
blank         | 
text          | where Bk is a symmetric positive-definite approximation of the Hessian ‚àá2 F (¬µk ). The solution of
              | this trust region subproblem provides a candidate for the new trust region center and, depending on
              | how well the reduction actually achieved by accepting the step compares to the reduction predicted
              | by the quadratic model, the step is accepted or rejected and the trust region radius ‚àÜk is modified
              | accordingly. The quality of the trust region step is assessed by comparing the actual-to-predicted
              | reduction ratio (œÅk ) to unity
              |                                                 F (¬µk ) ‚àí F (¬µÃÇk )
              |                                         œÅk =                        ,
              |                                                mk (¬µk ) ‚àí mk (¬µÃÇk )
              | where ¬µÃÇk is the candidate step, defined as the solution of the trust region subproblem. Once the
              | details of the step acceptance and radius modification are complete, it can be shown [48] that the
              | sequence of trust region centers {¬µk } converges to a first-order critical point
blank         | 
text          |                                             lim k‚àáF (¬µk )k = 0.
              |                                            k‚Üí‚àû
blank         | 
              | 
text          | The pivotal work in [159, 133] established convergence under only mild conditions‚Äîknown as the
              | fraction of Cauchy decrease‚Äîon the candidate step produced by the quadratic program. A slew
              | of specialized, efficient solvers have been developed that generate steps guaranteed to satisfy the
              | fraction of Cauchy decrease; see [133, 48] for an extensive overview.
              |    In many applications, it may be expensive or impossible to evaluate the objective function or
              | its gradient to construct the quadratic approximation, e.g., if F (¬µ) corresponds to the quantity of
              | interest of a partial differential equation or an iterative linear solver [93, 166, 92] is used to compute to
              | compute F (¬µ) or ‚àáF (¬µ), and a host of work [133, 189, 34, 35, 36, 48, 93, 216, 108, 109] has been done
              | to allow for inexact gradient evaluations to be used in the definition of the trust region subproblem
meta          | CHAPTER 1. INTRODUCTION                                                                               6
blank         | 
              | 
              | 
text          | and inexact objective evaluations in the computation of œÅk . MoreÃÅ [133] introduced an inexact gradient
              | condition that requires the gradient approximation at the trust region center, gk , asymptotically
              | approaches the true gradient, i.e., k‚àáF (¬µk ) ‚àí gk k ‚Üí 0 for any convergent sequence {¬µk }. While
              | this provides substantial flexibility over previous work that requires first-order consistency of the
              | approximation and model (gk = ‚àáF (¬µk )), this condition does not suggest an accuracy condition on
              | gk at a particular iteration. Carter [34, 35, 36] suggested the relative gradient error condition
blank         | 
text          |                                 k‚àáF (¬µk ) ‚àí gk k ‚â§ Œ∑ kgk k       Œ∑ ‚àà (0, 1),
blank         | 
text          | which has served as the basis for many trust region model management methods, including the
              | popular Trust Region Proper Orthogonal Decomposition [10] method. The Carter condition is
              | useful because it does not require gk be recomputed to higher accuracy after a failed step; however,
              | it requires the evaluation of the gradient error (or a tight bound), which may be impractical in many
              | situations. Toint [189] suggested the gradient condition
blank         | 
text          |                            k‚àáF (¬µk ) ‚àí gk k ‚â§ min{Œ∫1 ‚àÜk , Œ∫2 }          Œ∫1 , Œ∫2 > 0
blank         | 
text          | that requires increased accuracy as ‚àÜk decreases, i.e., after failed iterations, but relies on arbitrary
              | constants Œ∫1 , Œ∫2 . Heinkenschloss and Vincent [93] suggested a similar gradient condition in the
              | context of a Sequential Quadratic Programming (SQP) method
blank         | 
text          |                              k‚àáF (¬µk ) ‚àí gk k ‚â§ Œæ min{kgk k , ‚àÜk }         Œæ>0
blank         | 
text          | that requires increased accuracy after failed iterations or near convergence and also depends on an
              | arbitrary constant. The arbitrary constants required by the Toint [189] and Heinkenschloss-Vincent
              | [93] bounds are significant as they permit the use of error indicators that can completely circumvent
              | the need to compute or tightly bound the gradient error. Suppose an error indicator œïk : RN¬µ ‚Üí R
              | can be derived such that
              |                                  k‚àáF (¬µk ) ‚àí gk k ‚â§ Œæœïk (¬µk )       Œæ > 0,
blank         | 
text          | where Œæ > 0 is an arbitrary constant. Then the Heinkenschloss-Vincent [93] gradient condition will
              | be satisfied if the error indicator satisfies
blank         | 
text          |                                        œïk (¬µk ) ‚â§ Œ∫ min{kgk k , ‚àÜk },
blank         | 
text          | where Œ∫ > 0 is any user-defined constant. Since the error indicator is solely used to enforce the
              | required gradient condition, the constant Œæ > 0 is never computed and may depend on quantities
              | that, in general, cannot be computed such as Lipschitz constants or bounds on various quantities.
              | This work employs the Heinkenschloss-Vincent condition due to the required generality in handling
              | complex, nonlinear PDE-constrained optimization problems where tight gradient error bounds are
              | not readily available.
meta          | CHAPTER 1. INTRODUCTION                                                                               7
blank         | 
              | 
              | 
text          |    Similar to the inexact gradient condition used in the trust region subproblem, conditions have
              | been developed [34, 216, 109] for using inexact objective function evaluations in the actual-to-
              | predicted reduction ratio, œÅk . The asymptotic condition in [109] allows for the same flexibility as
              | the Heinkenschloss-Vincent gradient condition and will be used in this work. Kouri [109] replaced
              | the computation of œÅk with
              |                                                  œàk (¬µk ) ‚àí œàk (¬µÃÇk )
              |                                          œÅÃÉk =                        ,
              |                                                  mk (¬µk ) ‚àí mk (¬µÃÇk )
              | where œàk : RN¬µ ‚Üí R is the inexact objective model that satisfies
blank         | 
text          |                                                                                           1/œâ
              |        |F (¬µk ) ‚àí F (¬µÃÇk ) + œàk (¬µÃÇk ) ‚àí œàk (¬µk )| ‚â§ œÉ [Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }]       œÉ>0
blank         | 
text          | and œÉ is an arbitrary constant, {rk }‚àû
              |                                      k=1 is a forcing sequence such that rk ‚Üí 0, and ¬µÃÇk is the
              | candidate step at iteration k. This condition permits the use of an error indicator Œ∏k : RN¬µ ‚Üí R
              | such that
              |                         |F (¬µk ) ‚àí F (¬µ) + œàk (¬µ) ‚àí œàk (¬µk )| ‚â§ œÉŒ∏k (¬µ)          œÉ > 0.
blank         | 
text          | The true error can be disregarded and the inexact objective condition enforced solely based on the
              | error indicator
              |                                 Œ∏k (¬µÃÇk )œâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk },
blank         | 
text          | where œâ, Œ∑ ‚àà (0, 1) are algorithmic constants. When the Heinkenschloss-Vincent gradient condition
              | [93] and Kouri objective condition [109] are combined into a single trust region method, as seen in
              | [109], the entire algorithm proceeds without requiring queries to F (¬µ) or ‚àáF (¬µ) and guarantees
              | global convergence‚Äîthis flexibility will be built upon and leveraged in this work as I look to develop
              | methods that address large-scale, expensive problems where inexpensive, tight error bounds are not
              | available.
              |    In many cases, it is possible to obtain an approximation that is superior to the basic quadratic
              | model, which can be used to provide a better approximation model mk (¬µ) in the trust region
              | framework. Alexandrov [4, 6] introduced the trust region model management framework that al-
              | lows for this and proves global convergence, provided the approximation model satisfies first-order
              | consistency at trust region centers
blank         | 
text          |                                mk (¬µk ) = F (¬µk )       ‚àámk (¬µk ) = ‚àáF (¬µk ).
blank         | 
text          | These requirements can be weakened by introducing the inexact gradient conditions of [189, 35, 93]
              | and inexact objective condition of [34, 109]. This flexibility has been leveraged in a number of
              | contexts, most notably the Trust Region Proper Orthogonal Decomposition method [10, 57, 1,
              | 170, 186] where the approximation model is taken as the projection-based reduced-order model
              | whose reduced basis is computed via Proper Orthogonal Decomposition (POD) and the method
              | of snapshots [183] and the Carter condition [35] is employed. It was also leveraged in [108, 109]
              | in the context of PDE-constrained optimization uncertainty where the model problem employed
meta          | CHAPTER 1. INTRODUCTION                                                                            8
blank         | 
              | 
              | 
text          | dimension-adaptive sparse grids to approximate the integral of the PDE quantity of interest over
              | the stochastic space.
blank         | 
              | 
title         | 1.3.3    Projection-Based Model Reduction
text          | Another pivotal technology in this work is projection-based model reduction, which will be used to
              | define inexpensive approximation models for the expensive PDE discretization and solver underlying
              | the PDE-constrained optimization problem of interest. The concepts underlying modern reduced-
              | order models have been used in the context of modal decomposition for linear structural dynamics
              | for several decades [65]. In this approach, the dynamics of a particular structure are approximated
              | using its dominant modes, which are computed via an eigenvalue decomposition of the system mass
              | and stiffness matrices. Generalization to the case of a nonlinear structure exist [98], but have not
              | seen the same widespread adoption as the linear case.
              |    Modern approaches to projection-based model reduction include the reduced basis method [121,
              | 122, 17, 173] and methods based on Proper Orthogonal Decomposition (POD) [21, 101] and the
              | method of snapshots [183]. The reduced basis method employs a variational framework and con-
              | structs a reduced basis from solutions of the underlying PDE that are greedily sampled in the
              | parameter space at locations where an inexpensive error bound on the reduced-order model is max-
              | imized [149, 173]. This is usually embedded in an offline-online framework [17, 149, 173] where all
              | expensive operations related to sampling the PDE and construction of the reduced basis are con-
              | fined to an offline phase and the inexpensive reduced-order model is repeatedly queried in the online
              | phase. While this method possesses a beautiful mathematical framework, it relies on properties
              | of the underlying PDE such as linearity and ellipticity for the derivation of the error bounds and
              | the efficient offline-online decomposition. POD-based model reduction is a general framework that
              | uses POD to compress ‚Äúsnapshots‚Äù of the PDE solution at particular time instances and parameter
              | configurations to generate a physics-based basis that will be used to approximate the solution. The
              | governing equations are restricted to a low-dimensional ‚Äútrial‚Äù subspace and projected onto an ap-
              | propriate ‚Äútest‚Äù subspace. The result is a small nonlinear system of equations‚Äîfew unknowns from
              | the introduction of the trial subspace and few equations from the projection onto the test subspace.
              | An increasing popular approach in model reduction is to choose the test basis such that the resulting
              | reduced-order model minimizes the residual in some norm [31, 89]. Such reduced-order models have
              | been called ‚Äúoptimal‚Äù for a given trial subspace in this particular norm [31]. These ‚Äúoptimal‚Äù or
              | minimum-residual reduced-order models have been extensively studied in [115, 28, 31, 89]. Chapter 4
              | details several properties of minimum-residual reduced-order models, some of which are new, that
              | will be used in Chapters 5‚Äì6 in the construction of optimization methods based on reduced-order
              | models. An important contribution of this work is the extension of the concept of minimum-residual
              | reduced-order models from the primal setting to sensitivity and adjoint PDEs. It will be shown that
              | this approach to compute reduced sensitivities and adjoints will circumvent many difficulties that
              | arise in directly considering the sensitivity or adjoint of a minimum-residual reduced-order model.
              | Furthermore, conditions will be provided under which the minimum-residual sensitivity/adjoint
meta          | CHAPTER 1. INTRODUCTION                                                                              9
blank         | 
              | 
              | 
text          | reduced-order models agree with the sensitivity/adjoint of the primal reduced-order model. These
              | contributions are provided in Chapter 4.
              |    Both the reduced basis method and POD-based methods construct the trial basis from solution
              | snapshots. A number of works have considered snapshots based on other types of information,
              | including sensitivities [87, 86, 32, 85, 52, 210, 198], adjoints [57, 74], unconverged solutions [198],
              | residuals at unconverged solutions [198], and Krylov vectors from the linear system solve that arises
              | at each Newton-Raphson iteration [198]. However, it is well-known that the singular value decom-
              | position underlying POD is sensitive to the relative scaling of the columns in the data matrix and
              | this heterogeneous collection of snapshots should not be carelessly lumped into a single data matrix
              | for compression. That is, when incorporating fundamentally different types of snapshots (entries
              | have different physical units and likely different scales), care must be taken to ensure the resulting
              | decomposition is useful. The work in [32] weighs sensitivities by increment in the parameter to make
              | their units consistent with primal snapshots. A more general approach taken in [74, 210, 198] is to
              | use POD to compress on each type of snapshot individually then concatenate the resulting basis. In
              | [52] a separate basis was constructed for each sensitivity in the construction of a sensitivity ROM,
              | each of which was computed based on POD of snapshots of the corresponding sensitivity. This work
              | builds on the approach in [74, 210, 198] by using POD to build a basis from homogeneous snapshot
              | types and combining the results into a single basis. I further generalize this method to ensure partic-
              | ular snapshots are preserved in the resulting subspace. This will be pivotal in guaranteeing required
              | accuracy at trust region centers when the model reduction technology is combined with the trust
              | region method of Chapter 3 to produce globally convergent, efficient deterministic and stochastic
              | PDE-constrained optimization solvers in Chapters 5‚Äì6.
              |    Finally, partial differential equations that do not possess an affine dependence on their parameters
              | or state vector require an additional level of approximation for online efficiency. This additional
              | approximation, referred to as hyperreduction, is required to reduce the complexity of evaluating
              | nonlinear terms that are not amenable to precomputation [17, 175, 115, 41, 31, 59]. An overview
              | of the most popular hyperreduction methods are provided in Section 4.2.3. Nonlinearities that are
              | polynomial do not strictly require hyperreduction since they are amenable to precomputation and
              | all dependence on the large dimension of the underlying PDE can be confined to the offline phase
              | [149, 14, 16]. However, the approach quickly scales poorly with the size of the reduced-order model
              |                                                                            m+1
              | as the highest polynomial degree increases, e.g., they usually scale as O(ku   ) where ku is the
              | ROM size and m is the polynomial order. Section 4.2.1 provides a detailed formulation of fully
              | discrete PDEs with polynomial nonlinearities in the state and parameter, as well as the details of
              | the precomputation of the monomial terms.
blank         | 
              | 
title         | 1.3.4    Surrogate Methods for PDE-Constrained Optimization
text          | The methods introduced and developed in this thesis fall into the class of surrogate-based optimiza-
              | tion methods, whereby the expensive, high-fidelity model that defines the ‚Äútrue‚Äù objective F (¬µ) and
              | gradient ‚àáF (¬µ) is replaced by an inexpensive approximation. The surrogate models can be based
meta          | CHAPTER 1. INTRODUCTION                                                                              10
blank         | 
              | 
              | 
text          | on response surfaces [63], adaptive spatial discretizations [216], loose tolerances on linear solvers
              | [166, 216], partially converged solutions [62], projection-based reduced-order models [10, 171, 210],
              | and many other approximation models. This section provides a brief overview of methods that
              | use projection-based reduced-order models as a surrogate as they are most relevant to the methods
              | developed in this thesis. For a thorough review of surrogate-based optimization methods, see [63].
              | The methods reviewed in this section fall into two main categories: (1) those that adhere to a strict
              | offline-online decomposition and (2) those that do not. For chronological accuracy, methods that do
              | not distinguish between an offline and online phase are considered first.
              |    Alexandrov developed the Trust Region Model Management (TRMM) framework that uses a
              | general approximation model that satisfies first-order consistency in the context of unconstrained
              | [4] and nonlinearly constrained optimization [6]. The famous Trust Region Proper Orthogonal
              | Decomposition (TRPOD) method [10, 57] was among the first methods to leverage projection-
              | based reduced-order models in a globally convergent optimization algorithm. This method does
              | not exactly fit into Alexandrov‚Äôs TRMM framework as TRPOD uses the Carter condition [35] to
              | define the accuracy required of the reduced-order model to ensure convergence. This condition is
              | considerably weaker than fist-order consistency and allows a relatively small reduced-order model
              | to be used. In the TRPOD method, at the control corresponding to the trust region center, the
              | snapshots are collected from the full-order PDE simulation and compressed using POD. The size of
              | the reduced basis is selected to ensure the Carter condition is satisfied, which involves computing
              | the true gradient error at reduced-order models of increasing size. Later work on TRPOD also
              | collected adjoint snapshots and built a separate POD-based ROM for the adjoint PDE [57]. While
              | this leads to gradients that are not consistent with the quantities of interest computed from the
              | primal reduced-order model, it did not hinder convergence in the numerical experiments in [57].
              | TRPOD was originally developed for unconstrained problems and was later applied to problems with
              | nonlinear constraints [1, 186] following Alexandrov‚Äôs work [6]. A method similar to TRPOD is called
              | Optimality System POD (OS-POD) [113], which attempts to build a reduced-order model at the
              | optimal control. It formulates an optimization problem that consists of the unreduced optimization
              | problem, the reduced optimization problem, and the POD system. The monolithic optimization
              | problem is solved using a simple splitting method that results in a method similar to TRPOD
              | with two main exceptions: a trust region framework is not used to manage the approximation
              | model and OS-POD involves an intermediate step with the true gradient of the objective function.
              | Another surrogate optimization solver similar to TRPOD was developed in [208]. This method used
              | projection-based reduced-order models based on a Krylov-Pade approximation (and therefore specific
              | to linear PDEs) as the approximation model. Two significant contributions of this work are: (1) the
              | flexibility to handle generalized (non-quadratic) trust region constraints and (2) a new method to
              | assess the trust region step without evaluating the HDM. This thesis considers similar generalizations
              | over the standard TRPOD method with the most significant difference being the proposed methods
              | are built on the flexible trust region method of [109]. This flexibility is leveraged to use unconverged
              | solutions as snapshots and in the evaluation of the trust region step. It will also be used later to
meta          | CHAPTER 1. INTRODUCTION                                                                                                      11
blank         | 
              | 
              | 
              | 
text          |                               HDM                                Optimizer
blank         | 
text          |       HDM
              |                              ROB Œ¶
              |                  Compress
              |       HDM
              |                              ROM                                   ROM
              |                                    ROM
              |                                          ROM
blank         | 
              | 
text          |                                                      ROM
              |                                                            ROM
blank         | 
              | 
              | 
              | 
text          |                                                                                    ROM
              |                                                                                          ROM
              |                                                                                                ROM
blank         | 
              | 
text          |                                                                                                            ROM
              |                                                                                                                  ROM
              |               HDM      ROB                     ¬∑¬∑¬∑                   HDM     ROB                     ¬∑¬∑¬∑               ¬∑¬∑¬∑
blank         | 
              | 
text          | Figure 1.1: The adaptive approach to accelerate PDE-constrained optimization with projection-
              | based reduced-order models. Top left: block schematic of the workflow where few High-Dimensional
              | Model (HDM) samples are compressed to build the Reduced-Order Basis (ROB) and the resulting
              | Reduced-Order Model (ROM) is used in the optimization procedure, as long as it maintains accuracy.
              | When the accuracy degrades, an additional sample of the HDM is taken at the new point in the
              | parameter space and the ROB is enriched. Top right: schematic of parameter space (¬µ-space) where
              | the black dot and star are the initial guess and solution of the optimization problem, respectively, the
              | red circles indicate HDM samples, the gray regions are the ‚Äútrust regions‚Äù for the ROM constructed
              | at each iteration, the blue line is the trajectory of the ROM optimization procedure, and the blue star
              | is the optimal solution found by the ROM optimization. Bottom: schematic of the computational
              | cost where the expensive (HDM evaluations and ROB construction) and inexpensive components are
              | intermixed throughout the algorithm. These methods are usually equipped with global convergence
              | theory that guarantee convergence to a local optimum of the PDE-constrained optimization problem,
              | as indicated in the top right plot.
blank         | 
              | 
text          | build a two-level approximation to accelerate stochastic PDE-constrained optimization. All of the
              | methods based on Alexandrov‚Äôs TRMM framework, as well as the other variants described here,
              | are categorized as adaptive optimization procedures‚Äîsee Figure 1.1‚Äîsince the surrogate model is
              | not built once-and-for-all in an offline phase and repeatedly queried in the online phase; rather, the
              | surrogates are adaptively built on-the-fly during the optimization procedure.
              |    In contrast to the method that do not distinguish between offline and online cost are the reduced
              | basis methods that do make such a distinction [172, 174, 114, 125, 52]‚Äîsee Figure 1.2. These
              | methods sample the parameter space in an offline phase to collect snapshots, build a reduced
              | basis, and precompute PDE operators contracted with the reduced basis. In the online phase, the
              | reduced-order model is queried many times as the PDE-constrained optimization problem is solved
              | with the ROM in place of the original PDE. Due to the strict offline-online decomposition, global
              | convergence usually cannot be established. However, since these methods usually consider linear,
              | elliptic PDEs and a quadratic objective function (resulting in a convex optimization problem), error
              | bounds between the computed solution and unique optimum can be derived and computed. As the
meta          | CHAPTER 1. INTRODUCTION                                                                                    12
blank         | 
              | 
              | 
              | 
text          |              HDM
              |                                                      Optimizer
              |              HDM
              |               ..
              |                .                         ROB Œ¶
              |                         Compress
              |              HDM
              |                                                          ROM
              |              HDM
              |                                          Offline
blank         | 
              | 
              | 
              | 
text          |                                                                  ROM
              |                                                                        ROM
              |                                                                              ROM
              |                                                                                    ROM
              |                                                                                          ROM
              |                                                                                                ROM
              |                                                                                                      ROM
              |               HDM      HDM         ¬∑¬∑¬∑     HDM     HDM     ROB
blank         | 
              | 
text          | Figure 1.2: The offline-online approach to accelerate PDE-constrained optimization with projection-
              | based reduced-order models. Top left: block schematic of the workflow where a number of High-
              | Dimensional Model (HDM) samples are compressed to build the Reduced-Order Basis (ROB) in
              | an offline phase; the resulting inexpensive Reduced-Order Model (ROM) is repeatedly queried in
              | the online optimization phase. Top right: schematic of parameter space (¬µ-space) where the black
              | dot and star are the initial guess and solution of the optimization problem, respectively, the red
              | circles indicate HDM samples, the blue line is the trajectory of the ROM optimization procedure,
              | and the blue star is the optimal solution found by the ROM optimization. Bottom: schematic of
              | the computational cost where there is a clear distinction between the expensive components (HDM
              | evaluations and ROB construction) that are done once-and-for-all in the offline phase and the
              | inexpensive components (ROM evaluations) that are repeatedly queried in the online phase. In
              | general, these methods are not guaranteed to converge to a local optimum of the PDE-constrained
              | optimization problem, as indicated in the top right plot.
blank         | 
              | 
text          | assumptions on these methods are too strong for the applications of interest in this thesis, they will
              | not be considered further.
              |    To this point, only methods developed for deterministic PDE-constrained optimization problems
              | have been considered. In the context of PDE-constrained optimization under uncertainty, Kouri [108,
              | 109] used dimension-adaptive sparse grids to define the quadrature nodes in a stochastic collocation
              | method to define an inexpensive surrogate model (due to a quadrature rule with fewer points than
              | would be required by an isotropic sparse grid or tensor product rule). This approximation model was
              | embedded in the trust region method developed in those papers that allows for inexact gradient and
              | objective evaluations. Chen [44, 42, 43] introduced an additional level of approximation by using
              | reduced-order models in addition to sparse grids. This work focused on simple PDEs and employed
              | an offline-online framework (instead of a globally convergent trust region framework that breaks
              | the offline-online decomposition). The method developed in this work for efficient PDE-constrained
              | optimization under uncertainty is a crossover between these two methods: I develop a two-level
              | approximation based on projection-based model reduction and dimension-adaptive sparse grids and
              | embed the approximation model in a globally convergent trust region framework. This enables the
meta          | CHAPTER 1. INTRODUCTION                                                                             13
blank         | 
              | 
              | 
text          | framework to handle general PDEs, leverage the efficiency benefits of reduced-order models, and
              | ensure global convergence; see Chapter 6 for details.
              |    The methods developed in this thesis most resemble TRPOD in that snapshots of the HDM
              | at the trust region center will define the reduced-order model. A crucial difference that leads to
              | improved efficiency and flexibility is that the proposed methods will be built on a more general and
              | flexible trust region theory that permits the use of inexact gradient and objective evaluations and
              | allows for more general trust region constraints. While the present work mostly focuses on problems
              | with a relatively small parameter space compared to the state space, Appendix C uses concepts from
              | linesearch [71, 143] and subspace [54, 119, 137, 143, 207] methods to remove this restriction. Other
              | research that has considered the more difficult case of a large parameter space employs surrogate
              | models with a variable parametrization [168, 167] in the TRMM framework. Other work that
              | applies reduction to the parameter space include [117, 120]; however, these are not embedded in an
              | adaptation algorithm and cannot establish global convergence.
blank         | 
              | 
title         | 1.4     Thesis Accomplishments and Outline
text          | The contributions of this thesis are divided into two primary contributions and two auxiliary contri-
              | butions. The two primary contributions are: (1) the development of an efficient solver for determin-
              | istic PDE-constrained optimization problems that leverages projection-based reduced-order models
              | and partially converged PDE solutions and (2) the development of an efficient solver for stochastic
              | PDE-constrained optimization problems that leverages projection-based reduced-order models and
              | anisotropic sparse grids. The primary contributions were built on two independent auxiliary contri-
              | butions that have applications that extend well beyond the scope of this thesis: (1) the introduction
              | of a globally convergent, generalized trust region method for managing efficient approximation mod-
              | els and (2) the generalization and extension of minimum-residual projection-based reduced-order
              | models [115, 28, 31, 89] to sensitivity and adjoint PDEs.
              |    The proposed multifidelity trust region method extends the trust region method introduced in
              | [109] by allowing a generalized trust region constraint to be used, provided the approximation model
              | and trust region constraint are related by an asymptotic error bound that mirrors the inexact objec-
              | tive condition in [109]. The asymptotic error conditions on the gradient and objective evaluations
              | are identical to those in [109]. It will be shown that the traditional trust region constraint, i.e.,
              | the ball in RN¬µ with center ¬µk , trivially satisfies the required asymptotic relationship and therefore
              | the proposed trust region method exactly reduces to the method in [109] under this choice. Global
              | convergence of the proposed generalized trust region method is established and closely follows the
              | convergence theory in [133, 108, 109]. Unlike traditional trust region methods, the non-quadratic
              | trust region constraint eliminates the possibility of using specialized methods to solve the trust
              | region subproblem that automatically satisfy the fraction of Cauchy decrease [48]. As a result,
              | an interior-point method is outlined to solve the trust region subproblem (an optimization prob-
              | lem with a single nonlinear inequality constraint) exactly. While the method is established in the
meta          | CHAPTER 1. INTRODUCTION                                                                             14
blank         | 
              | 
              | 
text          | unconstrained setting, an augmented Lagrangian approach for extending it to nonlinear equality
              | constraints is detailed. This multifidelity trust region method constitutes one of the pillars of this
              | thesis that will be extensively used throughout. The second pillar is the primary PDE approximation
              | technology employed in this work: projection-based model reduction.
              |    While the concept of projection-based model reduction is not new, this work contributes to the
              | understanding of minimum-residual reduced-order models and extends it to apply to sensitivity
              | and adjoint PDEs. In particular, the concept of a minimum-residual reduced-order model for the
              | fully discrete sensitivity and adjoint PDE is introduced and important properties are established.
              | In particular, conditions are established that guarantee the reduced sensitivity and adjoint models
              | agree with the sensitivity and adjoint of the primal reduced-order model and exactly reconstruct the
              | high-dimensional model counterpart. These properties are crucial when the reduced-order model
              | is embedded into the trust region framework as they will be used to establish the error conditions
              | required for convergence. These minimum-residual sensitivity and adjoint reduced-order models, and
              | the surrounding theory, represent a significant contribution as it will be shown they are significantly
              | easier to implement in a large code-base and compute than the sensitivity and adjoint of the primal
              | reduced-order model.
              |    These two technologies‚Äîthe generalized trust region method and minimum-residual projection-
              | based reduced-order models‚Äîserve as pillars for the primary contributions of the thesis: efficient
              | optimization methods for deterministic and stochastic PDE-constrained optimization. The proposed
              | method for deterministic PDE-constrained optimization uses projection-based reduced-order models
              | as the approximation model in the generalized trust region method and residual-based error indi-
              | cators. For additional efficiency, partially converged primal and sensitivity/adjoint solutions are
              | used as snapshots in the construction of the reduced-order models and partially converged primal
              | solutions are used to evaluate the trust region step. The flexibility of the underlying trust region
              | framework is leveraged to ensure the use of partially converged solutions does not hinder conver-
              | gence. The proposed method for stochastic PDE-constrained optimization employs an additional
              | level of inexactness to efficiently integrate quantities of interest over the stochastic space to form
              | risk measures. This lead to the development of the two-level approximation of risk measures of
              | PDE quantities of interest that uses dimension-adaptive anisotropic sparse grids to perform efficient
              | integration in the stochastic space and model reduction for efficient PDE queries at each colloca-
              | tion node. This approximation is embedded in the multifidelity trust region method and global
              | convergence is established by employing a two-level, dimension-adaptive greedy algorithm to simul-
              | taneously construct the sparse grid and reduced-order basis to satisfy required error conditions. The
              | proposed method directly extends the work in [108, 109] that only defines the approximation model
              | using dimension-adaptive sparse grids with PDE queries at collocation nodes performed using the
              | high-dimensional model. It is also similar to [42, 43] that employs the same two-level approximation,
              | but embeds it in an offline-online framework and claims regarding convergence only apply to simple
              | PDEs.
              |    This thesis is organized as follows (Figure 1.3). Chapter 2 provides necessary background on
meta          | CHAPTER 1. INTRODUCTION                                                                                       15
blank         | 
              | 
              | 
text          |                                                Partially Converged
              |                                                                                 Residual-Based Error Bounds
              |                                                  PDE Solutions
              |                                                                                        (Appendix B)
              |                                                    (Chapter 5)
blank         | 
              | 
              | 
              | 
text          |  Deterministic PDE Optimization              Deterministic PDE Op-                 Stochastic PDE Opti-
              |  with ROMs ‚Äì Many Parameters                 timization with ROMs                 mization with SG-ROMs
              |          (Appendix C)                             (Chapter 5)                           (Chapter 6)
blank         | 
              | 
              | 
              | 
text          |                                   Multifidelity Trust-       Projection-Based
              |  Subspace Optimization Methods                                                    Anisotropic Sparse Grids
              |                                    Region Method             Model Reduction
              |          (Appendix C)                                                                   (Chapter 6)
              |                                      (Chapter 3)               (Chapter 4)
blank         | 
              | 
text          |                                                Trust Region Global              High-Order, Time-Dependent
              |                                                Convergence Theory                    PDE Optimization
              |                                                   (Appendix A)                         (Appendix D)
blank         | 
              | 
text          |                                    Figure 1.3: Organization of thesis
blank         | 
              | 
text          | partial differential equations, their discretization, and PDE-constrained optimization. Chapter 3
              | discusses some necessary elements of optimization theory and introduces the proposed generalized
              | trust region method for leveraging approximation models in an optimization setting. The con-
              | vergence proof for the proposed method is provided in Appendix A. This method will serve as a
              | cornerstone for the efficient solvers developed in Chapters 5 and 6 for deterministic and stochas-
              | tic PDE-constrained optimization. Chapter 4 introduces projection-based model reduction‚Äîthe
              | approximation models that will eventually define the trust region subproblems‚Äîincluding some
              | novel contributions pertaining to minimum-residual sensitivity and adjoint reduced-order models.
              | Chapter 5 introduces one of the primary contributions of this thesis: the use projection-based
              | reduced-order models in the generalized trust region framework to yield an efficient solver for de-
              | terministic PDE-constrained optimization problems. The potential of the method is demonstrated
              | on a number of problems in computational mechanics, including a large-scale industrial examples
              | of aerodynamic shape design of a full aircraft configuration. The second primary contribution of
              | this thesis is presented in Chapter 6: the extension of the method in Chapter 5 to handle stochastic
              | PDE-constrained optimization problems where an additional level of inexactness is introduced into
              | the approximation model through the use of anisotropic sparse grids to efficiently integrate risk
              | measures of PDE quantities of interest over the stochastic space. While the methods introduced in
              | these chapters implicitly assume the number of parameters is small in comparison to the size of the
              | PDE discretization, Appendix C develops a method to generalize these algorithms to the case where
              | the number of parameters and state variables are comparable. Finally, Chapter 7 offers conclusions
              | and ideas for future research and Appendix D introduces an adjoint method for optimization of
              | time-dependent PDEs, possibly with periodicity constraints, discretized with high-order methods.
title         | Chapter 2
blank         | 
title         | PDE-Constrained Optimization
blank         | 
text          | This chapter provides an overview of parametrized partial differential equations and PDE-constrained
              | optimization that will be used extensively in the remainder of this document. The focus is primarily
              | on static, nonlinear PDEs with either deterministic or stochastic coefficients and boundary condi-
              | tions. The various elliptic and hyperbolic PDEs encountered in this document are also introduced,
              | which include: the 1D viscous and inviscid Burgers‚Äô equation, linear elasticity, the total Lagrangian
              | form of the finite deformation continuum equations, the compressible Euler equations, and the
              | compressible Navier-Stokes equations. The chapter concludes with relevant concepts pertaining to
              | PDE-constrained optimization including: the continuous and discrete version of the optimization
              | problem, full-space and reduced-space solvers, gradient computations in the reduced-space approach
              | via the sensitivity and adjoint method, and approaches to handle side constraints, i.e., optimization
              | constraints other than the PDE constraint itself.
blank         | 
              | 
title         | 2.1     Parametrized Partial Differential Equations
text          | Consider a system of partial differential equations of the form: find U such that
blank         | 
text          |                             ‚àÇU
              |                                + G(U, ‚àáU ) = g(x, t)       x ‚àà B,   t‚ààT
              |                             ‚àÇt
              |                                 H(U, ‚àáU ) = h(x, t)        x ‚àà ‚àÇB, t ‚àà T                        (2.1)
blank         | 
text          |                                     U (x, t0 ) = U0 (x)    x‚ààB
blank         | 
text          | where T = (t0 , tf ) ‚äÇ R+ is the temporal domain, B ‚äÇ Rnsd is the spatial domain with boundary ‚àÇB,
              | U (x, t) is the unknown state vector with nc components, G and H are first-order spatial differential
              | operators, g and h are volumetric and boundary source terms, and U0 : B ‚Üí Rnsv is the initial
              | data. In the most general case, the domain B can be time-dependent, leading to rigid and deforming
              | domain problems. In such settings, an arbitrary Lagrangian-Eulerian description of the PDE can
              | be employed to transform the equations to a fixed domain; see Appendix D for additional details.
blank         | 
              | 
              | 
meta          |                                                   16
              | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                         17
blank         | 
              | 
              | 
text          |    From the solution of the partial differential equation (U ), relevant Quantities of Interest (QoIs)1
              | are defined as space-time integrals of various solution-dependent quantities over the domain. Quan-
              | tities of interest are essential from a practical perspective as they provide metrics to quantify the
              | performance and behavior of the system under consideration. In this work, QoIs will take the form
              |                                 Z Z                   Z Z
              |                       F(U ) =         fB (U ) dV dt +      f‚àÇB (U ) dA dt,                    (2.2)
              |                                        T   B                   T   ‚àÇB
blank         | 
              | 
text          | where fB and f‚àÇB are relevant pointwise quantities. The form in (2.2) is general in that it encom-
              | passes integrals over subsets of the spatial and temporal domains, as well as pointwise quantities at
              | fixed spatial locations or times. This results from the lack of regularity imposed on fB and f‚àÇB that
              | allows for the use of indicator or Dirac functions.
              |    In the remainder of this document, the primary interest will be in the behavior of solutions
              | (U ) and QoIs (F(U )) of the PDE under perturbations to data of the problem‚Äîthe domain (B)
              | and boundary (‚àÇB), source terms (g and h), initial condition (U0 ), or coefficients defining the
              | differential operators G and H. In subsequent sections, these parameters of the partial differential
              | equation will be the optimization variables whose values will be sought such that the objective QoI is
              | minimized and other QoI-based constraints are satisfied. Before proceeding to the discussion of PDE-
              | constrained optimization, the various PDEs considered in this document are introduced and details
              | regarding the discretization of parametrized partial differential equations and the corresponding
              | quantities of interest are discussed.
blank         | 
              | 
title         | 2.1.1     Examples
text          | This section provides specific examples of partial differential equations (2.1) and quantities of interest
              | (2.2) that will be encountered in this thesis. While the examples are mostly from the fields of solid
              | and fluid mechanics, this is not a fundamental restriction in any of the subsequent developments.
blank         | 
title         | Linear Elasticity
blank         | 
text          | Consider a solid body B ‚äÇ Rnsd subject to distributed body forces b(x, t) with boundary ‚àÇB de-
              | composed into two parts: ‚àÇBu and ‚àÇBt such that ‚àÇB = ‚àÇBu ‚à™ ‚àÇBt . Displacements are prescribed
              | along ‚àÇBu and ‚àÇBt is subject to prescribed traction forces. Under the assumption that the result-
              | ing deformations are infinitesimal and the pointwise stress and strain are related through a linear
              | relationship, the deformation of the body is governed by the following system of partial differential
              | equations
              |                                        œÅuÃà = ‚àá ¬∑ œÉ + b      x ‚àà B, t ‚àà T
              |                                         u = uÃÑ              x ‚àà ‚àÇBu , t ‚àà T                                  (2.3)
              |                                     œÉ ¬∑ n = tÃÑ              x ‚àà ‚àÇBt , t ‚àà T ,
              |   1 Quantity of Interest (singular) will be abbreviated QoI and Quantities of Interest (plural) will be abbreviated
blank         | 
meta          | QoIs.
              | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                  18
blank         | 
              | 
              | 
text          | where u(x, t) ‚àà Rnsd is the pointwise deformation and state vector of the PDE, œÉ(x, t) ‚àà Rnsd √ónsd
              | is the symmetric stress tensor, œÅ(x, t) ‚àà R+ is the density of the material that comprises B, b(x, t) ‚àà
              | Rnsd is the body force, uÃÑ(x, t) ‚àà Rnsd is the prescribed displacement on ‚àÇBu , tÃÑ(x, t) ‚àà Rnsd is the
              | prescribed traction on ‚àÇBt , and n(x) ‚àà Rnsd is the pointwise outward normal to the boundary. The
              | system of PDEs is closed with the stress-strain relationship (Hooke‚Äôs law)
blank         | 
text          |                                                 œÉ = C : ,                                            (2.4)
blank         | 
text          | where C ‚àà Rnsd √ónsd √ónsd √ónsd is the elasticity tensor with major and minor symmetry and (x, t) ‚àà
              | Rnsd √ónsd is the strain tensor. The kinematic constraint relates the deformation to strain
blank         | 
text          |                                                 1
              |                                                   ‚àáu + ‚àáuT .
blank         |                                                           
text          |                                            =                                                         (2.5)
              |                                                 2
blank         | 
text          | Remark. The system in (2.3) does not strictly fit into the form in (2.1) due to presence of the
              | second-order temporal derivative, i.e., the inertial term. This can be remedied by introducing the
              | velocity v = uÃá and defining the state vector U = (u, v). This will not preserve the structure of the
              | governing equations and they are usually treated directly in their second-order form.
blank         | 
text          |    There are number of relevant quantities of interest in linear elasticity including: (1) pointwise
              | displacement magnitude, (2) pointwise stress measures, (3) mass/volume, and (4) global stiffness,
              | to name a few. The volume of the structure and its global stiffness are defined as
              |                        Z                           Z              Z
              |                    V =    dV       and     S(u) =     uk bk dV +      uk tÃÑk dA,                      (2.6)
              |                              B                            B              ‚àÇB
blank         | 
text          | respectively, where summation from 1 to nsd over repeated indices is implied. The volume is a
              | purely geometric quantity of interest as it does not depend on the solution of the partial differential
              | equation.
blank         | 
title         | Finite Deformation Continuum Mechanics
blank         | 
text          | The system of partial differential equations that governs the physical setup of the previous section
              | in the general case, that is, without the linearity assumption, is
blank         | 
text          |                                     œÅuÃà = ‚àá ¬∑ P + b      X ‚àà B, t ‚àà T
              |                                       u = uÃÑ             X ‚àà ‚àÇBu , t ‚àà T                              (2.7)
              |                                  P ¬∑ N = tÃÑ              X ‚àà ‚àÇBt , t ‚àà T
blank         | 
text          | where œÅ, u, b, uÃÑ, tÃÑ are defined in the previous section on linear elasticity, P (X, t) is the first Piola-
              | Kirchhoff stress tensor, and N (X) is the pointwise outward normal to the boundary in the reference
              | configuration (B). The equations are closed with a general constitutive relationship
blank         | 
text          |                                                 P = P (F ),                                           (2.8)
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                                    19
blank         | 
              | 
              | 
text          |                 ‚àÇu
              | where F = I +       is the deformation gradient. The governing equations in (2.7), posed on the
              |                 ‚àÇX
              | reference or undeformed configuration (B), are called the total Lagrangian form [19]. The equations
              | can be transformed to the current or physical configuration using the diffeomorphism: x(X, t) :=
              | X + u(X, t), but the so-called updated Lagrangian form will not be considered in this document.
              | The quantities of interest from the previous section (2.6) will also be used here.
blank         | 
title         | General Conservation Laws
blank         | 
text          | The next sequence of partial differential equations considered take the form viscous or inviscid
              | conservation laws
blank         | 
text          |                              ‚àÇU
              |                                 + ‚àá ¬∑ FI (U ) + ‚àá ¬∑ FV (U, ‚àáU ) = g(x, t)                 x‚ààB                            (2.9)
              |                              ‚àÇt
blank         | 
text          | where FI is the inviscid flux, FV is the viscous flux, and g is a source term. Hyperbolic systems
              | of partial differential equations of this form describe propagation phenomena such as those in fluid
              | dynamics and electromagnetics.
blank         | 
title         | 1D Inviscid Burgers‚Äô Equation
blank         | 
text          | The first and simplest conservation law considered is the 1D inviscid Burgers‚Äô equation with an
              | inflow boundary condition, which describes shock propagation of a conserved variable, u
blank         | 
text          |                                   ‚àÇu      ‚àÇu
              |                                      +u        = g(x, t)         x ‚àà (xl , xr ), t ‚àà (t0 , tf )
              |                                   ‚àÇt      ‚àÇx                                                                            (2.10)
              |                                      u(xl , t) = h(t)            t ‚àà (t0 , tf ).
blank         | 
text          | This constitutes a conservation law of the form (2.9) where the conserved variable, inviscid flux, and
              | viscous flux are
              |                                                                 u2
              |                              U := u                FI (U ) :=                 FV (U, ‚àáU ) := 0.
              |                                                                 2
              | Two quantities of interest for Burgers‚Äô equation are: the regularized tracking-type functional and
              | the amount of the conserved variable that exits the domain through the outflow boundary
blank         | 
text          |                              tf       xr                                                           tf
              |                      1
              |                          Z        Z                                                          Z
              |                                             (u ‚àí uÃÑ)2 + Œ±g 2 dx dt
blank         |                                                            
text          |            T (u) =                                                        and      R(u) =               u(xr , t) dt,
              |                      2   t0       xl                                                              t0
blank         | 
              | 
text          | respectively, where uÃÑ is a target state and Œ± > 0 is a prescribed regularization constant.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                     20
blank         | 
              | 
              | 
title         | Compressible Navier-Stokes Equations
blank         | 
text          | The compressible Navier-Stokes equations govern viscous fluid flow in a domain B and take the form
blank         | 
text          |                                            ‚àÇœÅ      ‚àÇ
              |                                               +       (œÅui ) = 0,                                        (2.11)
              |                                            ‚àÇt     ‚àÇxi
              |                               ‚àÇ             ‚àÇ                     ‚àÇœÑij
              |                                  (œÅui ) +     (œÅui uj + p) = +           for i = 1, 2, 3,                (2.12)
              |                               ‚àÇt          ‚àÇxi                     ‚àÇxj
              |                             ‚àÇ            ‚àÇ                        ‚àÇqj      ‚àÇ
              |                                (œÅE) +        (uj (œÅE + p)) = ‚àí         +      (uj œÑij ),                 (2.13)
              |                             ‚àÇt          ‚àÇxi                       ‚àÇxj     ‚àÇxj
blank         | 
text          | where œÅ is the fluid density, u1 , u2 , u3 are the velocity components, and E is the total energy. The
              | viscous stress tensor and heat flux are given by
blank         |                                                                                             
text          |                    ‚àÇui   ‚àÇuj   2 ‚àÇuk                                    ¬µ ‚àÇ            p 1
              |      œÑij = ¬µ           +     ‚àí       Œ¥ij            and          qj = ‚àí             E + ‚àí uk uk .        (2.14)
              |                    ‚àÇxj   ‚àÇxi   3 ‚àÇxk                                    Pr ‚àÇxj         œÅ 2
blank         | 
text          | Here, ¬µ is the viscosity coefficient and Pr = 0.72 is the Prandtl number which we assume to be
              | constant. For an ideal gas, the pressure p has the form
blank         |                                                               
text          |                                                          1
              |                                          p = (Œ≥ ‚àí 1)œÅ E ‚àí uk uk ,                                        (2.15)
              |                                                          2
blank         | 
text          | where Œ≥ is the adiabatic gas constant. All walls have no-slip boundary conditions, i.e., ui = 0 for
              | i = 1, 2, 3. Equations (2.11)-(2.13) can be written in conservation form as
              |                         Ô£Æ       Ô£π                      Ô£Æ                                             Ô£π
              |                             œÅ                                  œÅu1           œÅu2               œÅu3
              |                         Ô£Ø     Ô£∫                      Ô£Ø                                 Ô£∫
              |                         Ô£ØœÅu1 Ô£∫                       Ô£Ø p + œÅu2     œÅu1 u2     œÅu1 u3 Ô£∫
              |                         Ô£Ø     Ô£∫                      Ô£Ø        1                        Ô£∫
              |                     U = Ô£ØœÅu2 Ô£∫             FI (U ) = Ô£Ø œÅu1 u2     p + œÅu22    œÅu2 u3 Ô£∫                   (2.16)
              |                         Ô£Ø     Ô£∫                      Ô£Ø                                 Ô£∫
              |                         Ô£Ø     Ô£∫                      Ô£Ø                                 Ô£∫
              |                         Ô£ØœÅu Ô£∫                        Ô£Ø œÅu u                         2 Ô£∫
              |                         Ô£∞   3 Ô£ª                      Ô£∞     1 3     œÅu 2 u 3  p  + œÅu3  Ô£ª
              |                           œÅE                          u1 (E + p) u2 (E + p) u3 (E + p)
blank         | 
text          |                                             Ô£Æ                                              Ô£π
              |                                                    0                 0             0
              |                                           Ô£Ø                                                Ô£∫
              |                                           Ô£Ø ‚àíœÑ11                 ‚àíœÑ21          ‚àíœÑ31        Ô£∫
              |                                           Ô£Ø                                                Ô£∫
              |                             FV (U, ‚àáU ) = Ô£Ø ‚àíœÑ12                 ‚àíœÑ22          ‚àíœÑ32        Ô£∫.            (2.17)
              |                                           Ô£Ø                                                Ô£∫
              |                                           Ô£Ø                                                Ô£∫
              |                                           Ô£Ø ‚àíœÑ                   ‚àíœÑ23          ‚àíœÑ33        Ô£∫
              |                                           Ô£∞     13                                         Ô£ª
              |                                            q1 ‚àí ui œÑi1         q2 ‚àí ui œÑi2   q3 ‚àí ui œÑi3
blank         | 
text          | While there are a plethora of quantities of interest in fluid dynamics, the most relevant quantities
              | tend to be time-averaged integrated forces and moments on surfaces, particularly in aerodynamics
              | applications. The time-averaged force in the ith direction on a surface ‚àÇBw takes the form
blank         | 
text          |                                  1
              |                                     Z Z
              |                           Fi =              (p + œÅui uj nj ‚àí œÑji nj ) dA dt.                             (2.18)
              |                                |T | T ‚àÇBw
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             21
blank         | 
              | 
              | 
title         | Compressible Euler Equations
blank         | 
title         | The compressible Euler equations
blank         | 
text          |                                             ‚àÇœÅ      ‚àÇ
              |                                                +       (œÅui ) = 0,                               (2.19)
              |                                             ‚àÇt     ‚àÇxi
              |                                ‚àÇ             ‚àÇ
              |                                   (œÅui ) +     (œÅui uj + p) = 0 for i = 1, 2, 3,                 (2.20)
              |                                ‚àÇt          ‚àÇxi
              |                              ‚àÇ            ‚àÇ
              |                                 (œÅE) +        (uj (œÅE + p)) = 0                                  (2.21)
              |                              ‚àÇt          ‚àÇxi
blank         | 
text          | model an inviscid fluid. The Navier-Stokes equations in (2.11)-(2.13) reduce to the compressible
              | Euler equations above in the limit of no viscosity, i.e., ¬µ ‚Üí 0. The conservation form is identical
              | to the Navier-Stokes case with FV (U, ‚àáU ) := 0 and the time-averaged force on a surface ‚àÇBw are
              | defined as
              |                                              1
              |                                                    Z Z
              |                                      Fi =                    (p + œÅui uj nj ) dA.                (2.22)
              |                                             |T |   T   ‚àÇBw
blank         | 
              | 
title         | Compressible Navier-Stokes Equations‚ÄîIsentropic Assumption
blank         | 
text          | In situations where the entropy in the system is constant, i.e., adiabatic and reversible, the Navier-
              | Stokes equations can be simplified to its isentropic form. For a perfect gas, the entropy is defined
              | as
              |                                              s = p/œÅŒ≥ = constant,                                (2.23)
blank         | 
text          | which explicitly relates the pressure and density of the flow, rendering the energy equation redundant
              | and leads to
blank         | 
text          |                                      ‚àÇœÅ     ‚àÇ
              |                                          +      (œÅui ) = 0,                                      (2.24)
              |                                      ‚àÇt    ‚àÇxi
              |                          ‚àÇ            ‚àÇ                     ‚àÇœÑij
              |                             (œÅui ) +     (œÅui uj + p) = +                 for i = 1, 2, 3.       (2.25)
              |                          ‚àÇt          ‚àÇxi                    ‚àÇxj
blank         | 
text          | This effectively reduces the square system of PDEs of size nsd + 2 to one of size nsd + 1. It can
              | be shown, under suitable assumptions, that the solution of the isentropic approximation of the
              | Navier-Stokes equations converges to the solution of the incompressible Navier-Stokes equations as
              | the Mach number approaches zero [118, 51, 64]. In conservation form, the compressible, isentropic
              | Navier-Stokes equations are
              |                          Ô£Æ       Ô£π                           Ô£Æ                               Ô£π
              |                              œÅ                                   œÅu1        œÅu2        œÅu3
              |                                                          Ô£Øp + œÅu21
              |                        Ô£Ø    Ô£∫                            Ô£Ø                                   Ô£∫
              |                        Ô£ØœÅu1 Ô£∫                                             œÅu1 u2     œÅu1 u3 Ô£∫
              |                     U =Ô£Ø    Ô£∫                  FI (U ) = Ô£Ø                                   Ô£∫   (2.26)
              |                        Ô£ØœÅu Ô£∫
              |                        Ô£∞ 2Ô£ª
              |                                                          Ô£Ø œÅu u
              |                                                          Ô£∞ 1 2           p + œÅu22    œÅu2 u3 Ô£∫Ô£ª
              |                         œÅu3                                œÅu1 u3         œÅu2 u3    p + œÅu23
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                22
blank         | 
              | 
              | 
text          |                                                 Ô£Æ                        Ô£π
              |                                                     0      0         0
              |                                                Ô£Ø‚àíœÑ11     ‚àíœÑ21      ‚àíœÑ31 Ô£∫
              |                                                Ô£Ø                        Ô£∫
              |                                  FV (U, ‚àáU ) = Ô£Ø
              |                                                Ô£Ø‚àíœÑ
              |                                                                         Ô£∫.                         (2.27)
              |                                                Ô£∞ 12      ‚àíœÑ22      ‚àíœÑ32 Ô£∫
              |                                                                         Ô£ª
              |                                                 ‚àíœÑ13     ‚àíœÑ23      ‚àíœÑ33
blank         | 
text          | and the time-averaged, integrated forces are computed according to (2.18).
blank         | 
              | 
title         | 2.1.2     Discretization: Parametrization
text          | The primary interest in this document is not the study of partial differential equations themselves,
              | rather the behavior of PDE solutions (U ) and QoIs (F(U )) under perturbations to the PDE itself,
              | e.g., the domain (B), source terms (g and h), and coefficients of the differential operators (usually
              | manifest as material properties in physical problems). This will lead naturally to the discussion of
              | optimization in the next section where we seek to find the PDE domain, source term, and coefficients
              | that minimizes some QoI and meets performance constraints on other QoIs.
              |    In general, the quantities defining the PDE lie in infinite-dimensional function spaces, which are
              | not convenient or practical to work with in a computational setting. Furthermore, it is difficult to
              | design practical and relevant perturbation strategies in these spaces that will be useful in engineering
              | and scientific applications. Accordingly, the remainder of this section discusses the finite-dimensional
              | parametrization of the partial differential equation in (2.1). This will entail the definition of a vector
              | of N¬µ parameters, ¬µ ‚àà RN¬µ , and a precise description of the dependence of the PDE on ¬µ. In
              | general, the parameters can be decomposed as ¬µ = (¬µB , ¬µg , ¬µh , ¬µG , ¬µH , ¬µU0 ), where
blank         | 
text          |                            B = B(¬µB )                           U0 (x) = U0 (x, ¬µU0 )
              |                       g(x, t) = g(x, t, ¬µg )                   h(x, t) = h(x, t, ¬µh )              (2.28)
              |                   G(U, ‚àáU ) = G(U, ‚àáU, ¬µG )              H(U, ‚àáU ) = H(U, ‚àáU, ¬µH ).
blank         | 
text          | All quantities are assumed to be continuously differentiable with respect to their respective param-
              | eters. This level of granularity is not significant for this document, but must be exploited in a
              | computational setting for an efficient implementation. Therefore, only the monolithic vector ¬µ is
              | considered and the PDE dependence on this parameter takes the form
blank         | 
text          |                              B = B(¬µ)                           U0 (x) = U0 (x, ¬µ)
              |                        g(x, t) = g(x, t, ¬µ)                    h(x, t) = h(x, t, ¬µ)                (2.29)
              |                     G(U, ‚àáU ) = G(U, ‚àáU, ¬µ)              H(U, ‚àáU ) = H(U, ‚àáU, ¬µ).
blank         | 
text          | The various quantities in (2.28)-(2.29) are fundamentally different and specialized techniques have
              | been developed to parametrize each. In the remainder of this section, a few techniques are discussed
              | that are relevant to the shape and topology parametrization of B and parametrization of space-time
              | functions such as the source terms and initial conditions, as they will be most relevant to problems
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              23
blank         | 
              | 
              | 
text          | encountered in subsequent chapters.
blank         | 
title         | Parametrization of spatial functions
blank         | 
text          | The first type of operator that arises in PDE applications, particularly in the context of optimal
              | and distributed control [214, 190], that requires parametrization are spatial functions such as the
              | source terms in (2.1). This section seeks to define a parameter vector ¬µ such that the set {g(x, ¬µ) |
              | ¬µ ‚àà RN¬µ } includes a relevant set of scalar-valued functions with some level of regularity for a given
              | application. Only scalar-valued functions will be considered in this section as vector-valued function
              | can be parametrized through the parametrization each component‚Äîwith either the same or different
              | parameters for each.
              |    Local interpolation is a general strategy for parametrizing spatial functions in nsd dimensions.
              | In this setting, the domain B is decomposed into elements of standard shapes‚Äîsuch as simplices
              | or hyper-rectangles‚Äîand a set of polynomials of a given degree are introduced over each element.
              | The coefficients of each polynomial in the discretization of B comprise the parameter vector and the
              | parametrized spatial function takes the form
blank         | 
text          |                                                     N¬µ
              |                                                     X
              |                                         g(x, ¬µ) =         ¬µI NI (x)                              (2.30)
              |                                                     I=1
blank         | 
              | 
text          | where NI are the shape functions and ¬µI are the components of ¬µ. While a parametrization of this
              | form can be applied in any number of spatial dimensions and has a built-in refinement mechanism (by
              | subdividing the elements in the discretization and defining polynomials over the new elements), it
              | can lead to large parameter vectors, i.e., N¬µ  1. An additional benefit of such a parametrization is
              | the use of an unstructured discretization of B to refine regions where increased resolution is required,
              | if such information is known.
              |    On the other end of the spectrum lie global interpolation methods where the interpolant is
              | defined based on information from the entire domain B or parameter vector ¬µ. Cubic splines in one
              | dimension fall into this category‚Äîthe parameter vector defines the value of the spline at ‚Äúknots‚Äù
              | and an interpolant is constructed that passes through these values and satisfies boundary conditions.
              | In higher dimensions, radial basis functions [27] serve a similar purpose.
blank         | 
title         | Parmetrization of spatio-temporal functions
blank         | 
text          | In time-dependent applications, spatio-temporal functions are often used to describe source terms,
              | boundary conditions, or even domain deformations. To optimize over these types of terms, they must
              | be parametrized with a finite number of parameters. One option is to consider the domain B √ó T
              | as a domain in nsd + 1 dimensions and apply the local interpolation method of the previous section.
              | Alternatively, the spatio-temporal function can be defined and parametrized using a separation of
              | variables approach
              |                                     g(x, t, ¬µ) = gs (x, ¬µ)gt (t, ¬µ),                             (2.31)
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                               24
blank         | 
              | 
              | 
text          | where gs is a parametrized spatial function in Rnsd and gt is a parametrized univariate function. Any
              | of the methods discussed in the previous section can be employed to parametrize gs and gt . This
              | approach enables certain spatial or temporal requirements‚Äîsuch as periodicity‚Äîto be explicitly
              | enforced in the parametrization of g(x, t, ¬µ) through the selection of gs and gt . For example, if gt (t)
              | is taken as a periodic function of period T , then g( ¬∑ , t, ¬∑ ) is guaranteed to be periodic with period
              | T.
blank         | 
title         | Shape parametrization of domain, B
blank         | 
text          | Parametrization of the shape (at a fixed topology) of two- and three-dimensional objects with a finite
              | number of intuitive parameters is essential in computer graphics as well as a number of engineering
              | disciplines, usually in the context of design. A plethora of shape parametrization techniques exist
              | [99, 177, 9, 61], each with strengths and weaknesses. These methods can be divided into two
              | distinct classes: (1) those that parametrize B directly and (2) those that parametrize the boundary
              | ‚àÇB := B \ B and extend the deformation to the interior‚Äîusually by solving an auxiliary PDE.
              |      Methods that parametrize B directly define an analytical mapping
blank         | 
text          |                                         œï : Rnsd √ó RN¬µ ‚Üí Rnsd                                     (2.32)
blank         | 
text          |                                                        0                   0
              | that maps the reference domain B to the new shape B , i.e., œï(B, ¬µ) = B . These methods are usually
              | easily parallelized as they involve local operations, i.e., given ¬µ, any subset v ‚äÇ B gets transformed
              |                                                                                          0
              | as œï(v, ¬µ) independent of the action of œï on B \ v. Smoothness of the new shape, B is guaranteed
              | from the smoothness of the original shape and mapping. In many cases, a mapping of the form (2.32)
              | can be defined analytically given a geometry of interest and requirements of the parametrization.
              | For example, the camber of the NACA0012 airfoil in Figure 2.1 can be parametrized with three
              | parameters using a Gaussian of the form
blank         | 
text          |                                                                     2
              |                                       œï(X, ¬µ) = ¬µ1 e‚àí¬µ2 (X1 ‚àí¬µ3 )                                 (2.33)
blank         | 
text          | where ¬µ1 , ¬µ2 , ¬µ3 control the magnitude, sharpness, and center of the camber, respectively; see
              | the shape corresponding to ¬µ1 = 0.2, ¬µ2 = 2.0, ¬µ3 = 0.0 in Figure 2.1. While this approach is
              | trivial to parallelize and can lead to highly intuitive parameters, it can be cumbersome for complex
              | 3D geometries and requires considerable expertise in designing parameters. Another approach for
              | parametrizing B directly that is extremely popular in the computer graphics community is known
              | as Free Form Deformation (FFD) [179]. In this method, a nsd -dimensional lattice of control points
              | define an analytic function on the interior of the lattice, which can be extended to the entire space.
              | In this setting, the displacement of the control nodes of the lattice are the parameters, which induce
              | a deformation on the volume enclosed by the lattice and thus any body embedded in it. Figure 2.2
              | shows a circle parametrized with FFD based on B-splines, including the undeformed and deformed
              | geometry and FFD lattice. While FFD is more general and flexible than manual parametrization
              | and nearly as parallelizable, it may quickly lead to a large number of parameters, which may lead to
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                               25
blank         | 
              | 
              | 
              | 
text          | Figure 2.1: Left: Undeformed NACA0012 airfoil and surrounding triangular mesh. Right: Deforma-
              | tion of R2 according to mapping œï in (2.33) that deforms the NACA0012 geometry and surrounding
              | mesh.
blank         | 
              | 
text          | slower convergence in the context of optimization. When gradient-based optimization techniques are
              | employed, this trade-off is usually worthwhile, particularly in aerodynamic applications. The number
              | of parameters may be reduced by combining the manual parametrization with FFD techniques, that
              | is, introduce a FFD lattice to control the underlying geometry and a manual parametrization that
              | controls the FFD lattice nodes. Figures 2.3 shows the parametrization of a model of a Volkswagen
              | Passat using FFD with two relevant and intuitive shape parameters‚Äîthe height of the roof and
              | taper of the trunk. Figure 2.4 shows the parametrization of the Common Research Model (CRM)
              | geometry with one intuitive parameter‚Äîthe dihedral of the wing.
              |    The other class shape parametrization methods defines a parametrization of the boundary ‚àÇB and
              | propagates the deformation to the interior B, usually via the solution of a partial differential equation
              | such as linear or nonlinear elasticity with prescribed displacement on ‚àÇB [58, 155]. Analytical
              | methods such as splines (nsd = 2) or Non-Uniform Rational B-Splines (NURBS) patches (nsd = 3)
              | are commonly used for the surface parametrization. Another popular method uses the design element
              | concept where a finite element mesh is defined such that it encloses the geometry of interest, ‚àÇB,
              | and the finite element shape functions define the deformation of the enclosed volume2 . Figure 2.5
              | provides an example of a NACA0012 airfoil parametrized with a single cubic design element.
              |    All of the parametrization methods considered in this section are useful in parametrizing the
              | shape of an object with a fixed topology. Methods for parametrizing the topology of a domain will
              | be discussed in the next section‚Äîthey are fundamentally different and inevitably lead to a large
              | number of parameters, N¬µ  1.
blank         | 
title         | Topology parametrization of domain, B
blank         | 
text          | Two prevailing methods are available for parametrizing the topology of a domain, B: (1) density-
              | based methods [181, 20] and (2) level set methods [192]. Density methods define the topology of the
              | domain using an indicator function
              |                                                 œá : Rnsd ‚Üí {0, 1},                                (2.34)
              |   2 The   design element concept can also be used to directly parametrize B.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                          26
blank         | 
              | 
              | 
              | 
text          | Figure 2.2: Top left: Undeformed geometry of a circle (blue) and a FFD lattice (gray). Top center :
              | Perturbation of FFD control nodes according to an x-directed elongation mode and resulting shape
              | of the circle. Top right: Perturbation of FFD control nodes according to a bending mode and
              | resulting shape of the circle. Bottom: Local perturbations to individual FFD control nodes in the y
              | direction and the resulting shape of the circle.
blank         | 
              | 
text          | where œá(x) = 1 if x ‚àà B and œá(x) = 0 otherwise. The topology of B is then parametrized by
              | parametrizing the function œá using any of the methods previously discussed. The most common
              | approach to parametrize œá is to partition a subset of Rnsd into N¬µ elements or patches of finite
              | volume and define œá to be constant within each element k with value ¬µk ‚àà {0, 1}3 . Figures 2.6 ‚Äì
              | 2.9 show the topology of a cantilever, cube, and lacrosse head parametrized with a density-based
              | approach that uses a constant value of œá in each element. This approach has the advantage of a
              | simple implementation, but smooth topologies can only be obtained if an extremely large number
              | of elements are used, i.e., N¬µ  1.
              |    Conversely, level set methods define the topology implicitly by identifying all surfaces or interfaces
              | as the zero level-set of an implicit function,
blank         | 
text          |                                                      œÜ : Rnsd ‚Üí R,                                            (2.35)
blank         | 
text          | where B = {x ‚àà Rnsd | œÜ(x) ‚â§ 0} and ‚àÇB = {x ‚àà Rnsd | œÜ(x) = 0}. The parametrization
              | of the spatial function œÜ using any of the techniques previously discussed leads to the topology
              | parametrization.
blank         | 
              | 
title         | 2.1.3        Discretization: Governing Equations
text          | With the techniques described in the previous section, the parametrization of the PDE can be
              | encoded in the finite-dimensional vector ¬µ ‚àà RN¬µ that contains all types of parameters considered.
              |   3 It   is often necessary to relax the range of ¬µk to [0, 1] to obtain a continuous optimization problem.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              27
blank         | 
              | 
              | 
              | 
text          | Figure 2.3: Free form deformation lattices and Volkswagen Passat geometry: (left) undeformed
              | configuration, (top right) deformed configuration with lowered roof, and (bottom right) deformed
              | configuration with steeply tapered trunk.
blank         | 
              | 
              | 
              | 
text          | Figure 2.4: Free form deformation lattice and Common Research Model (CRM) geometry: (left)
              | undeformed configuration and (right) deformed configuration with positive dihedral.
blank         | 
              | 
text          | The partial differential equation in (2.1) under the finite-dimensional parametrization takes the form:
              | for any ¬µ ‚àà RN¬µ , find U such that
blank         | 
text          |                         ‚àÇU
              |                            + G(U, ‚àáU, ¬µ) = g(x, t, ¬µ)         x ‚àà B(¬µ),   t‚ààT
              |                         ‚àÇt
              |                             H(U, ‚àáU, ¬µ) = h(x, t, ¬µ)          x ‚àà ‚àÇB(¬µ), t ‚àà T                   (2.36)
blank         | 
text          |                                 U (x, t0 , ¬µ) = U0 (x, ¬µ)     x ‚àà B(¬µ).
blank         | 
text          | At this point, the parametrized PDE in (2.36) will be discretized in the usual two-step manner:
              | discretization in space, i.e., semi-discretization, to yield a system of Ordinary Differential Equations
              | (ODEs) and subsequent temporal discretization. A less commonly used alternative is to employ a
              | monolithic space-time discretization. Given the generality of the differential operators in (2.36), it
              | is inappropriate to commit to a single spatial discretization method given the myriad of possibilities
              | including finite differences, finite volumes, finite elements, and discontinuous Galerkin and spectral
              | methods. The most appropriate method depends on a number of factors including the properties
              | of the spatial operators in (2.36), regularity of the solution U , and the complexity of the domain
              | B. Finite volume, finite element, and discontinuous Galerkin methods will be used to discretize the
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              28
blank         | 
              | 
              | 
              | 
text          |                       (a) ¬µ1 = 0.1                                   (b) ¬µ2 = 0.1
blank         | 
              | 
              | 
              | 
text          |                       (c) ¬µ3 = 0.1                                   (d) ¬µ4 = 0.1
blank         | 
              | 
              | 
              | 
text          |                       (e) ¬µ5 = 0.1                                   (f) ¬µ6 = 0.1
blank         | 
              | 
              | 
              | 
text          |                       (g) ¬µ7 = 0.1                                   (h) ¬µ8 = 0.1
blank         | 
text          | Figure 2.5: Shape parametrization of a NACA0012 airfoil using a cubic design element. Blue nodes
              | and lines designate the undeformed design element and shape and black nodes and lines designate
              | the deformed design element and shape.
blank         | 
              | 
text          | various PDEs that arise in this work. At this point, an unspecified spatial discretization is applied
              | to the parametrized PDEs in (2.36) to yield the nonlinear system of ODEs: for any ¬µ ‚àà RN¬µ , find
              | u such that
              |                                      M uÃá = r(u, t, ¬µ)         t‚ààT
              |                                                                                                  (2.37)
              |                                      u(0) = u0 (¬µ)
blank         | 
text          | where u( ¬∑ ) ‚àà RNu is the semi-discrete state vector, M ‚àà RNu √óNu is the mass matrix, r : RNu √óR+ √ó
              | RN¬µ ‚Üí RNu is the nonlinear function that encodes the spatial discretization of the partial differential
              | equation and boundary conditions in (2.36), and u0 (¬µ) ‚àà RNu is the parameter-dependent initial
              | condition that arises from the spatial discretization of U0 (x, ¬µ). If the partial differential equation
              | in (2.36) is steady or static, i.e., U,t = 0, (2.37) becomes
blank         | 
text          |                                               r(u, ¬µ) = 0                                        (2.38)
blank         | 
text          | and the discretization is complete.
blank         | 
text          | Remark. As written, the mass matrix M is time- and parameter-independent, which will not be
              | the case if the domain is time- or parameter-dependent or, for example, if corotator-based shell
              | elements are used in a finite element discretization of structural problems [19]. In the first case, the
              | mass matrix can be completely fixed by using an arbitrary Lagrangian-Eulerian mapping to a fixed
              | reference domain [211]; see Appendix D for details.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                            29
blank         | 
              | 
              | 
              | 
text          | Figure 2.6: Left: Quadrilateral mesh of a subset of R2 corresponding to a rectangle (160 √ó 100
              | elements) whose topology is parametrized by a density-based method. Right: An example of an
              | admissible topology of the density-based topological parametrization‚Äîan optimized cantilever de-
              | signed to maximize the global stiffness of the structure under a vertical load at the right end.
blank         | 
              | 
              | 
              | 
text          | Figure 2.7: Left: Quadrilateral mesh of a subset of R2 corresponding to a rectangle (160 √ó 100
              | elements) with a hole whose topology is parametrized by a density-based method. Right: An example
              | of an admissible topology of the density-based topological parametrization‚Äîa Michell structure
              | [37, 94] designed to maximize the global stiffness of the structure under a vertical load at the right
              | end.
blank         | 
              | 
              | 
              | 
text          | Figure 2.8: Left: Hexahedral mesh of a subset of R3 corresponding to a cube (35 √ó 35 √ó 35 elements)
              | whose topology is parametrized by a density-based method. Right: An example of an admissible
              | topology of the density-based topological parametrization‚Äîa trestle designed to maximize the global
              | stiffness of the structure under a vertical load.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                       30
blank         | 
              | 
              | 
              | 
text          | Figure 2.9: Left: Tetrahedral mesh of a subset of R3 corresponding to an unoptimized lacrosse
              | head (475, 666 elements) whose topology is parametrized by a density-based method. Right: An
              | example of an admissible topology of the density-based topological parametrization‚Äîan unconverged
              | maximum stiffness topology. The entire object is included in the top row and the bottom row is a
              | slice to show internal voids in the optimized shape.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                   31
blank         | 
              | 
              | 
text          |           Table 2.1: Butcher Tableau for s-stage diagonally implicit Runge-Kutta scheme
blank         | 
text          |                                            c1     a11
              |                                            c2     a21     a22
              |                                            ..      ..      ..     ..
              |                                             .       .       .       .
              |                                            cs     as1     as2     ¬∑¬∑¬∑        ass
              |                                                   b1      b2      ¬∑¬∑¬∑        bs
blank         | 
              | 
text          |    For time-dependent problems, the system of ODEs is discretized to yield the complete dis-
              | cretization: a sequence of algebraic, nonlinear systems of equations. The two prevailing classes of
              | high-order implicit temporal integration schemes are: (a) Backward Differentiation Formulas (BDF)
              | and (b) Implicit Runge-Kutta (IRK). BDF schemes are multistep schemes that have the general
              | form
              |                                            n‚àí1
              |                                            X
              |                                M u(n) ‚àí           Œ±i M u(i) = Œ∫‚àÜtr(u(n) , tn , ¬µ)                     (2.39)
              |                                             i=0
blank         | 
text          | where Œ±i and Œ∫ are constants that define different schemes, such as (1) BDF1 (backward Euler):
              | Œ∫ = Œ±n‚àí1 = 1 and Œ±0 = ¬∑ ¬∑ ¬∑ Œ±n‚àí2 = 0, (2) BDF2: Œ∫ = 2/3, Œ±n‚àí1 = 4/3, Œ±n‚àí2 = ‚àí1/3, Œ±0 =
              | ¬∑ ¬∑ ¬∑ = Œ±n‚àí3 = 0, and (3) BDF3: Œ∫ = 6/11, Œ±n‚àí1 = 18/11, Œ±n‚àí2 = ‚àí9/11, Œ±n‚àí3 = 2/11, Œ±0 =
              | ¬∑ ¬∑ ¬∑ = Œ±n‚àí4 = 0. They are popular since high-order accuracy can be achieved at the cost of a single
              | nonlinear solve of size Nu at each time step. However, they suffer from initialization issues and are
              | limited to second-order accuracy, if A-stability is required. In contrast, IRK schemes are single-step
              | methods that can be A-stable and arbitrarily high-order, at the cost of solving an enlarged nonlinear
              | system of equations of size s ¬∑ Nu , for an s-stage scheme, at each time step. For practical problems,
              | this can be prohibitively expensive, in terms of memory and CPU time.
              |    A particular subclass of the IRK schemes, known as Diagonally Implicit Runge-Kutta (DIRK)
              | schemes [3], are capable of achieving high-order accuracy with the desired stability properties, with-
              | out requiring the solution of an enlarged system of equations. The DIRK schemes are defined by a
              | lower triangular Butcher tableau (Table 2.1) and take the following form when applied to (2.37)
blank         | 
text          |                                     u(0) = u0 (¬µ)
              |                                                            s
              |                                                                        (n)
              |                                                            X
              |                                     u(n) = u(n‚àí1) +              bi ki                                (2.40)
              |                                                            i=1
blank         |                                                                         
text          |                                      (n)            (n)
              |                                  M ki      = ‚àÜtn r ui , tn‚àí1 + ci ‚àÜtn , ¬µ ,
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s, where Nt are the number of time steps in the temporal discretiza-
              | tion and s is the number of stages in the DIRK scheme. The temporal domain T is discretized into
              | Nt segments with endpoints {t0 , t1 , . . . , tNt }, with the nth segment having length ‚àÜtn = tn ‚àí tn‚àí1
              |                                                     (n)
              | for n = 1, . . . , Nt . Additionally, in (2.40), ui       is used to denote the approximation of u(n) at the
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             32
blank         | 
              | 
              | 
text          | ith stage of time step n
blank         | 
text          |                                                                                i
              |                         (n)       (n)           (n)                                      (n)
              |                                                                                X
              |                     ui        = ui (u(n‚àí1) , k1 , . . . , ks(n) ) = u(n‚àí1) +         aij kj .   (2.41)
              |                                                                                j=1
blank         | 
              | 
text          | From (2.40), a complete time step requires the solution of a sequence of s nonlinear systems of
              | equation of size Nu .
              |    From this exposition on the spatio-temporal discretization of the parametrized partial differential
              | equation in (2.36), finding the solution of the continuous form of the equations‚Äîa task that requires
              | searching an infinite-dimensional trial space for the solution U ‚Äîhas been reduced the task of finding
              | the solution of the algebraic nonlinear system of equations in (2.38) or a sequence of such equations.
              | The solution, u, of the algebraic equations can be used, along with the shape functions underlying
              | the spatio-temporal discretization, to reconstruct an approximation to the solution U (x, t).
blank         | 
text          | Remark. One option to treat second-order temporal problems, such as those in (2.3) and (2.7) is to
              | recast them in first-order form, as discussed in the previous section, and apply a BDF or IRK/DIRK
              | scheme, as developed in this section. However, it is usually better to apply specialized integrators
              | that work directly on the second-order form of the equation, such as the Newmark scheme [139]
              | or generalized Œ±-method [45], as these schemes are constructed with tunable damping to promote
              | stability‚Äîa particularly important consideration in these problems.
blank         | 
text          |    At this point, the governing equation and its parameters have been discretized. The final dis-
              | cretization task is to treat the quantity of interest. To ensure the truncation error of the governing
              | equation and quantity of interest exactly match, a solver-consistent discretization [211] is employed
              | and detailed in the next section.
blank         | 
              | 
title         | 2.1.4    Discretization: Quantities of Interest
text          | Quantities of interest are among the most important aspects of a computational physics simulation,
              | particularly in engineering applications. Optimization problems, the main focus of this work, are
              | completely driven by quantities of interest as these comprise the objective and constraint functions.
              | Therefore, care must be taken in the discretization of the integrals in (2.2) since this will introduce
              | an additional error, i.e., on top of the error in the discretization of the PDE itself. To ensure
              | the quantity of interest discretization does not dominate, thereby lowering the global order of the
              | scheme, it is necessary that its discretization order matches that of the the governing equations.
              | Clearly, it is wasteful to discretize this to a higher order than the state equation, using a similar
              | argument.
              |    For these reasons, discretization of (2.2) will be done in a solver-consistent manner, i.e., the
              | spatial and temporal discretization used for the governing
              |                                                       Z     equation Zwill also be used for the quan-
              | tities of interest. Define fh as the approximation of   fB (U ) dV +      f‚àÇB (U ) dA using the shape
              |                                                              B                  ‚àÇB
              | functions underlying the spatial discretization of the governing equations. This ensures the spatial
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                            33
blank         | 
              | 
              | 
text          | integration error in the quantity of interest exactly matches that of the governing equations. Next,
              | define                                                            Z   t
              |                                          Fh (u, ¬µ, t) :=                  fh (u, ¬µ, œÑ ) dœÑ,                     (2.42)
              |                                                                     t0
blank         | 
text          | where the temporal domain is taken to be T = (t0 , tf ). Before the temporal discretization of the
              | governing equations can be applied to discretize the integral in (2.42), it must be converted to an
              | ODE. This is accomplished via differentiation of (2.42) with respect to t to yield
blank         | 
text          |                                                FÃáh (u, ¬µ, t) = fh (u, ¬µ, t).                                    (2.43)
blank         | 
text          | Augmenting the semi-discrete governing equations with this ODE results in the enlarged system of
              | ODEs                                     "          #" #                  "                 #
              |                                           M        0  uÃá                      r(u, ¬µ, t)
              |                                                                    =                            .               (2.44)
              |                                             0      1       FÃáh              fh (u, ¬µ, t)
blank         | 
text          | At this point, the same temporal discretization used for the governing equations in the previous
              | section can be applied to discretize (2.44). A monolithic discretization of this form ensures the
              | temporal truncation error of the governing equations and quantity of interest will exactly match.
              | The development will proceed assuming a DIRK scheme is used‚Äîthe same procedure would apply
              | if BDF or another first-order temporal discretization was applied. Application of the DIRK scheme
              | yields the fully discrete governing equations and corresponding solver-consistent discretization of the
              | quantity of interest (2.2)
blank         | 
text          |                                                            s
              |                                                                       (n)
              |                                                            X
              |                                 u(n) = u(n‚àí1) +                  bi ki
              |                                                            i=1
              |                                                              s
              |                                                                                                                 (2.45)
blank         |                                                                                             
text          |                                   (n)       (n‚àí1)                       (n)
              |                                                            X
              |                                 Fh      = Fh           +         bi fh ui , ¬µ, tn‚àí1 + ci ‚àÜtn
              |                                                            i=1
blank         |                                                                             
text          |                                (n)                     (n)
              |                             M ki        = ‚àÜtn r       ui ,   ¬µ, tn‚àí1 + ci ‚àÜtn .
blank         | 
text          |                                                   (n)
              | for n = 1, . . . , Nt , i = 1, . . . , s, and ui         is defined in (2.41). Finally, the functional in (2.42) is
              | evaluated at time t = tf to yield the solver-consistent approximation of Fh (u, ¬µ, tf )
blank         | 
text          |                                                    (1)                              (Nt )
              |                        F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) ) := Fh              ‚âà Fh (u, ¬µ, tf ).   (2.46)
blank         | 
text          |     While the spatially solver-consistent discretization of QoIs is widely used, particularly in the con-
              | text of finite element methods, temporal discretization is commonly done via low-order quadrature
              | rules, usually the trapezoidal rule [193, 130, 124, 205, 102]. The main advantage of this solver-
              | consistent discretization is the asymptotic discretization order of the governing equation and quan-
              | tity of interest are guaranteed to exactly match, which ensures there is no wasted error in ‚Äúover-
              | integrating‚Äù one of the terms. The solver-consistent discretization also has the advantage of a natural
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              34
blank         | 
              | 
              | 
text          | and convenient implementation given the spatial and temporal discretization implementation. Fi-
              | nally, this method has the additional convenience
              |                                            Z tn     of keeping a high-order accurate ‚Äúcurrent‚Äù value
              |                                        (n)
              | of the integral, i.e. at time step n, Fh ‚âà      fh (œÑ ) dœÑ to high-order accuracy. This property does
              |                                             t0
              |                                                        Z tn
              | not hold for high-order numerical quadrature since          fh (œÑ ) dœÑ will involve u(n+j) , where j ‚â• 1
              |                                                         t0
              | depends on the quadrature rule used.
              |    This completes the discussion of parametrized deterministic partial differential equations. Before
              | proceeding to the main topic of this work, PDE-constrained optimization, the notion of parametric
              | stochastic partial differential equations is introduced and relevant details discussed, such as mean-
              | ingful risk measures and methods to discretize the stochastic space. This will lead to the discussion
              | of PDE-constrained optimization that will be applicable in both the deterministic and stochastic
              | setting.
blank         | 
              | 
title         | 2.2        Parametrized Stochastic Partial Differential Equations
text          | This section generalizes the concepts discussed in Section 2.1 to the case where uncertainty is present
              | in the parametrized partial differential equation‚Äîfor simplicity only static problems are considered.
              | The ultimate goal is to setup the stochastic PDE-constrained optimization problem. The discussion
              | begins with the formulation of parametrized, Stochastic Partial Differential Equations (SPDEs) and
              | introduces the concept of risk measures of PDE quantities of interest. These risk measures will
              | comprise the objective and constraint functions in stochastic (risk-averse) optimization problems.
              | The SPDE will be discretized in space using the techniques introduced in Section 2.1 and collocation
              | will be used to discretize the stochastic space. Finally, the deterministic and stochastic PDE-
              | constrained optimization problems will be collectively detailed in Section 2.3.
              |    Let B be a bounded domain in Rnsd and let (‚Ñ¶, F, P ) be a complete probability space. Here
              | ‚Ñ¶ is the set of outcomes, F ‚äÇ 2‚Ñ¶ is the œÉ-algebra of events, and P : F ‚Üí [0, 1] is a probability
              | measure. Consider the stochastic boundary value problem: find U such that P -almost everywhere
              | in ‚Ñ¶
              |                            G(U, ‚àáU, ¬µ, œâ) = g(x, ¬µ, œâ)        x ‚àà B(¬µ, œâ)
              |                                                                                                  (2.47)
              |                            H(U, ‚àáU, ¬µ, œâ) = h(x, ¬µ, œâ)        x ‚àà ‚àÇB(¬µ, œâ),
blank         | 
text          | where B ‚äÇ Rnsd is the spatial domain with boundary ‚àÇB, U is the unknown solution, ¬µ ‚àà RN¬µ
              | are deterministic parameters, G and H are first-order spatial differential operators, and g and h
              | are volumetric and boundary source terms. For generality, the domain, boundary, source terms,
              | and differential operators are all taken as stochastic. The presence of the parameter vector ¬µ
              | indicates that the differential operators, source terms, and boundary conditions have already been
              | parametrized using the techniques in Section 2.1.2. Each realization of the parametrized, stochastic
              | PDE in (2.47), i.e., for a given œâ ‚àà ‚Ñ¶, constitutes a deterministic PDE of the form (2.1), which
              | can be discretized according to the methods outlined in that section. The following finite-noise
              | assumption [13, 12] allows the source of randomness to be approximated using a finite number of
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                 35
blank         | 
              | 
              | 
text          | independent random variables.
blank         | 
text          | Assumption 2.1 (Finite-dimensional noise). The stochastic terms in (2.47) depend on a finite
              | number of real-valued random variables, i.e.,
blank         | 
text          |                               G( ¬∑ , ¬∑ , ¬∑ , œâ) = G( ¬∑ , ¬∑ , ¬∑ , Y1 (œâ), . . . , YNy (œâ))
              |                                  g( ¬∑ , ¬∑ , œâ) = g( ¬∑ , ¬∑ , , Y1 (œâ), . . . , YNy (œâ))               (2.48)
              |                                     B( ¬∑ , œâ) = B( ¬∑ , Y1 (œâ), . . . , YNy (œâ)),
blank         | 
text          |                         y N
              | where Ny ‚àà N+ and {Yn }n=1 are real-valued, independent random variables. A similar expansion is
              | assumed to hold for the boundary terms.
blank         | 
text          |    Define Œûn := Yn (‚Ñ¶) as the image of the random variables in Assumption 2.1 and Œû = Œû1 ‚äó ¬∑ ¬∑ ¬∑ ‚äó
              | ŒûNy ‚äÇ RNy . Let œÅn : Œûn ‚Üí R+ denote the probability density of the random variable Yn and,
              |                                  y    N
              | due to the independence of {Yn }n=1 , the joint density of the random vector Y = (Y1 , . . . , YN ) is
              | œÅ : Œû ‚Üí R+ where œÅ = œÅ1 ‚äó ¬∑ ¬∑ ¬∑ ‚äó œÅNy . The finite noise assumption allows a change of variables that
              | converts the parametrized stochastic partial differential equation in (2.47) to: find U (¬µ, y) such
              | that for all y ‚àà Œû
              |                            G(U, ‚àáU, ¬µ, y) = g(x, ¬µ, y)                  x ‚àà B(¬µ, y)
              |                                                                                                      (2.49)
              |                           H(U, ‚àáU, ¬µ, y) = h(x, ¬µ, y)                   x ‚àà ‚àÇB(¬µ, y)
blank         | 
text          | for ¬µ ‚àà RN¬µ .
blank         | 
              | 
title         | 2.2.1    Risk Measures of Quantities of Interest
text          | The uncertainty that has been incorporated in the partial differential equation in (2.47) will be
              | propagated to the quantities of interest through the solution U (¬µ, y) and possibly the domain
              | B(¬µ, y), boundary ‚àÇB(¬µ, y), and differential operators. To formulate a well-defined and meaningful
              | optimization problem, we consider an objective and constraints that consist of risk measures of these
              | uncertain quantities of interest. For the remainder of this section, let X be a real-valued random
              | variable, defined as X(y; ¬µ) = F(U (¬µ, y), ¬µ, y) where F is the quantity of interest in (2.2) without
              | temporal dependence, generalized to the stochastic case, i.e.,
              |                               Z                         Z
              |                F(U, ¬µ, y) =           fB (U, ¬µ, y) dV +                          f‚àÇB (U, ¬µ, y) dA.   (2.50)
              |                                     B(¬µ, y)                           ‚àÇB(¬µ, y)
blank         | 
              | 
text          | The dependence of the random variable X on the parameter will be dropped for the remainder of
              | this section as treatment of the stochastic dimension is the focus.
              |    The simplest risk measure is the expected value of the random variable
              |                                             Z
              |                                      E[X] =    œÅ(y)X(y) dy.                                          (2.51)
              |                                                        Œû
blank         | 
text          | The mean of the random variable does not necessarily encode a useful measure of risk, but is a
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              36
blank         | 
              | 
              | 
text          | straightforward generalization of the deterministic quantity of interest to the stochastic case. An
              | obvious deficiency in its use as a risk measure is it does not incorporate the spread of random variable
              | about the mean. The mean plus semideviation, defined as
blank         | 
text          |                                      RŒ≤ [X] = E[X] + Œ≤E[(X ‚àí E[X])+ ]                            (2.52)
blank         | 
text          | for Œ≤ ‚àà R+ , where (x)+ = max{0, x} overcomes this limitation. In an optimization setting, the value
              | of Œ≤ must be determined to balance minimization of the (expected) quantity of interest with risk
              | aversion, which may be difficult to do in practice. Another relevant risk measure is the Œ≤-quantile
              | of the random variable, also called the value-at-risk, defined as the smallest value such that the
              | probability that the random variable lies below said value is at least Œ≤, i.e.,
blank         | 
text          |                                VaRŒ≤ [X] = inf{t ‚àà R | Pr[X ‚â§ t] ‚â• Œ≤},                            (2.53)
blank         | 
text          | where                                              Z
              |                                      Pr[X ‚â§ t] =                 œÅ(y) dy.                        (2.54)
              |                                                     y‚ààŒû:X(y)‚â§t
blank         | 
text          | The main disadvantage of the value-at-risk is that it fail to emphasize rare and low probability
              | events, which tend to be particularly important in engineering settings since they often correspond
              | to failure. The conditional value-at-risk, defined as
blank         | 
text          |                                         CVaRŒ≤ [X] = inf FŒ≤ (t, X),                               (2.55)
              |                                                         t‚ààR
blank         | 
              | 
text          | where
              |                                                         1
              |                                      FŒ≤ (t, X) = t +       E [(X ‚àí t)+ ]                         (2.56)
              |                                                        1‚àíŒ≤
              | circumvents this limitation. While the conditional value-at-risk is non-smooth (due to the presence of
              | the max operator) and non-trivial to evaluate, it emphasizes rare events for Œ≤  0. In the remainder
              | of this thesis, only the expectation risk measure will be considered for simplicity. All developments
              | will extend to any smooth risk measure. For non-smooth risk measures such as the semideviation
              | and conditional value-at-risk, well-defined smoothed approximations can be used [110] in place of
              | the risk measure itself.
blank         | 
              | 
text          | 2.2.2    Examples
              | 1D Steady, Viscous Burgers‚Äô Equation with Uncertain Coefficients
blank         | 
text          | The only stochastic partial differential equation considered in this thesis is the 1D steady, viscous
              | Burgers‚Äô equation with uncertain boundary conditions, source term, and viscosity
blank         | 
text          |                ‚àíŒΩ(y)‚àÇxx u(x, y) + u(x, y)‚àÇx u(x, y) = g(x, y)           x ‚àà (xl , xr ), y ‚àà Œû
              |                                                                                                  (2.57)
              |                u(xl , y) = d0 (y),     u(xr , y) = d1 (y)               y ‚àà Œû.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                37
blank         | 
              | 
              | 
text          | The risk-neutral measure (expectation) of the tracking-type functional
              |                                       Z xr                                     
              |                                       1
              |                                              (u(x, ¬∑ ) ‚àí uÃÑ(x))2 + Œ±g(x, ¬∑ )2 dx .
blank         |                                                                             
text          |                          T (u) = E
              |                                       2 xl
blank         | 
text          | will be used to define an optimal control problem in Chapter 6.
blank         | 
title         | Static Linear Elasticity with Uncertain Loading
blank         | 
text          | Another stochastic partial differential equation that will be considered in future work is linear elastic-
              | ity with stochasticity in the load conditions. Consider the same physical setup as the deterministic
              | linear elasticity setup‚Äîa solid body B ‚äÇ Rnsd subject to uncertain distributed body forces with
              | boundary ‚àÇB decomposed into two parts: ‚àÇBu and ‚àÇBt such that ‚àÇB = ‚àÇBu ‚à™ ‚àÇBt . Displacements
              | are prescribed along ‚àÇBu and ‚àÇBt is subject to uncertain, prescribed traction forces. Under the
              | assumption that the resulting deformations are infinitesimal and the pointwise stress and strain
              | are related through a linear relationship, the deformation of the body is governed by the following
              | system of partial differential equations
blank         | 
text          |                              ‚àá ¬∑ œÉ(x, y) + b(x, y) = 0            x ‚àà B,    y‚ààŒû
              |                                           u(x, y) = uÃÑ(x)         x ‚àà ‚àÇBu                          (2.58)
              |                                        œÉ(x, y) ¬∑ n = tÃÑ(x, y)     x ‚àà ‚àÇBt , y ‚àà Œû,
blank         | 
text          | where u is the pointwise deformation and state vector of the PDE, œÉ is the stress tensor, b is the
              | uncertain body force, uÃÑ is the prescribed displacement on ‚àÇBu , tÃÑ is the uncertain, prescribed traction
              | on ‚àÇBt , and n is the pointwise outward normal to the boundary. The system of PDEs is closed with
              | the stress-strain relationship (Hooke‚Äôs law)
blank         | 
text          |                                                   œÉ=C:                                            (2.59)
blank         | 
text          | and the kinematic constraint relates deformation to strain as
blank         | 
text          |                                                  1
              |                                                    ‚àáu + ‚àáuT .
blank         |                                                            
text          |                                             =                                                     (2.60)
              |                                                  2
blank         | 
text          | The quantities of interest considered are the volume of the structure‚Äîa deterministic quantity since
              | it is a geometrical quantity and all uncertainty is in the loading‚Äîand the expectation of the tracking
              | functional
              |                                                       Z                                      
              |                                                       1
              |                    Z
              |              V =        dV       and     T (u) = E        (u(x, ¬∑ ) ‚àí uÃÑ)k (u(x, ¬∑ ) ‚àí uÃÑ)k dV .   (2.61)
              |                     B                                 2 B
blank         | 
title         | 2.2.3     Finite-Dimensional Approximation
text          | Since analytical techniques cannot, in general, be used to solve parametrized stochastic differential
              | equations in (2.47), discretization techniques are applied to reduce the continuous (differential)
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              38
blank         | 
              | 
              | 
text          | form of the problem to a discrete (algebraic) form. Unlike the continuous formulation, the discrete
              | problem can be solved using computational methods and resources. Two types of discretization
              | must be applied to the SPDE in (2.47) to the yield a (sequence of) algebraic equations that are
              | amenable numerical computation‚Äî-spatio-temporal and stochastic discretization.
              |    Each realization of the SPDE in (2.47), i.e., for a given y ‚àà Œû, constitutes a deterministic
              | parametrized partial differential equation and requires spatio-temporal discretization (only spatial
              | discretization for static problems), e.g., such as those in Section 2.1.3, to yield a discrete problem.
              | The specific spatial discretization technique is left unspecified since the appropriate choice depends
              | on the properties of the differential operators G( ¬∑ , ¬∑ , ¬µ, y) and H( ¬∑ , ¬∑ , ¬µ, y). The semi-discrete
              | form of the SPDE in (2.49) is: find u such that
blank         | 
text          |                                       r(u, ¬µ, y) = 0       ‚àÄy ‚àà Œû.                               (2.62)
blank         | 
text          | A variant of the Implicit Function Theorem (Theorem 2.1) implies the existence of a continuous
              | function u(¬µ, y), defined implicitly as the solution of r( ¬∑, ¬µ, y) = 0. The corresponding semi-
              | discrete stochastic quantity of interest and its risk measure take the form
blank         | 
text          |                              f (u, ¬µ, y)     and       R[f (u(¬µ, ¬∑ ), ¬µ, ¬∑ )],                   (2.63)
blank         | 
text          | where R is any risk measure introduced in the previous section.
              |    Despite the spatial discretization, the semi-discrete form of the SPDE in (2.62) can still not be
              | treated computationally as the set Œû contains infinitely many points. There are a few approaches,
              | including stochastic Galerkin methods and stochastic collocation, to discretize the stochastic dimen-
              | sion and yield a fully discrete form of the SPDE that can be solved in a computational setting. This
              | work uses stochastic collocation whereby the equation is (2.62) is enforced only on a finite subset of
              | Œû, that is, (2.62) is replaced with
blank         | 
text          |                                       r(u, ¬µ, y) = 0       ‚àÄy ‚àà Œûh                               (2.64)
blank         | 
text          | where Œûh ‚äÇ Œû and card(Œûh ) < ‚àû. The integrals involved in the computation of the risk measure
              | must then be approximated with a quadrature scheme with nodes Œûh . Section 6.1.3 details an
              | efficient method to construct Œûh using anisotropic sparse grids [67].
blank         | 
              | 
title         | 2.3     PDE-Constrained Optimization
text          | Given the exposition on parametrized partial differential equations in the previous section, atten-
              | tion is turned to the main interest of this document: optimization problems governed by partial
              | differential equations. There are three primary components required to define a PDE-constrained
              | optimization problem:
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                               39
blank         | 
              | 
              | 
text          |    ‚Ä¢ the governing partial differential equation and corresponding state vector that define the phys-
              |       ical problem of interest,
blank         | 
text          |    ‚Ä¢ an objective function and constraint functions‚Äîthe goal of the optimization problem‚Äîthese
              |       are usually quantities of interest of the partial differential equation that define a performance
              |       measure to be optimized and design requirements, and
blank         | 
text          |    ‚Ä¢ optimization parameters‚Äîusually a control or design‚Äîthat are used to meet the performance
              |       requirements.
blank         | 
text          | Each of these components were discussed in the previous section, including details pertaining to their
              | formulation and discretization, and concrete examples were provided. The remainder of this section
              | will consider an abstract vector of parameters, ¬µ ‚àà RN¬µ , i.e., the parameter space has already been
              | discretized and the discrete parameters can control any aspect of the PDE (shape/topology of the
              | domain, boundary conditions, coefficient in differential operators). To encompass the wide array of
              | features in the parametrized partial differential equations discussed previously, an abstract PDE of
              | the form
              |                                              D(U, ¬µ) = 0                                          (2.65)
blank         | 
text          | will be considered, where U is the state vector and D is the differential operator. The abstract
              | quantity of interest will be denoted
              |                                                  F(U, ¬µ)                                          (2.66)
blank         | 
text          | and will be the objective function or cost functional in the remainder of this section. This notation
              | will encompass static and time-dependent, deterministic and stochastic PDEs from previous sections.
              | In the static, deterministic case, U is understood to be only a function of space, i.e., U = U (x),
              | and F is likely an integral over a volume or surface. In the time-dependent, deterministic case, U
              | is a function of space and time, i.e., U = U (x, t), and F is a space-time integral. In the stochastic
              | counterparts, U also depends on the realization, i.e., U ( ¬∑ ) = U ( ¬∑ , y), and F requires an integral
              | over the stochastic space to compute the risk measure of the quantity of interest. The discretized
              | PDE and QoI will be denoted
blank         | 
text          |                                    r(u, ¬µ) = 0      and      f (u, ¬µ),                            (2.67)
blank         | 
text          | respectively, where u is the discrete state vector. While this abstract framework would lead to a hor-
              | ribly inefficient implementation, it is useful to consider these various cases at once as the same issues
              | and concepts regarding PDE-constrained optimization arise in all cases. Each case must be consid-
              | ered separately to obtain a formulation that will be efficient from an implementation viewpoint. The
              | remainder of this chapter discusses important details in the formulation of PDE-constrained opti-
              | mization problems and introduces concepts and notation that will be used throughout the remainder
              | of this thesis.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             40
blank         | 
              | 
              | 
title         | 2.3.1    Continuous vs. Discrete Formulation
text          | The many steps involved in the discretization of partial differential equations provides a large degree
              | of flexibility in the formulation of the PDE-constrained optimization problem. Namely, it can be
              | formulated at the continuous level or at any stage in the discretization process and these will not,
              | in general, be equivalent for finite values of the discretization parameter since the operations of
              | differentiation and discretization do not commute.
              |    The PDE-constrained optimization problem at the continuous level takes the form
blank         | 
text          |                                        minimize       F(U, ¬µ)
              |                                                U, ¬µ
              |                                                                                                 (2.68)
              |                                        subject to D(U, ¬µ) = 0.
blank         | 
text          | The continuous formulation of the optimization problem, also known as the differentiate-then-
              | discretize approach [78], proceeds by deriving the optimality conditions of (2.68), which leads to
              | a system of partial differential equations that includes the primal and adjoint PDE and optimality
              | condition. These PDEs are discretized using the methods introduced in the previous section and
              | solved using an iterative method. Since this process is heavily dependent on the specific form of the
              | PDE and QoI under consideration, a specific example is provided next.
blank         | 
text          | Example 1 (Optimal control of Poisson‚Äôs equation). Consider the optimal control problem that
              | looks to find a distributed control z(x) such that u(x), the solution of the Poisson equation with
              | homogeneous Dirichlet boundary conditions, matches a given target state uÃÑ(x) with a penalty on the
              | magnitude of the control. This problem is stated precisely as
blank         | 
text          |                                        1                          Œ±
              |                                            Z                          Z
              |                                                           2
              |                          minimize             (u(x) ‚àí uÃÑ(x)) dx +          z(x)2 dx
              |                           u(x), z(x)   2    ‚Ñ¶                     2    ‚Ñ¶
blank         | 
text          |                          subject to    ‚àí ‚àÜu(x) = z(x)         x‚àà‚Ñ¶                               (2.69)
blank         | 
text          |                                            u(x) = 0       x ‚àà ‚àÇ‚Ñ¶.
blank         | 
text          | The Lagrangian of this PDE-constrained optimization problem is
blank         | 
text          |                      1                  Œ±
              |                        Z                  Z          Z                   Z
              |        L(u, z, Œª) =      (u ‚àí uÃÑ)2 dx +     z 2 dx ‚àí    Œª [‚àí‚àÜu ‚àí z] dx ‚àí     Œªu dx.             (2.70)
              |                      2 ‚Ñ¶                2 ‚Ñ¶           ‚Ñ¶                   ‚àÇ‚Ñ¶
blank         | 
              | 
text          | Any solution of (2.69) must render the Lagrangian stationary, i.e.,
blank         | 
text          |                                   d
              |                                      L(u + Œ¥u, z, Œª)|=0 = 0         ‚àÄŒ¥u
              |                                   d
              |                                   d
              |                                      L(u, z + Œ¥z, Œª)|=0 = 0         ‚àÄŒ¥z                       (2.71)
              |                                   d
              |                                   d
              |                                      L(u, z, Œª + Œ¥Œª)|=0 = 0         ‚àÄŒ¥Œª.
              |                                   d
blank         | 
text          | After direct differentiation of the Lagrangian and subsequent integration-by-parts, the first condition
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                 41
blank         | 
              | 
              | 
text          | in (2.71) reduces to
blank         | 
text          |       d
              |                                   Z                                  Z
              |          L(u + Œ¥u, z, Œª)|=0 =         [‚àí‚àÜŒª ‚àí (u ‚àí uÃÑ)] Œ¥u dx ‚àí           Œª [Œ¥u + ‚àáŒ¥u ¬∑ n] dx = 0   (2.72)
              |       d                            ‚Ñ¶                                 ‚àÇ‚Ñ¶
blank         | 
text          | Since this relation holds for all Œ¥u, it is equivalent to the following partial differential equation
blank         | 
text          |                                  ‚àí‚àÜŒª(x) = u(x) ‚àí uÃÑ(x)               x‚àà‚Ñ¶
              |                                                                                                      (2.73)
              |                                       Œª(x) = 0                       x ‚àà ‚àÇ‚Ñ¶,
blank         | 
text          | which is known as the adjoint PDE. Direct differentiation of the Lagrangian (2.70) reduces the second
              | condition in (2.71) to
blank         | 
text          |                             d
              |                                                            Z
              |                                L(u, z + Œ¥z, Œª)|=0 =           (Œ±z + Œª)Œ¥z dx = 0,                   (2.74)
              |                             d                              ‚Ñ¶
blank         | 
text          | which is equivalent to the pointwise relationship
blank         | 
text          |                                               Œª(x) = ‚àíŒ±z(x).                                         (2.75)
blank         | 
text          | This is known as the optimality condition. Finally, the last condition in (2.71) reduces to
blank         | 
text          |                    d
              |                                                    Z                         Z
              |                       L(u, z, Œª + Œ¥Œª)|=0 =           Œ¥Œª [‚àí‚àÜu ‚àí z] dx ‚àí            Œ¥Œªu dx = 0       (2.76)
              |                    d                              ‚Ñ¶                           ‚àÇ‚Ñ¶
blank         | 
text          | and recovers the governing PDE since this holds for all Œ¥Œª
blank         | 
text          |                                       ‚àí‚àÜu(x) = z(x)               x‚àà‚Ñ¶
              |                                                                                                      (2.77)
              |                                            u(x) = 0               x ‚àà ‚àÇ‚Ñ¶.
blank         | 
text          | The adjoint PDE (2.73), optimality condition (2.75), and primal PDE (2.77) comprise the optimality
              | system at the continuous level, known as the Karush-Kuhn-Tucker (KKT) conditions. Thus, the
              | optimal control problem in (2.71) reduces to: find u(x), z(x), and Œª(x) such that
blank         | 
text          |                                  ‚àí‚àÜu(x) = z(x)                        x‚àà‚Ñ¶
              |                                         u(x) = 0                      x ‚àà ‚àÇ‚Ñ¶
              |                                  ‚àí‚àÜŒª(x) = u(x) ‚àí uÃÑ(x)                x‚àà‚Ñ¶                            (2.78)
              |                                         Œª(x) = 0                      x ‚àà ‚àÇ‚Ñ¶
              |                                         z(x) = ‚àíŒª(x)/Œ±                x ‚àà ‚Ñ¶.
blank         | 
text          | The derivation of the above KKT system at the continuous level is the first step in the differentiate-
              | then-discretize approach to PDE-constrained optimization. The KKT system is solved by discretizing
              | the parameters, quantities of interest, and primal and adjoint PDEs with the methods outlined in
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                            42
blank         | 
              | 
              | 
text          | Sections 2.1.2‚Äì2.1.4. This leads to a coupled system of equations that are solved to yield an approx-
              | imation to (2.69). In general, different discretization methods and levels of refinement can be used
              | for the primal and adjoint equations. This is one of the advantages of this approach compared to the
              | discretize-then-differentiate approach discussed next [78].
blank         | 
text          |    The discrete formulation, also known as the discretize-then-differentiate approach [78], first dis-
              | cretizes the optimization problem in (2.68) to yield
blank         | 
text          |                                        minimize     f (u, ¬µ)
              |                                           u, ¬µ
              |                                                                                                (2.79)
              |                                        subject to r(u, ¬µ) = 0.
blank         | 
text          | Subsequently, the optimality conditions of (2.79) are derived by introducing its Lagrangian
blank         | 
text          |                                  L(u, ¬µ, Œª) = f (u, ¬µ) ‚àí ŒªT r(u, ¬µ)                            (2.80)
blank         | 
text          | and requiring its stationarity, i.e., (u, ¬µ, Œª) such that
blank         | 
text          |                     ‚àÇL                     ‚àÇL                    ‚àÇL
              |                        (u, ¬µ, Œª) = 0          (u, ¬µ, Œª) = 0         (u, ¬µ, Œª) = 0.             (2.81)
              |                     ‚àÇu                     ‚àÇ¬µ                    ‚àÇŒª
blank         | 
text          | These are the Karush-Kuhn-Tucker (KKT) conditions [143] and lead to the coupled system of
              | nonlinear algebraic equations
              |                                        ‚àÇr               ‚àÇf
              |                                           (u, ¬µ)T Œª =      (u, ¬µ)T
              |                                        ‚àÇu               ‚àÇu
              |                                        ‚àÇr               ‚àÇf                                     (2.82)
              |                                           (u, ¬µ)T Œª =      (u, ¬µ)T
              |                                        ‚àÇ¬µ               ‚àÇ¬µ
              |                                            r(u, ¬µ) = 0,
blank         | 
text          | which are solved simultaneously.
              |    The continuous formulation has a significant disadvantage in that it does not possess discrete
              | consistency in the reduced space setting (Section 2.3.2), that is, the computed gradient is not the
              | true gradient of the computed QoI since differentiation and discretization do not commute. This
              | makes it difficult to use blackbox optimizers to solve the optimization problem as convergence may
              | fail or be slowed when supplied with inconsistent gradients. Despite this disadvantage, there a
              | number of advantages to the continuous formulation. Different discretizations can be used for the
              | primal, sensitivity, and adjoint equations (either predefined or adaptively refined grids) depending
              | on required resolution of each. In shape optimization problems, there is no need to account for the
              | grid motion in the sensitivity and adjoint equations since they are posed directly on the new domain.
              | Finally, this approach can naturally be embedded in an optimization framework that leverages and
              | manages inexact gradients since error bounds on the computed sensitivities and adjoints are available
              | [108, 109].
              |    The discrete formulation has a number of advantages as compared to the continuous framework,
              | most notable discrete consistency of computed functionals and gradients. Additionally, the discrete
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                43
blank         | 
              | 
              | 
text          | formulation allows for the use of sophisticated differentiation software, such as automatic [161] and
              | symbolic [126] differentiation, to compute the various quantities that arise in the sensitivity and
              | adjoint equations. The discrete setting also allows for a large degree of flexibility in the quantities of
              | interest and optimization parameters considered, particularly if a well-defined differentiation frame-
              | work is used in the implementation. The continuous approach requires re-deriving the corresponding
              | adjoint equations for each partial differential equation, boundary condition, and quantity of interest.
              | For these reasons, the discrete formulation will be solely considered throughout the remainder of
              | this thesis.
              |    In certain situations, the continuous and discrete formulations of the optimization problem are
              | equivalent. This equivalence holds if the scheme used to discretize the partial differential equation is
              | adjoint consistent‚Äîthat is, the discrete adjoint equations correspond to a consistent discretization
              | of the continuous adjoint equations [11, 84]. This property is not crucial for this work since the
              | discrete formulation is considered and therefore gradients automatically possess discrete consistency.
              | However, it has been shown that an adjoint consistent discretization of the PDE is necessary for
              | optimal convergence rates in L2 and in quantities of interest [97, 83, 82].
blank         | 
              | 
title         | 2.3.2     Full Space vs. Reduced Space Approach
text          | To this point, the PDE-constrained optimization problem has been posed as an optimization problem
              | over the state and parameter. This is usually called a full space or one-shot formulation as the
              | solution of the PDE and optimization problem are sought simultaneously. In contrast, the reduced
              | space approach explicitly enforces the PDE constraint and considers an optimization problem over
              | the parameters only. In the optimization community, this is commonly referred to as nonlinear
              | elimination of equality constraints [71, 143].
              |    To consider the reduced space approach to PDE-constrained optimization, the following assump-
              | tion on existence and uniqueness of solutions of the PDE is crucial.
blank         | 
text          | Assumption 2.2. For any ¬µ ‚àà RN¬µ , there exists a unique u(¬µ) such that r(u(¬µ), ¬µ) = 0.
blank         | 
text          |    From this assumption, it is clear that optimization of pure Neumann problems is not possible in
              | the reduced space setting as the solutions of these problems are only unique up to a constant. For
              | such problems, a full space setting is more appropriate. The implicit function theorem guarantees
              | Assumption 2.2 holds if r is sufficiently regular and its Jacobian is invertible; in fact, it guarantees
              | the existence of a smooth function that maps ¬µ ‚àà RN¬µ to the corresponding solution of the PDE,
              | u(¬µ).
blank         | 
text          | Theorem 2.1 (Implicit Function Theorem). Let A be an open set in RNu √ó RN¬µ and suppose
              | r : A ‚Üí RNu is a C r function (r ‚â• 1). Consider uÃÑ ‚àà RNu and ¬µÃÑ ‚àà RN¬µ such that r(uÃÑ, ¬µÃÑ) = 0 and
              | ‚àÇr
              |     (uÃÑ, ¬µÃÑ) is invertible. Then, there exists a neighborhood B ‚äÇ RN¬µ of ¬µÃÑ and a unique C r function
              | ‚àÇu
              | u : RN¬µ ‚Üí RNu such that uÃÑ = u(¬µÃÑ) and r(u(¬µ), ¬µ) = 0 for all ¬µ ‚àà B.
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             44
blank         | 
              | 
              | 
text          |    This mapping from ¬µ to u(¬µ) is used to define a quantity of interest that only depends on ¬µ
blank         | 
text          |                                         F (¬µ) := f (u(¬µ), ¬µ).                                   (2.83)
blank         | 
text          | Thus, the constrained optimization problem in (2.79) can be reduced to the unconstrained optimiza-
              | tion problem
              |                                            minimize F (¬µ)                                       (2.84)
              |                                             ¬µ‚ààRN¬µ
blank         | 
text          | since the solution of the PDE is fully accounted for in u(¬µ). In a gradient-based optimization
              | setting, the reduced space approach requires the computation of gradients of quantities of interest
              | that account for the total dependence on ¬µ, that is, the explicit dependence on ¬µ and the implicit
              | dependence through the solution of the PDE itself. This will be the focus of the next two sections
              | that consider two distinct approaches to obtaining such gradients.
              |    There are a number of advantages of the reduced space approach over full space methods, par-
              | ticularly in the context of large-scale, practical problems. First, the optimization problem is smaller
              | and simpler‚Äîit is only posed over the parameters since the state variable is taken as an implicit
              | function of these parameters, u(¬µ), and the nonlinearly constrained optimization problem is reduced
              | to an unconstrained one. The reduced space framework also allows for the use of state-of-the-art
              | PDE solvers and black-box optimizers since it decouples the solution of the PDE and the optimiza-
              | tion problem. This is particularly important in the context of computational fluid dynamics where
              | specialized methods exist for solving the steady-state partial differential equation such as pseudo-
              | transient continuation [104, 105]. For these reasons, the remainder of this thesis will focus solely
              | on the reduced space formulation of PDE-constrained optimization. The close this discussion, it is
              | worthwhile to mention some advantages of the full space approach: (1) it does not require Assump-
              | tion 2.2, thereby enlarging the class of problems to which it can be applied and (2) it is usually
              | more efficient than the reduced space approach since it does not require full resolution of the PDE
              | solution at every iteration.
blank         | 
              | 
title         | 2.3.3    Sensitivity Method for Computing Gradients
text          | Once one commits to using a reduced space approach, the gradient of F (¬µ) in (2.84) must be
              | computed, if a gradient-based optimization method is to be employed. This gradient must account
              | for the explicit dependence of f on ¬µ as well as its implicit dependence through the solution of
              | the PDE. Throughout the remainder of this chapter, u(¬µ) will be used to denote the function in
              | Theorem 2.1 that maps ¬µ to the solution of the PDE. Application of the chain rule leads to the
              | expansion
              |                            dF       ‚àÇf             ‚àÇf           ‚àÇu
              |                               (¬µ) =    (u(¬µ), ¬µ) +    (u(¬µ), ¬µ)    (¬µ).                         (2.85)
              |                            d¬µ       ‚àÇ¬µ             ‚àÇu           ‚àÇ¬µ
              | Furthermore, since u(¬µ) is the solution of the PDE for any ¬µ, r(u(¬µ), ¬µ) = 0 and
blank         | 
text          |                 dr                           ‚àÇr             ‚àÇr           ‚àÇu
              |                    (u(¬µ), ¬µ) = 0     =‚áí         (u(¬µ), ¬µ) +    (u(¬µ), ¬µ)    (¬µ) = 0             (2.86)
              |                 d¬µ                           ‚àÇ¬µ             ‚àÇu           ‚àÇ¬µ
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                                45
blank         | 
              | 
              | 
text          |                                                                    ‚àÇr
              | From the assumptions in Theorem 2.1, the Jacobian matrix              (u(¬µ), ¬µ) is invertible, which leads
              |                                                                    ‚àÇu
              | to the following expression for the sensitivity
blank         | 
text          |                               ‚àÇu         ‚àÇr             ‚àÇr
              |                                  (¬µ) = ‚àí    (u(¬µ), ¬µ)‚àí1    (u(¬µ), ¬µ).                               (2.87)
              |                               ‚àÇ¬µ         ‚àÇu             ‚àÇ¬µ
blank         | 
text          | Combining this equation for the sensitivity with the expression for the gradient of F leads to
blank         | 
text          |                 dF       ‚àÇf             ‚àÇf           ‚àÇr             ‚àÇr
              |                    (¬µ) =    (u(¬µ), ¬µ) ‚àí    (u(¬µ), ¬µ)    (u(¬µ), ¬µ)‚àí1    (u(¬µ), ¬µ).                   (2.88)
              |                 d¬µ       ‚àÇ¬µ             ‚àÇu           ‚àÇu             ‚àÇ¬µ
blank         | 
text          | This method of computing the gradient of F is known as the sensitivity or direct method. An
              |                                                                      ‚àÇu
              | important observation is that each column of the sensitivity matrix,    , requires the solution of
              |                                                                      ‚àÇ¬µ
              | a linear system of equation with the Jacobian matrix‚Äîa total of N¬µ linear systems with the same
              | matrix and different right-hand sides. In large-scale applications, particularly for time-dependent
              | problems, this will be a very expensive endeavor‚Äîsee Appendix D for details regarding the sensitivity
              | method for time-dependent problems. An advantage of the sensitivity method is that once the
              |              ‚àÇu
              | sensitivity,     is computed, the gradient of any number of functionals can be computed essentially
              |              ‚àÇ¬µ
              | for free. This is useful if the problem has a large number of side constraints‚Äîsee Section 2.3.5.
              |    Before closing this discussion on sensitivity analysis, define the sensitivity residual as
blank         | 
text          |                                                      ‚àÇr          ‚àÇr
              |                                r ‚àÇ (u, v, ¬µ) :=         (u, ¬µ) +    (u, ¬µ)v,                        (2.89)
              |                                                      ‚àÇ¬µ          ‚àÇu
blank         | 
text          | which is motivated from the sensitivity equations in (2.86). Clearly, we have
blank         |                                                                
text          |                                          ‚àÇ             ‚àÇu
              |                                      r           u(¬µ),    (¬µ), ¬µ = 0.
              |                                                        ‚àÇ¬µ
blank         | 
text          | The sensitivity residual will be used as an error indicator for any approximation u, w of the true
              |                                        ‚àÇu
              | primal solution u(¬µ) and sensitivity      (¬µ), as well as an error bound on the corresponding ap-
              |                                        ‚àÇ¬µ
              | proximation of ‚àáF (¬µ)‚Äîsee Appendix B. In a similar manner, the gradient computation in (2.88)
              | is generalized to consider non-equilibrium solutions u and sensitivities w
blank         | 
text          |                                                      ‚àÇf          ‚àÇf
              |                                g ‚àÇ (u, w, ¬µ) =          (u, ¬µ) +    (u, ¬µ)w                         (2.90)
              |                                                      ‚àÇ¬µ          ‚àÇu
blank         | 
text          | as this will play a role in the residual-based error bounds on QoIs (Appendix B). The next section
              | introduces a method to compute ‚àáF (¬µ)‚Äîthe adjoint method‚Äîthat circumvents the large cost of
              | the sensitivity approach when N¬µ  1.
blank         | 
              | 
title         | 2.3.4    Adjoint Method for Computing Gradients
text          | The adjoint method is an alternative approach to compute the gradient of F that circumvents the
              | sensitivity computation in (2.87) and only requires a single linear system solve with the transpose
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              46
blank         | 
              | 
              | 
text          | of the Jacobian matrix to compute the entire gradient. In this section, three different derivations of
              | the adjoint method will be provided, leading to various interpretations of the adjoint variable.
              |      The first and simplest derivation of the adjoint method applies a simple algebraic trick to the
              | gradient expression in (2.88) to yield
              |                                                                   !T
              |                 dF   ‚àÇf   ‚àÇf ‚àÇr ‚àí1 ‚àÇr   ‚àÇf           ‚àÇr ‚àíT ‚àÇf T        ‚àÇr   ‚àÇf      ‚àÇr
              |                    =    ‚àí             =    ‚àí                              =    ‚àí ŒªT              (2.91)
              |                 d¬µ   ‚àÇ¬µ ‚àÇu ‚àÇu ‚àÇ¬µ        ‚àÇ¬µ           ‚àÇu ‚àÇu             ‚àÇ¬µ   ‚àÇ¬µ      ‚àÇ¬µ
blank         | 
text          | where the arguments u(¬µ) and ¬µ have been dropped for brevity and Œª(¬µ) is defined as the solution
              | of
              |                                  ‚àÇr                   ‚àÇf
              |                                     (u(¬µ), ¬µ)T Œª(¬µ) =    (u(¬µ), ¬µ)T .                            (2.92)
              |                                  ‚àÇu                   ‚àÇu
              | The linearized equations in (2.92) are known as the adjoint equations and Œª is the adjoint or dual
              | variable. From (2.91) and (2.92) it is clear the gradient of F can be computed from one linear system
              | solve, regardless of N¬µ .
              |      The second derivation proceeds by introducing Œª as an arbitrary test function, multiplying it
              | by the sensitivity equations in (2.86), and adding the resulting expression to the equation for the
              | gradient of F in (2.85)                                        
              |                               dF   ‚àÇf   ‚àÇf ‚àÇu        ‚àÇr   ‚àÇr ‚àÇu
              |                                  =    +       ‚àí ŒªT      +         .                              (2.93)
              |                               d¬µ   ‚àÇ¬µ ‚àÇu ‚àÇ¬µ          ‚àÇ¬µ ‚àÇu ‚àÇ¬µ
              | This equation is valid since the term in brackets on the right side is identically zero from (2.86) and
              | the fact that all terms are evaluated at the primal and sensitivity solutions. Recall the goal is to get
              |                     dF                                        ‚àÇu
              | an expression for      that is independent of the sensitivity    . To this end, the terms in (2.93) are
              |                     d¬µ                                        ‚àÇ¬µ
              | re-arranged such that the sensitivity is isolated
blank         |                                                             
text          |                                dF   ‚àÇf      ‚àÇr     ‚àÇf      ‚àÇr ‚àÇu
              |                                   =    ‚àí ŒªT    +      ‚àí ŒªT       .                               (2.94)
              |                                d¬µ   ‚àÇ¬µ      ‚àÇ¬µ     ‚àÇu      ‚àÇu ‚àÇ¬µ
blank         | 
text          | Define Œª, which has remained arbitrary to this point, as the solution of the adjoint equation
blank         | 
text          |                                  ‚àÇr                   ‚àÇf
              |                                     (u(¬µ), ¬µ)T Œª(¬µ) =    (u(¬µ), ¬µ)T                              (2.95)
              |                                  ‚àÇu                   ‚àÇu
blank         | 
text          |                                                                             dF
              | and the expression in the brackets vanishes, leading to an expression for      that is independent of
              |                                                                             d¬µ
              | the sensitivities
              |                              dF       ‚àÇf                   ‚àÇr
              |                                 (¬µ) =    (u(¬µ), ¬µ) ‚àí Œª(¬µ)T    (u(¬µ), ¬µ)                          (2.96)
              |                              d¬µ       ‚àÇ¬µ                   ‚àÇ¬µ
              | and agrees with (2.92).
              |      The final derivation will introduce the adjoint variable as the Lagrange multipliers corresponding
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                            47
blank         | 
              | 
              | 
text          | to the PDE constraint of the auxiliary PDE-constrained optimization problem
blank         | 
text          |                                       minimize        f (u, ¬µÃÇ)
              |                                            u
              |                                                                                                (2.97)
              |                                       subject to r(u, ¬µÃÇ) = 0,
blank         | 
text          | where ¬µÃÇ is fixed, i.e., not an optimization variable. Assumption 2.2 implies that the optimization
              | problem is equivalent to the nonlinear system of equation
blank         | 
text          |                                                 r(u, ¬µÃÇ) = 0.
blank         | 
text          | This follows directly from ¬µÃÇ being fixed and uniqueness of the solution of r(¬∑, ¬µ) = 0, i.e., the
              | feasible set of the optimization problem in (2.97) is {u(¬µÃÇ)} and therefore u(¬µÃÇ) must be the solution
              | of (2.97), regardless of the objective function. The Lagrangian of the optimization problem in (2.97)
              | is
              |                                   L(u, Œª) = f (u, ¬µÃÇ) ‚àí ŒªT r(u, ¬µÃÇ)                            (2.98)
blank         | 
text          | and the KKT system is
              |                                  ‚àÇL   ‚àÇf              ‚àÇr
              |                                     =    (u, ¬µÃÇ) ‚àí ŒªT    (u, ¬µÃÇ) = 0
              |                                  ‚àÇu   ‚àÇu              ‚àÇu                                       (2.99)
              |                                  ‚àÇL
              |                                     = ‚àír(u, ¬µÃÇ)                  =0
              |                                  ‚àÇŒª
              | The first condition is exactly the adjoint equations in (2.92) and the second condition is the PDE
              |                                                                         dF
              | constraint. Substitution into (2.94) yields the familiar expression for
              |                                                                         d¬µ
blank         | 
text          |                            dF       ‚àÇf                   ‚àÇr
              |                               (¬µ) =    (u(¬µ), ¬µ) ‚àí Œª(¬µ)T    (u(¬µ), ¬µ).                        (2.100)
              |                            d¬µ       ‚àÇ¬µ                   ‚àÇ¬µ
blank         | 
text          | Thus, the adjoint variable has been introduced as an algebraic trick to re-arrange the operations
              | in (2.88), a test function multiplying the sensitivity equations, and the Lagrange multipliers of an
              | auxiliary PDE-constrained optimization problem. Appendix D details the derivation of the adjoint
              | equations‚Äîusing the test function and Lagrange multiplier approach‚Äîfor a time-dependent PDE
              | posed on a deforming domain and discretized with high-order spatial and temporal schemes. Similar
              | to the previous section, the adjoint equations in (2.92) are used to motivate the definition of the
              | adjoint residual
              |                                                 ‚àÇf           ‚àÇr
              |                              r Œª (u, v, ¬µ) :=      (u, ¬µ)T ‚àí    (u, ¬µ)T v,                    (2.101)
              |                                                 ‚àÇu           ‚àÇu
              | which will be used as an error measure when inexact primal and adjoint solution are used to compute
              | ‚àáF (¬µ)‚Äîsee Appendix B. In a similar manner, the gradient computation in (2.91) is generalized to
              | consider non-equilibrium solutions u and adjoints z
blank         | 
text          |                                                  ‚àÇf              ‚àÇr
              |                               g Œª (u, z, ¬µ) =       (u, ¬µ) ‚àí z T    (u, ¬µ)                    (2.102)
              |                                                  ‚àÇ¬µ              ‚àÇ¬µ
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                             48
blank         | 
              | 
              | 
text          | as this will play a role in the residual-based error bounds on QoI gradients (Appendix B).
blank         | 
              | 
title         | 2.3.5    Optimization Problems with Side Constraints
text          | To this point, unconstrained PDE-constrained optimization problems, i.e, optimization problems
              | where the PDE is the only constraint, have been solely considered due to simplicity in the exposition.
              | Nearly all practical problems, particularly in a design setting, will have additional performance
              | constraints, usually referred to as side constraints. In this case, the fully discrete optimization
              | problem in the full space takes the form
blank         | 
text          |                                        minimize      f (u, ¬µ)
              |                                            u, ¬µ
blank         | 
text          |                                        subject to r(u, ¬µ) = 0
              |                                                                                                (2.103)
              |                                                      c(u, ¬µ) = 0
              |                                                      d(u, ¬µ) ‚â§ 0,
blank         | 
text          | where c and d are equality and inequality side constraints, respectively. In a gradient-based opti-
              | mization framework, the terms
              |                                            ‚àÇc ‚àÇc ‚àÇd ‚àÇd
              |                                              ,  ,  ,
              |                                            ‚àÇu ‚àÇ¬µ ‚àÇu ‚àÇ¬µ
              | are required in addition to
              |                                            ‚àÇf ‚àÇf ‚àÇr ‚àÇr
              |                                              ,  ,  ,   ,
              |                                            ‚àÇu ‚àÇ¬µ ‚àÇu ‚àÇ¬µ
              | the terms required in the case without side constraints. In the reduced space setting, the optimization
              | problem becomes
              |                                        minimize       F (¬µ)
              |                                              ¬µ
blank         | 
text          |                                        subject to C(¬µ) = 0                                     (2.104)
blank         | 
text          |                                                       D(¬µ) ‚â§ 0,
blank         | 
text          | where F (¬µ) is defined in (2.83) and
blank         | 
text          |                        C(¬µ) := c(u(¬µ), ¬µ)          and      D(¬µ) := d(u(¬µ), ¬µ).                (2.105)
blank         | 
text          | This is a nonlinearly constrained optimization problem over ¬µ and a gradient-based optimization
              | setting will require the Jacobians of the constraints
blank         | 
text          |                                                   dC dD
              |                                                     ,   ,
              |                                                   d¬µ d¬µ
blank         | 
text          | which can be computed using the sensitivity or adjoint approach discussed previously. In the case
              | where one of the constraints does not depend on the state vector u, the sensitivity/adjoint method
              | are not needed as the gradient will be equivalent to the partial derivative with respect to ¬µ. If
              |                                                   ‚àÇu
              | the sensitivity approach is used, the sensitivity    (¬µ) is computed once-and-for-all and used to
              |                                                   ‚àÇ¬µ
meta          | CHAPTER 2. PDE-CONSTRAINED OPTIMIZATION                                                              49
blank         | 
              | 
              | 
text          | reconstruct the required gradients as
blank         | 
text          |                             dF          ‚àÇf             ‚àÇf           ‚àÇu
              |                                (¬µ) =       (u(¬µ), ¬µ) +    (u(¬µ), ¬µ)    (¬µ)
              |                             d¬µ          ‚àÇ¬µ             ‚àÇu           ‚àÇ¬µ
              |                             dC          ‚àÇc             ‚àÇc           ‚àÇu
              |                                (¬µ) =       (u(¬µ), ¬µ) +    (u(¬µ), ¬µ)    (¬µ)                      (2.106)
              |                             d¬µ          ‚àÇ¬µ             ‚àÇu           ‚àÇ¬µ
              |                             dD          ‚àÇd             ‚àÇd           ‚àÇu
              |                                (¬µ) =       (u(¬µ), ¬µ) +    (u(¬µ), ¬µ)    (¬µ)
              |                             d¬µ          ‚àÇ¬µ             ‚àÇu           ‚àÇ¬µ
blank         | 
text          | Thus, even though the sensitivity computation requires a linear solve for each entry in ¬µ, it is used to
              | construct the gradient of any number of functionals and is efficient when the number of constraints
              | is large compared to N¬µ .
              |    Conversely, the adjoint equation is tied to a specific functional and each separate constraint
              | requires the solution of a different adjoint equation
blank         | 
text          |                                ‚àÇr                       ‚àÇf
              |                                   (u(¬µ), ¬µ)T Œªf (¬µ) =      (u(¬µ), ¬µ)T
              |                                ‚àÇu                       ‚àÇu
              |                                ‚àÇr                       ‚àÇc
              |                                   (u(¬µ), ¬µ)T Œªc (¬µ) =      (u(¬µ), ¬µ)T                           (2.107)
              |                                ‚àÇu                       ‚àÇu
              |                                ‚àÇr                       ‚àÇd
              |                                   (u(¬µ), ¬µ)T Œªd (¬µ) =      (u(¬µ), ¬µ)T .
              |                                ‚àÇu                       ‚àÇu
blank         | 
text          | Once the dual variable for each functional has been computed, the required derivatives are recon-
              | structed as
              |                             dF         ‚àÇf                     ‚àÇr
              |                                (¬µ) =      (u(¬µ), ¬µ) ‚àí Œªf (¬µ)T    (u(¬µ), ¬µ)
              |                             d¬µ         ‚àÇ¬µ                     ‚àÇ¬µ
              |                             dC         ‚àÇc                     ‚àÇr
              |                                (¬µ) =      (u(¬µ), ¬µ) ‚àí Œªc (¬µ)T    (u(¬µ), ¬µ)                      (2.108)
              |                             d¬µ         ‚àÇ¬µ                     ‚àÇ¬µ
              |                             dD         ‚àÇd                     ‚àÇr
              |                                (¬µ) =      (u(¬µ), ¬µ) ‚àí Œªd (¬µ)T    (u(¬µ), ¬µ).
              |                             d¬µ         ‚àÇ¬µ                     ‚àÇ¬µ
              |    Although it is not common, certain cases arise where it is possible to use nonlinear elimination
              | to remove side constraints, identical to elimination of the PDE constraint in the reduced space
              | approach. In these cases, for each ¬µ, there must exist a u(¬µ) that satisfies the PDE and side
              | constraint. In general, this mapping will be different from the one defined in Theorem 2.1 and will
              | modify the sensitivity and adjoint equations. Appendix D provides a concrete example of a time-
              | dependent PDE-constrained optimization problem with two side constraints‚Äîthe first is a lower
              | bound on a QoI (not amenable to elimination) and the second requires time-periodicity of the PDE
              | solution (amenable to elimination). Nonlinear elimination is applied to the periodicity constraint
              | and the adjoint equations are modified accordingly.
title         | Chapter 3
blank         | 
title         | Generalized Multifidelity Trust
              | Region Method
blank         | 
text          | Given the broad discussion on partial differential equations and PDE-constrained optimization in
              | Chapter 2, the scope will be narrowed to consider only the reduced-space framework for the remain-
              | der of the document. In this setting, nonlinear elimination is used to explicitly enforce the PDE
              | constraint and eliminate the state variables from the optimization problem. This leads to an uncon-
              | strained or constrained optimization problem, depending on the presence of side constraints, over
              | only the parameters, ¬µ. Each query to the objective or constraint requires a primal PDE solve and
              | each query to the corresponding gradient requires (possibly many) sensitivity or adjoint PDE solves.
              | For PDEs with uncertain coefficients, an ensemble of primal and dual PDE solves are required to
              | evaluate these optimization functionals and gradients (in order to evaluate risk-averse measures of
              | quantities of interest). For large-scale problems that commonly arise in engineering and scientific
              | practice, this will be an expensive endeavor. To mitigate this computational burden, a globally
              | convergent optimization method is developed that enables the use of inexpensive, locally accurate
              | approximation models. This chapter will develop the multifidelity optimization method with an
              | abstract approximation model for the sake of generality, i.e., any approximation model that satisfies
              | the assumptions to be laid out. Chapters 5‚Äì6 will detail the use of projection-based reduced-order
              | models as the approximation model.
              |    This chapter begins with necessary background regarding unconstrained optimization theory.
              | Subsequently, an error-aware multifidelity trust region method‚Äîone of the auxiliary contributions
              | of this thesis‚Äîis developed. Finally, the special case of an unconstrained problem, i.e., no side
              | constraints, is generalized to handle nonlinear equality constraints.
blank         | 
              | 
              | 
              | 
meta          |                                                  50
              | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                               51
blank         | 
              | 
              | 
title         | 3.1         Unconstrained Optimization
text          | Consider the unconstrained optimization of a twice-continuously differentiable function F : RN¬µ ‚Üí R
              | that is bounded below, i.e., F ‚àà {g ‚àà C 2 (RN¬µ ) | inf g(¬µ) > ‚àí‚àû}, stated as
blank         | 
text          |                                             minimize F (¬µ).                                         (3.1)
              |                                              ¬µ‚ààRN¬µ
blank         | 
              | 
text          | This is the exact form of the reduced-space PDE-constrained optimization problem in (2.84). In
              | general, it is desirable to find the global minimum of (3.1), i.e., the point ¬µ‚àó such that F (¬µ‚àó ) ‚â§ F (¬µ)
              | for all ¬µ ‚àà RN¬µ ; however, it is impossible to construct an efficient and reliable global optimization
              | algorithm for an arbitrary nonlinear function and we settle for local minima, as defined in Defini-
              | tion 3.1.
blank         | 
text          | Definition 3.1 (Unconstrained local minima). A point ¬µ‚àó ‚àà RN¬µ is a local minima of F if there
              | is a neighborhood N of ¬µ‚àó such that F (¬µ‚àó ) ‚â§ F (¬µ) for all ¬µ ‚àà N .
blank         | 
text          |    From Theorem 3.1, if a point ¬µ‚àó ‚àà RN is a local minima of (3.1), it must be a stationary
              | point (Definition 3.2) of the function F . This is known as a first-order condition since it places a
              | requirement on the gradient of F .
blank         | 
text          | Theorem 3.1 (First-order unconstrained optimality conditions). If ¬µ‚àó is a local minimizer of F (¬µ)
              | and F is continuously differentiable in a neighborhood of ¬µ‚àó , then
blank         | 
text          |                                               ‚àáF (¬µ‚àó ) = 0.                                         (3.2)
blank         | 
text          | Proof. See [143]
blank         | 
text          | Definition 3.2 (Unconstrained stationary point). Any point ¬µ that satisfies ‚àáF (¬µ) = 0 is called a
              | stationary point.
blank         | 
text          |    There are also second-order necessary and sufficient conditions for ¬µ‚àó to be a local minima of
              | (3.1) that involve (semi-)positive definiteness of the Hessian of F [143]. This work will primarily be
              | concerned with first-order optimality conditions.
blank         | 
              | 
title         | 3.1.1       Error-Aware Multifidelity Trust Region Method
text          | In this section, we consider optimization problems of the form (3.1) where the evaluation of F and its
              | gradient are expensive and look to develop an optimization algorithm that leverages an inexpensive
              | approximation model, mk (¬µ), at iteration k. It is assumed that evaluation of mk (¬µ) and its gradient
              | are substantially less expensive than the corresponding operation with F (¬µ). The approximation
              | model mk (¬µ) is required to be locally accurate around the kth iterate, ¬µk , but may be inaccurate
              | away from this point. An inexpensive optimization procedure involving the approximation model
              | is intended to improve the current iterate and make progress toward the optimal solution. Due to
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                             52
blank         | 
              | 
              | 
text          | the inherent locality of the approximation model, it will not suffice to consider the unconstrained
              | optimization problem
              |                                            minimize mk (¬µ)                                         (3.3)
              |                                              ¬µ‚ààRN¬µ
blank         | 
text          | as the new iterate ¬µk+1 may fail to make progress toward the local minima of (3.1). For this reason,
              | the optimization problem is only posed within a trust region, defined as the sublevel sets of a function
              | œëk : RN¬µ ‚Üí R+ , i.e.,
              |                                        minimize      mk (¬µ)
              |                                          ¬µ‚ààRN¬µ
              |                                                                                                    (3.4)
              |                                        subject to œëk (¬µ) ‚â§ ‚àÜk .
blank         | 
text          | In traditional trust region methods [48], the model is taken as the quadratic approximation of F at
              | ¬µk and the trust region constraint is the Euclidean distance from ¬µk
blank         | 
text          |                                                       1
              |                 mk (¬µ) = F (¬µk ) + ‚àáF (¬µk )(¬µ ‚àí ¬µk ) + (¬µ ‚àí ¬µk )T ‚àá2 F (¬µk )(¬µ ‚àí ¬µk )
              |                                                       2
              |                 œëk (¬µ) = k¬µ ‚àí ¬µk k2 ,
blank         | 
text          | A plethora of variants have been proposed that leverage inexact gradients and Hessians in the
              | definition of mk (¬µ) [189, 35, 48, 108] and non-quadratic model objectives [4, 48, 10, 108]. In this
              | work, the trust region constraint itself is generalized such that error bounds between the objective
              | function and approximation model can be directly leveraged. This will, in a sense, define an error-
              | aware trust region.
              |    Before proceeding to the statement of the complete generalized trust region algorithm, an inter-
              | pretation of an error-aware trust region is provided for a special case. Suppose the scalar-valued
              | constraint function œëk : RN¬µ ‚Üí R+ is defined as the Euclidean norm of a linear vector-valued error
              | indicator œëk : RN¬µ ‚Üí Rm , i.e.,
              |                                           œëk (¬µ) = kœëk (¬µ)k2 .
blank         | 
text          | Additionally, suppose the approximation model is exact at trust region centers and this is reflected
              | in the vector-valued error indicator (œëk (¬µk ) = 0), i.e.,
blank         | 
text          |                                          œëk (¬µ) = Ak (¬µ ‚àí ¬µk ),
blank         | 
text          | where Ak ‚àà Rm√óN¬µ is a fixed matrix. Then the constraint function can be expanded as
blank         | 
text          |                                   œë(¬µ) = kœëk (¬µ)k2 = k¬µ ‚àí ¬µk kAT Ak ,                              (3.5)
              |                                                                   k
blank         | 
              | 
              | 
text          | which is precisely a traditional trust region constraint in the ATk Ak -norm. Consider the eigenvalue
              | decomposition of the symmetric positive (semi)-definite matrix ATk Ak
blank         | 
text          |                                           ATk Ak = Qk Œõk QTk ,                                     (3.6)
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                             53
blank         | 
              | 
              | 
              | 
text          |                                           ‚àÜk
              |                                           Œª2 q2        ‚àÜk
              |                                                        Œª 1 q1
              |                                                   ¬µk
blank         | 
              | 
              | 
              | 
text          | Figure 3.1: Geometry of trust region constraint in special case where œëk = kAk (¬µ ‚àí ¬µk )k2 =
              | k¬µ ‚àí ¬µk kAT Ak . The eigenvalue decomposition of ATk Ak is ATk Ak = Qk Œõk QTk with eigenvectors
              |            k
              | qi = Qk ei and eigenvalues Œªi = eTi Œõk ei .
blank         | 
              | 
text          | where Qk is an orthogonal matrix of eigenvectors of ATk Ak and Œõk is the diagonal matrix of non-
              | negative eigenvalues. The trust region constraint œëk (¬µ) ‚â§ ‚àÜk with œëk (¬µ) defined in (3.5) is an ellipse
              |                                                            ‚àÜk
              | with principal axis directions qi = Qk ei and lengths T         for i = 1, . . . , m, where ei ‚àà RN¬µ is
              |                                                          ei Œõei
              | the ith canonical vector; see Figure 3.1. Thus the ellipse is stretched (compressed) in directions
              | corresponding small (large) eigenvalues. The matrix ATk Ak represents the sensitivity of the error
              | indicator with respect to the components of ¬µ, which provides intuition to the ellipse interpretation:
              | directions where the error indicator is highly sensitive to perturbations (large eigenvalues) correspond
              | to small principal axes and vice versa.
              |    For the sake of both generality and efficiency, the proposed generalized trust region method
              | will allow the model gradient to be inexact at trust region centers, ¬µk . Aside from the standard
              | assumption imposed on the model function, mk , such as twice-continuous differentiability and uni-
              | formly bounded Hessians, the proposed method requires the existence of functions œëk : RN¬µ ‚Üí R+ ,
              | œïk : RN¬µ ‚Üí R+ and arbitrary constants Œ∂, Œæ > 0 such that
blank         | 
text          |                      |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ Œ∂œëk (¬µ)     ‚àÄ¬µ ‚àà Rk
              |                                                                                                    (3.7)
              |                                   k‚àáF (¬µk ) ‚àí ‚àámk (¬µk )k ‚â§ Œæœïk (¬µk )
blank         | 
text          | where Rk := {¬µ ‚àà RN¬µ | œëk (¬µ) ‚â§ ‚àÜk }. The first bound in (3.7) requires the variation of mk
              | from ¬µk to ¬µ to be related to the variation of F from ¬µk to ¬µ. The does not necessarily place a
              | requirement on the pointwise accuracy of mk with respect to F , even at the trust region center ¬µk .
              | However, a requirement on pointwise accuracy is sufficient to lead to the bound in (3.7) as follows.
              | Suppose there exists a function œák : RN¬µ ‚Üí R+ and arbitrary constant Œ∫ > 0 such that
blank         | 
text          |                                      |F (¬µ) ‚àí mk (¬µ)| ‚â§ Œ∫œák (¬µ).                                   (3.8)
blank         | 
text          | A simple application of the triangle inequality lead to the first bound in (3.7) with Œ∂ = Œ∫ and
              | œëk (¬µ) = œák (¬µk ) + œák (¬µ). The second bound in (3.7) is a requirement on the gradient accuracy
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                              54
blank         | 
              | 
              | 
text          | at the trust region center. The existence of the arbitrary constants implies œëk (¬µ) and œïk (¬µ) are
              | asymptotic error bounds, which will provide considerable flexibility in deriving explicit expressions
              | for them, even for general PDE-constrained optimization problems, in Chapters 5‚Äì6 when specific
              | approximation models are considered.
              |    Each iteration k of the proposed trust region method will rely on four main steps: (1) definition
              | of the trust region model, mk (¬µ), and constraint, œëk (¬µ), (2) computation of a candidate point, ¬µÃÇk ,
              | for the next iterate as the solution of the optimization problem (3.4), (3) computation of the ratio
              | of the actual reduction realized by ¬µÃÇk to that predicted by the model
blank         | 
text          |                                                F (¬µk ) ‚àí F (¬µÃÇk )
              |                                        œÅk =                        ,                                (3.9)
              |                                               mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
text          | and (4) using the value of œÅk , decide whether to accept or reject the candidate, ¬µÃÇk , and how to
              | modify the trust region radius, ‚àÜk . Each of these steps will be detailed in the sections to follow.
              | The generalized trust region algorithm that incorporates these steps is summarized in Algorithm 1.
              | A proof of global convergence, that is, convergence to a local minima from any starting point ¬µ0 , is
              | provided in Appendix A. The computation of the actual-to-predicted reduction ratio (œÅk ) is a severe
              | bottleneck of Algorithm 1 since it requires an evaluation of F . Another approximation model will be
              | introduced to enable an approximation of œÅk to be used in place of the true value without destroying
              | global convergence. Therefore, the modified trust region method, summarized in Algorithm 2,
              | circumvents the primary bottleneck of Algorithm 1.
blank         | 
title         | Step 1: Model and constraint update
blank         | 
text          | The first and most important step in an iteration of the generalized trust region method is the
              | definition of the model function, mk (¬µ), and constraint, œëk (¬µ). To guarantee global convergence,
              | the model must be equipped with error bounds of the form (3.7) and conditions must be placed on
              | the value of the error indicators, œëk (¬µ) and œïk (¬µ), at trust region centers to control the quality of
              | the approximation. The requirement on œëk (¬µk ) is simply
blank         | 
text          |                                             œëk (¬µk ) ‚â§ Œ∫œë ‚àÜk                                      (3.10)
blank         | 
text          | where 0 < Œ∫œë < 1 is an algorithmic constant, which ensures the feasible set of (3.4) is not empty
              | and the trust region center (¬µk ) is in the feasible set. In the special case where œëk (¬µ) is a pointwise
              | error indicator of the form œák (¬µk ) + œák (¬µ), it places a requirement on the accuracy of the model
              | at the trust region center. In the next section and Lemma A.1, this condition will also be used to
              | circumscribe a traditional trust region feasible set (with modified radius) inside the feasible set of
              | (3.4), which enables standard results from trust region theory to be recycled.
              |    The requirement on œïk (¬µk ) is recycled from [93, 108, 109]
blank         | 
text          |                                  œïk (¬µk ) ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }                             (3.11)
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                                       55
blank         | 
              | 
              | 
              | 
text          | Algorithm 1 Error-aware multifidelity trust region method with exact objective evaluations
              |  1:   Initialization: Given
              |                        ¬µ0 , ‚àÜ0 , 0 < Œ≥ < 1, ‚àÜmax > 0, 0 < Œ∑1 < Œ∑2 < 1, 0 < Œ∫œë < 1, 0 < Œ∫œï
              |  2:   Model and constraint update: Choose a model, mk (¬µ), constraint, œëk (¬µ), and gradient
              |       error bound, œïk (¬µ), such that
blank         | 
text          |                            kF (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )k ‚â§ Œ∂œëk (¬µ)            ¬µ ‚àà Rk
              |                                           k‚àáF (¬µk ) ‚àí ‚àámk (¬µk )k ‚â§ Œæœïk (¬µk )
              |                                                            œëk (¬µk ) ‚â§ Œ∫œë ‚àÜk
              |                                                            œïk (¬µk ) ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }
blank         | 
text          |     where Œ∂, Œæ > 0 are arbitrary constants and Rk = {¬µ ‚àà RN¬µ | œëk (¬µ) ‚â§ ‚àÜk }
              |  3: Step computation: Approximately solve the trust region subproblem
blank         | 
text          |                                          min mk (¬µ)       subject to      œëk (¬µ) ‚â§ ‚àÜk
              |                                       ¬µ‚ààRN¬µ
blank         | 
text          |       for a candidate, ¬µÃÇk , that satisfies œëk (¬µÃÇk ) ‚â§ ‚àÜk and
blank         | 
text          |                                                                                              k‚àámk (¬µk )k
blank         |                                                                                                           
text          |                   mk (¬µk ) ‚àí mk (¬µÃÇk ) ‚â• Œ∫s k‚àámk (¬µk )k min (1 ‚àí              Œ∫œë )Œ∫‚àí1
              |                                                                                    ‚àáœë ‚àÜk ,
              |                                                                                                 Œ≤k
blank         | 
text          |       where Œ∫s ‚àà (0, 1), k‚àáœëk (¬µ)k ‚â§ Œ∫‚àáœë for all ¬µ ‚àà Rk , and Œ≤k := 1 + sup                    ‚àá2 mk (¬µ) .
              |                                                                                       ¬µ‚ààRk
              |  4:   Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio approximation
              |       according to
              |                                            F (¬µk ) ‚àí F (¬µÃÇk )
              |                                      œÅk =
              |                                           mk (¬µk ) ‚àí mk (¬µÃÇk )
              |  5:   Step acceptance:
blank         | 
text          |                  if         œÅk ‚â• Œ∑1       then     ¬µk+1 = ¬µÃÇk          else       ¬µk+1 = ¬µk         end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                       if      œÅk ‚â§ Œ∑ 1             then       ‚àÜk+1 ‚àà (0, Œ≥œëk (¬µÃÇk )]              end if
              |                       if      œÅk ‚àà (Œ∑1 , Œ∑2 )      then       ‚àÜk+1 ‚àà [Œ≥œëk (¬µÃÇk ), ‚àÜk ]            end if
              |                       if      œÅk ‚â• Œ∑ 2             then       ‚àÜk+1 ‚àà [‚àÜk , ‚àÜmax ]                 end if
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                           56
blank         | 
              | 
              | 
text          | where Œ∫œï > 0 is an algorithmic constant. The main purpose of the gradient condition is to ensure
              | sufficient accuracy in the model gradient is obtained near convergence (k‚àámk (¬µk )k small) or after
              | failed steps (‚àÜk small). When combined with the error bound in (3.7), it also guarantees a local
              | minima of F is approached as k‚àámk (¬µk )k ‚Üí 0. The error bounds and requirements on the error
              | indicators are summarized in (3.12)-(3.15) below
blank         | 
text          |                  |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ Œ∂œëk (¬µ)     ¬µ ‚àà Rk                    (3.12)
              |                               k‚àáF (¬µk ) ‚àí ‚àámk (¬µk )k ‚â§ Œæœïk (¬µk )                               (3.13)
              |                                               œëk (¬µk ) ‚â§ Œ∫œë ‚àÜk                                 (3.14)
              |                                               œïk (¬µk ) ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }.            (3.15)
blank         | 
text          | where Œæ, Œ∂ > 0 are arbitrary constants.
              |    Traditional trust region methods (œëk (¬µ) = k¬µ ‚àí ¬µk k) that allow for inexact objective and gra-
              | dient evaluations [93, 108, 109] naturally fit requirements (3.12)-(3.15) as follows. Consider an
              | arbitrary model, mk (¬µ), and gradient error bound, œïk (¬µk ), such that (3.13) and (3.15) are satis-
              | fied. Then, œëk (¬µ) = k¬µ ‚àí ¬µk k automatically satisfies (3.12) and (3.14). Condition (3.14) is trivial
              | to verify since œëk (¬µk ) = 0. Condition (3.12) is verified, following [108], by considering the Taylor
              | expansion of F and mk about ¬µk
blank         | 
text          |                                                      1
              |                F (¬µ) = F (¬µk ) + ‚àáF (¬µk )(¬µ ‚àí ¬µk ) + (¬µ ‚àí ¬µk )T ‚àá2 F (y)(¬µ ‚àí ¬µk )
              |                                                      2
              |                                                        1
              |               mk (¬µ) = mk (¬µk ) + ‚àámk (¬µk )(¬µ ‚àí ¬µk ) + (¬µ ‚àí ¬µk )T ‚àá2 mk (z)(¬µ ‚àí ¬µk )
              |                                                        2
blank         | 
text          | where y, z ‚àà RN¬µ are arbitrary points that lie on the line between ¬µ and ¬µk . Subtracting these
              | equations, subsequent rearrangement, and application of the triangle inequality leads to
blank         | 
text          |                                                                         1                             2
              | |F (¬µk )‚àíF (¬µ)+mk (¬µ)‚àímk (¬µk )| ‚â§ k‚àáF (¬µk ) ‚àí ‚àámk (¬µk )k k¬µ ‚àí ¬µk k+       ‚àá2 F (y) ‚àí ‚àámk (z) k¬µ ‚àí ¬µk k .
              |                                                                         2
blank         | 
text          | The gradient condition (3.15) and the fact that ¬µ ‚àà Rk = {y ‚àà RN | ky ‚àí ¬µk k ‚â§ ‚àÜk } are used to
              | reduce the above inequality to
blank         | 
text          |                                                              1
              |                |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ (ŒæŒ∫œï + Œ±k )‚àÜk k¬µ ‚àí ¬µk k
              |                                                              2
blank         | 
text          |                   ‚àá2 F (¬µ) + ‚àá2 mk (¬µ) is well-defined if the objective and model Hessians
blank         |                                            
text          | where Œ±k = sup
              |             ¬µ‚ààRk
              | are uniformly bounded on Rk . Furthermore, assume these Hessians are uniformly bounded on all
              | of RN¬µ , i.e., there exists Œ± > 0 such that Œ±k ‚â§ Œ±. This assumption and the introduction of an
              | algorithmic parameter ‚àÜmax such that ‚àÜk ‚â§ ‚àÜmax (see discussion of step 4) leads to the desired
              | result
              |                            |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ Œ∂œëk (¬µ)
blank         | 
text          | where Œ∂ = (ŒæŒ∫œï + Œ±/2)‚àÜmax is a constant.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            57
blank         | 
              | 
              | 
text          | Remark. A similar generalization of trust region methods was introduced in [208] that used reduced-
              | order models for the approximation model of the linear PDE. However, the method in that work
              | requires the pointwise error bound (3.8) on the objective accuracy. The bound on the objective
              | variation in (3.7) was shown to be a weaker condition than the pointwise bound since (3.8) is a
              | special case of (3.7). Additionally, the objective variation bound provides considerable flexibility
              | compared to the pointwise bound. For example, the bound in (3.7) encompasses the traditional trust
              | region constraint œëk (¬µ) = k¬µ ‚àí ¬µk k without requiring zeroth-order consistency mk (¬µk ) = F (¬µk ),
              | whereas this would would be required by the pointwise bound since œëk (¬µk ) = 0. Therefore the bound in
              | (3.7) enables the generalized trust region method to reduce to a traditional trust region method, even
              | when the model objective is inexact at trust region centers. This will be exploited in Chapters 5‚Äì6.
blank         | 
text          | Step 2: Step candidate as solution of trust region subproblem
blank         | 
text          | The model and trust region constraint functions are used to form the trust region subproblem in
              | (3.4) whose minimizer is used as the candidate for ¬µk+1 . In traditional trust region methods, it is
              | well-known that the trust region subproblem does not need to be solved exactly. In fact, it may be
              | as difficult to solve the trust region subproblem as the original unconstrained optimization problem
              | (3.1). Define the Cauchy point (Definition 3.3) as the minimizer of the trust region subproblem
              | restricted to the steepest decent direction. It turns out that an essential component in the global
              | convergence theory of trust region methods is the decrease in the model realized by the Cauchy
              | point (Theorem 3.2) [133].
blank         | 
text          | Definition 3.3 (Cauchy point). The Cauchy point of the trust region subproblem
blank         | 
text          |                                       minimize     mk (¬µ)
              |                                        ¬µ‚ààRN¬µ
              |                                                                                                 (3.16)
              |                                       subject to   k¬µ ‚àí ¬µk k ‚â§ ‚àÜk
blank         | 
text          |               ‚àó                  ‚àó
              | is ¬µC
              |     k = ¬µk ‚àí s ‚àámk (¬µk ), where s is the solution of the univariate optimization problem
blank         | 
              | 
text          |                                     minimize    mk (¬µk ‚àí s‚àámk (¬µk ))
              |                                       s‚â•0
              |                                                                                                 (3.17)
              |                                     subject to s k‚àámk (¬µk )k ‚â§ ‚àÜk .
blank         | 
text          | Theorem 3.2 (Cauchy decrease). The decrease in the model from ¬µk to the Cauchy point ¬µC
              |                                                                                       k is at
              | least
              |                                                                    kmk (¬µk )k
blank         |                                                                                  
text          |                                             1
              |                     mk (¬µk ) ‚àí   mk (¬µC
              |                                       k)   ‚â• k‚àámk (¬µk )k min                  , ‚àÜk ,            (3.18)
              |                                             2                         Œ≤k
blank         | 
text          | where Œ≤k := 1 + sup    ‚àá2 mk (¬µ) .
              |                 ¬µ‚ààRk
blank         | 
text          | Proof. See Theorem 6.3.1 of [48].
blank         | 
text          |    Therefore, instead of requiring the candidate for ¬µk+1 be the exact minimizer of (3.16), it suffices
              | to use any point that achieves a fraction of the Cauchy decrease [133, 48]. This not only provides
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                           58
blank         | 
              | 
              | 
text          | an opportunity for efficiency, but also a convenient framework for analyzing global convergence
              | properties.
              |    Since these results pertaining to the Cauchy point and its connection to global convergence
              | theory are specific to the quadratic constraint constraint in (3.16), this section aims to generalize
              | these concepts for trust region subproblems of the form (3.4). This can easily be done if the gradient
              | of the constraint is bounded within the trust region, i.e., k‚àáœëk (¬µ)k ‚â§ Œ∫‚àáœë for all ¬µ ‚àà Rk . In this
              | case, Lemma A.1 guarantees {¬µ ‚àà RN¬µ | k¬µ ‚àí ¬µk k ‚â§ (1 ‚àí Œ∫œï )Œ∫‚àí1
              |                                                             ‚àáœë ‚àÜk } ‚äÇ {¬µ ‚àà R
              |                                                                              N¬µ
              |                                                                                 | œëk (¬µ) ‚â§ ‚àÜk }.
              | Thus, from Theorem 3.2, there exists a point in the trust region ¬µ ‚àà Rk such that
blank         | 
text          |                                                              kmk (¬µk )k
blank         |                                                                                            
text          |                mk (¬µk ) ‚àí mk (¬µ) ‚â• Œ∫s k‚àámk (¬µk )k min                   , (1 ‚àí Œ∫œï )Œ∫‚àí1
              |                                                                                     ‚àáœë ‚àÜk       (3.19)
              |                                                                 Œ≤k
blank         | 
text          | where Œ∫s ‚àà (0, 1). Appendix A will show that this condition that resembles the fraction of Cauchy
              | decrease leads to global convergence of the proposed trust region method.
              |    In this work, a substantial cost difference is assumed to separate evaluations of F (¬µ) and ‚àáF (¬µ)
              | from evaluations mk (¬µ) and ‚àámk (¬µ) so the trust region subproblem is solved exactly with little
              | penalty. Section 3.1.2 details an interior-point method to solve the trust region subproblem (3.4).
blank         | 
title         | Step 3: Actual-to-predicted decrease ratio
blank         | 
text          | After the candidate ¬µÃÇk has been computed, the ratio between the reduction in F and mk that would
              | be realized by taking this step is computed according (3.9). This will be used in the next section to
              | determine if the step should be accepted and to modify the trust region radius. The computation
              | of œÅk according to (3.9) requires queries to the expensive function F (¬µ) and therefore constitutes
              | a major bottleneck in the trust region algorithm. Following the work in [109], this bottleneck is
              | mitigated through the introduction of another approximation, œàk (¬µ) that will be used solely in the
              | computation of œÅk , i.e.,
              |                                              œàk (¬µk ) ‚àí œàk (¬µÃÇk )
              |                                       œÅk =                        .                             (3.20)
              |                                              mk (¬µk ) ‚àí mk (¬µÃÇk )
              | Define œàk : RN¬µ ‚Üí R as an approximation of the F (¬µ), equipped with a familiar asymptotic error
              | bound: there exists a constant œÉ > 0 such that
blank         | 
text          |                             |F (¬µk ) ‚àí F (¬µ) + œàk (¬µ) ‚àí œàk (¬µk )| ‚â§ œÉŒ∏k (¬µ),                    (3.21)
blank         | 
text          | where Œ∏k : RN¬µ ‚Üí R+ is an error indicator. To ensure global convergence (see Appendix A for
              | proof) in the presence of this additional approximation, Œ∏k (¬µÃÇk ) must satisfy
blank         | 
text          |                               Œ∏kœâ (¬µÃÇk ) ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk },                    (3.22)
blank         | 
text          | where œâ ‚àà (0, 1), Œ∑ < min{Œ∑1 , 1 ‚àí Œ∑2 }, and {rk }‚àû
              |                                                   k=1 is a sequence such that rk ‚Üí 0. The algorithmic
              | parameters Œ∑1 and Œ∑2 are related to the specifics of the step assessment and radius modification
              | detailed in the next section. The forcing sequence rk is required to ensure œÅk in (3.20) approaches
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                             59
blank         | 
              | 
              | 
text          | the true ratio between the actual and predicted reduction and is taken in this work as rk = 1/(k +1).
              |    The flexibility afforded by the use of an approximation model reveals an immediate and obvious
              | improvement to the generalized trust region algorithm. The error bound required between œàk (¬µ) and
              | Œ∏k (¬µ) in (3.21) is identical to the relationship between mk (¬µ) and œëk (¬µ) in (3.7), which immediately
              | suggests the choice œàk (¬µ) = mk (¬µ) and Œ∏k (¬µ) = œëk (¬µ). From the discussion above, this choice will
              | lead to a globally convergent algorithm provided
blank         | 
text          |                                œëk (¬µÃÇk )œâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }.                    (3.23)
blank         | 
text          | This condition is inexpensive to check since it only involves queries to the approximation model
              | and error indicator and does not require evaluations of the expensive objective function F (¬µ).
              | Additionally, the choice œàk (¬µ) = mk (¬µ) guarantees the approximation of the actual-to-predicted
              | reduction ratio is always unity, i.e.,
blank         | 
text          |                                  œàk (¬µk ) ‚àí œàk (¬µÃÇk )   mk (¬µk ) ‚àí mk (¬µÃÇk )
              |                           œÅk =                        =                      = 1.                (3.24)
              |                                  mk (¬µk ) ‚àí mk (¬µÃÇk )   mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
text          | Thus, for a given iteration k, if the approximation model mk (¬µ) and error indicator œëk (¬µ) chosen
              | in the first step of the generalized trust region algorithm satisfy (3.23), œÅk can be taken as unity
              | without any additional work. The next section will classify such a step as very successful and the
              | step will be accepted ¬µk+1 = ¬µÃÇk and the trust region radius increased. In the event that (3.23)
              | is not satisfied, the choice œàk (¬µ) = mk (¬µ) and Œ∏k (¬µ) = œëk (¬µ) is not sufficient to guarantee global
              | convergence and œàk (¬µ) and Œ∏k (¬µ) must be constructed to satisfy (3.21)-(3.22).
              |    Algorithm 2 states the optimized trust region method that incorporates this additional level
              | of approximation. Appendix A details the global convergence proof for this algorithm. Global
              | convergence of Algorithm 1, i.e., without the œàk approximation model, follows trivially by taking
              | œàk (¬µ) = F (¬µ) and Œ∏k (¬µ) = 0.
blank         | 
title         | Step 4: Step assessment and radius update
blank         | 
text          | Once œÅk is computed according to either (3.9) or (3.20), the quality of the step is assessed by
              | comparing œÅk to unity, i.e., the actual-to-predicted ratio if the model was perfect, mk (¬µ) = F (¬µ).
              | If the œÅk is close to unity, the step is accepted by setting ¬µk+1 = ¬µÃÇk and the trust region radius is
              | increased. In the case where œÅk  1, especially if it is negative (the true objective fails to decrease:
              | F (¬µÃÇk ) > F (¬µk )), the step is rejected, ¬µk+1 = ¬µk , and trust region radius is decreased.
              |    For a practical algorithm, define algorithmic constants 0 < Œ∑1 < Œ∑2 < 1 that will indicate
              | values of œÅk that govern step acceptance and the radius update. If œÅk ‚â§ Œ∑1 , the model did not
              | substantially reduce the true objective so the step is rejected and the trust region radius decreased
              | such that ‚àÜk+1 ‚â§ Œ≥œëk (¬µÃÇk ) where 0 < Œ≥ < 1 is a constant. This will be called an unsuccessful
              | step. Modification of the radius in this manner ensures that if the optimization problem in (3.4)
              | terminates at a point ¬µÃÇk strictly interior to the feasible set Rk = {¬µ ‚àà RN¬µ | œëk (¬µ) ‚â§ ‚àÜk }, the
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                                      60
blank         | 
              | 
              | 
              | 
text          | Algorithm 2 Error-aware multifidelity trust region method with inexact objective evaluations
              |  1:   Initialization: Given
blank         | 
text          |                       ¬µ0 , ‚àÜ0 , 0 < Œ≥ < 1, ‚àÜmax > 0, 0 < Œ∑1 < Œ∑2 < 1, 0 < Œ∫œë < 1, 0 < Œ∫œï ,
              |                                   œâ ‚àà (0, 1), {rk }‚àû
              |                                                    k=1 ‚äÇ [0, ‚àû) such that rk ‚Üí 0
blank         | 
text          |  2:   Model and constraint update: Choose a model, mk (¬µ), constraint, œëk (¬µ), and gradient
              |       error bound, œïk (¬µ), such that
blank         | 
text          |                           |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ Œ∂œëk (¬µ)            ¬µ ‚àà Rk
              |                                          k‚àáF (¬µk ) ‚àí ‚àámk (¬µk )k ‚â§ Œæœïk (¬µk )
              |                                                          œëk (¬µk ) ‚â§ Œ∫œë ‚àÜk
              |                                                          œïk (¬µk ) ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }
blank         | 
text          |     where Œ∂, Œæ > 0 are arbitrary constants and Rk = {¬µ ‚àà RN¬µ | œëk (¬µ) ‚â§ ‚àÜk }
              |  3: Step computation: Approximately solve the trust region subproblem
blank         | 
text          |                                         min mk (¬µ)       subject to          œëk (¬µ) ‚â§ ‚àÜk
              |                                     ¬µ‚ààRN¬µ
blank         | 
text          |       for a candidate, ¬µÃÇk , that satisfies œëk (¬µÃÇk ) ‚â§ ‚àÜk and
blank         | 
text          |                                                                           k‚àámk (¬µk )k
blank         |                                                                                      
text          |                mk (¬µk ) ‚àí mk (¬µÃÇk ) ‚â• Œ∫s k‚àámk (¬µk )k min (1 ‚àí Œ∫œë )Œ∫‚àí1 ‚àÜ
              |                                                                    ‚àáœë k ,                                  (3.25)
              |                                                                              Œ≤k
blank         | 
text          |       where Œ∫s ‚àà (0, 1), k‚àáœëk (¬µ)k ‚â§ Œ∫‚àáœë for all ¬µ ‚àà Rk , and Œ≤k := 1 + sup                   ‚àá2 mk (¬µ)
              |                                                                                       ¬µ‚ààRk
              |  4:   Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio approximation
              |       according to
              |                       Ô£±
              |                       Ô£¥
              |                       Ô£≤          1           if œëk (¬µÃÇk )œâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }
              |                  œÅk =
              |                       Ô£¥ œàk (¬µk ) ‚àí œàk (¬µÃÇk ) otherwise
              |                       Ô£≥
              |                         mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
text          |       where œàk (¬µ) and Œ∏k (¬µ) satisfy
blank         | 
text          |                      kœàk (¬µk ) ‚àí œàk (¬µ) + mk (¬µ) ‚àí mk (¬µk )k ‚â§ œÉŒ∏k (¬µ)              ¬µ ‚àà Rk
              |                                                                                                            (3.26)
              |                                                          Œ∏kœâ (¬µÃÇk )   ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }
blank         | 
text          |     where Œ∑ < min{Œ∑1 , 1 ‚àí Œ∑2 } and œÉ > 0 is an arbitrary constant
              |  5: Step acceptance:
blank         | 
text          |                 if        œÅk ‚â• Œ∑1        then      ¬µk+1 = ¬µÃÇk            else     ¬µk+1 = ¬µk       end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                      if      œÅk ‚â§ Œ∑ 1             then         ‚àÜk+1 ‚àà (0, Œ≥œëk (¬µÃÇk )]            end if
              |                      if      œÅk ‚àà (Œ∑1 , Œ∑2 )      then         ‚àÜk+1 ‚àà [Œ≥œëk (¬µÃÇk ), ‚àÜk ]          end if
              |                      if      œÅk ‚â• Œ∑ 2             then         ‚àÜk+1 ‚àà [‚àÜk , ‚àÜmax ]               end if
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                               61
blank         | 
              | 
              | 
text          | feasible set at the next iteration, Rk+1 , will not include ¬µÃÇk . If œÅk ‚àà (Œ∑1 , Œ∑2 ), the step is accepted
              | and the trust region radius is not modified, ‚àÜk+1 = ‚àÜk . This is called a successful step. Finally,
              | if œÅk ‚â• Œ∑2 , the step is accepted since the model predicted the decrease in the objective to high
              | accuracy. In this type of very successful step, the trust region constraint may be too restrictive
              | so the radius is increased, usually according to ‚àÜk+1 = min{(1/Œ≥)‚àÜk , ‚àÜmax }, where ‚àÜmax is an
              | algorithmic parameter that specifies the maximum trust region radius.
blank         | 
title         | Summary
blank         | 
text          | Two variants of a generalized, multifidelity trust region method were introduced in this section and
              | global convergence was established for both methods in Appendix A. The first version, presented
              | in Algorithm 1, requires the computation of the exact ratio of actual-to-predicted reduction. This
              | method is completely prescribed once the approximation function mk (¬µ) and error indicators œëk (¬µ)
              | and œïk (¬µ) that satisfy (3.12)-(3.15) have been defined. Unlike traditional trust region methods,
              | the trust region subproblem of the proposed method is a difficult optimization problem that cannot
              | leverage the many highly efficient trust region solvers‚Äîsee [48] for a review‚Äîand calls for an exact
              | nonlinear optimization solver. Section 3.1.2 presents a simple primal interior point method based
              | on quasi-Newton search directions and a backtracking linesearch to solve the optimization problem
              | in (3.4) exactly. This guarantees the candidate point ¬µÃÇk will satisfying the fraction of Cauchy
              | decrease condition (Theorem 3.2), an important component of the global convergence theory. The
              | variant of the error-aware multifidelity trust region method presented in Algorithm 2 leverages
              | an approximation of the actual-to-predicted reduction ratio. In additional to mk (¬µ) and the error
              | indicators œëk (¬µ), œïk (¬µ), this method requires the construction of an additional approximation model
              | œàk (¬µ) and error indicator Œ∏k (¬µ) that satisfy (3.21)-(3.22). This flexibility was leveraged to define
              | condition (3.23) that ensures the choice œàk (¬µ) = mk (¬µ) and Œ∏k (¬µ) = œëk (¬µ) will preserve global
              | convergence and guarantees step k is very successful without requiring queries to the expensive
              | objective F (¬µ) or construction of a new approximation model.
              |    As written, Algorithms 1 and 2 are skeletons since details pertaining to the construction of
              | the approximation models mk (¬µ), œàk (¬µ) and error indicators œëk (¬µ), œïk (¬µ), Œ∏k (¬µ) have been ab-
              | stracted away. This will serve as the point of departure for Chapters 5‚Äì6, which will construct
              | these approximation models and error indicators for the specific class of problems under consider-
              | ation. In particular, Chapter 5 will use projection-based reduced-order/hyperreduced models and
              | residual-based error bounds to define these trust region functions in the context of deterministic
              | PDE-constrained optimization. Chapter 6 will combine projection-based reduced-order models and
              | sparse grids to define the approximation model to efficiently solve stochastic PDE-constrained opti-
              | mization problems. The error indicators will use residual-based error bounds to account for pointwise
              | error and dimension-adaptive sparse grids [67] to account for truncation error. The proposed meth-
              | ods will implicitly assume there are relatively few parameters compared to the size of the state vector
              | N¬µ  Nu (since reduction will solely be applied to the state space). Appendix C will discuss the
              | use of linesearch and subspace methods to extend the proposed methods to efficiently handle many
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                             62
blank         | 
              | 
              | 
text          |                                                 œÜŒ≥k (x)
blank         | 
              | 
              | 
              | 
text          |                                                                            x
blank         | 
text          | Figure 3.2: Logarithmic barrier function (3.27) corresponding to mk (x) = x4 ‚àí x3 (     ), œëk (x) = x2 ,
              | ‚àÜk = 1 with Œ≥ = 0.1 (    ) and Œ≥ = 0.0001 (     ).
blank         | 
              | 
text          | optimization variables, i.e., N¬µ = O(Nu ).
blank         | 
              | 
title         | 3.1.2     Interior-Point Method for Trust Region Subproblem
text          | The trust region subproblem employed in the proposed generalized trust region method is a general
              | nonlinear program and cannot be (approximately) solved with the plethora of highly efficient and
              | specialized trust region subproblem solvers that have been developed [133, 48]. In this work, the
              | trust region subproblem is solved exactly (up to a tolerance on the first-order optimality conditions),
              | which is in opposition to most trust region methods that only seek a point that achieves a fraction
              | of the Cauchy decrease. Due to the assumed substantial cost separation between the evaluation
              | of F and mk , an exact trust region solver comes at a relatively small penalty in cost. In fact, it
              | may even be substantially more efficient than finding an approximate minimizer if it can result in
              | even one fewer query to F . While any nonlinear optimization solver can be employed to solve the
              | trust region subproblem (3.4), an interior point method [143] is used since the trust region center is
              | strictly interior to the feasible set from condition (3.14).
              |    Consider the logarithmic barrier function associated with the optimization problem (3.4)
blank         | 
text          |                                 œÜŒ≥k (¬µ) = mk (¬µ) ‚àí Œ≥ log [‚àÜk ‚àí œëk (¬µ)] .                         (3.27)
blank         | 
text          | This function, shown in Figure 3.2 for a specific choice of mk and œëk , tends to +‚àû as ¬µ approaches
              | the boundary of the feasible set. This ensures that an unconstrained optimization problem of the
              | form
              |                                             minimize œÜŒ≥k (¬µ)                                     (3.28)
              |                                              ¬µ‚ààRN¬µ
blank         | 
text          | will remain interior to the feasible set (provided the initial guess is a feasible point). The uncon-
              | strained optimization problem in (3.28) approaches the constrained optimization problem in (3.41)
              | as the barrier parameter goes to zero, i.e., Œ≥ ‚Üí 0. Thus, the constrained optimization problem in
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            63
blank         | 
              | 
              | 
text          | (3.4) has been reduced to a sequence of unconstrained optimization problems (3.28) correspond-
              | ing to a sequence Œ≥p ‚Üí 0. A more robust variant of the primal interior-point method discussed
              | is a primal-dual approach that avoids the difficulty of solving (3.28) when Œ≥ approaches 0; how-
              | ever, only the primal approach will be considered for simplicity. Each unconstrained optimization
              | problem is solved using a quasi-Newton method with Broyden-Fletcher-Goldfarb-Shanno (BFGS)
              | Hessian updates and a backtracking linesearch to satisfy the Armijo sufficient decrease condition.
              | Quasi-Newton methods look to improve an iterate ¬µjk (the subscript k denotes the trust region, or
              | major, iteration and the superscript j denotes the subproblem, or minor, iteration) by search along
              | a direction, pjk , defined as the solution of
blank         | 
text          |                                               Bkj pjk = ‚àí‚àáœÜŒ≥k (¬µjk ),                           (3.29)
blank         | 
text          | where Bkj is a symmetric positive-definite approximation of the Hessian ‚àá2 œÜŒ≥k (¬µjk ). The BFGS
              | Hessian update defines Bkj from Bkj‚àí1 according to
blank         | 
text          |                                                                   T                T
              |                                                         ykj ykj           Bkj sjk sjk Bkj
              |                                   Bkj+1   =   Bkj   +      T
              |                                                                       ‚àí       T
              |                                                                                                 (3.30)
              |                                                         ykj sjk            sjk Bkj sjk
blank         | 
text          | where
              |                            sjk = ¬µj+1
              |                                   k   ‚àí ¬µjk             ykj = ‚àáœÜŒ≥k (¬µj+1     Œ≥   j
              |                                                                      k ) ‚àí ‚àáœÜk (¬µk )
blank         | 
text          | and the Hessian approximation is initialized as the identity Bk0 = I (implying the first search
              | direction p0k is the steepest descent direction). With the search direction computed according to
              | (3.29), the new subproblem iterate is computed as
blank         | 
text          |                                                ¬µj+1
              |                                                 k   = ¬µjk + Œ±pjk ,                              (3.31)
blank         | 
text          | where Œ± > 0 is selected such that the Armijo condition
blank         | 
text          |                                                                                T
              |                                œÜŒ≥k (¬µjk + Œ±pjk ) ‚â§ œÜŒ≥k (¬µjk ) + Œ±cpjk ‚àáœÜŒ≥k (¬µjk )               (3.32)
blank         | 
text          | is satisfied, where c > 0 is a constant usually taken as c = 10‚àí4 . The step length Œ± is determined
              | via a backtracking algorithm, i.e., Œ± = œÑ n , where œÑ ‚àà (0, 1) is the backtrack factor and n ‚â• 0 is the
              | smallest integer such that (3.32) is satisfied. The complete algorithm is summarized in Algorithm 3.
blank         | 
              | 
title         | 3.1.3     Numerical Experiment: Contrived
text          | The generality and performance of the multifidelity trust region method proposed in this chapter is
              | demonstrated on the canonical Rosenbrock problem
blank         | 
text          |                             minimize
              |                                  2
              |                                      F (¬µ) := 100(¬µ2 ‚àí ¬µ21 )2 + (1 ‚àí ¬µ1 )2 .                    (3.33)
              |                                ¬µ‚ààR
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                        64
blank         | 
              | 
              | 
              | 
text          | Algorithm 3 Interior Point BFGS Method with Backtracking Linesearch
              |  1:   Initialization: Given
blank         | 
text          |                                  ¬µ0k = ¬µk , Œ≥, B0 = I, 0 < c < 1, 0 < œÑ < 1
blank         | 
text          |  2:   Search direction computation: Define step direction, pjk , as the solution of
blank         | 
text          |                                                 Bkj pjk = ‚àí‚àáœÜŒ≥k (¬µjk )
blank         | 
text          |  3:   Linesearch: Define the step length as Œ± = œÑ n where n is the smallest integer such that
              |                                                                                     T
              |                                 œÜŒ≥k (¬µjk + œÑ n pjk ) ‚â§ œÜŒ≥k (¬µjk ) + œÑ n cpjk ‚àáœÜŒ≥k (¬µjk )
blank         | 
text          |  4:   Update iterate: Given search direction pjk and step length Œ±, update current iterate
blank         | 
text          |                                                  ¬µj+1
              |                                                   k   = ¬µjk + Œ±pjk
blank         | 
text          |  5:   BFGS update: Define sjk and ykj as
blank         | 
text          |                               sjk = ¬µj+1
              |                                      k   ‚àí ¬µjk            ykj = ‚àáœÜŒ≥k (¬µj+1     Œ≥   j
              |                                                                        k ) ‚àí ‚àáœÜk (¬µk )
blank         | 
text          |       and Hessian approximation update as
              |                                                                     T                   T
              |                                                           ykj ykj           Bkj sjk sjk Bkj
              |                                     Bkj+1   =   Bkj   +      T
              |                                                                         ‚àí       T
              |                                                           ykj sjk            sjk Bkj sjk
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                               65
blank         | 
              | 
              | 
text          | The approximation model and error indicators that will be used are not less expensive to evaluate
              | than the objective function F (¬µ), which is an underlying assumption of the proposed methods. The
              | purpose of this section is to study the behavior of the generalized trust region algorithm on a simple
              | problem; Chapters 5 and 6 will consider more interesting applications where the approximation
              | model and error indicators are substantially less expensive to evaluate than F (¬µ).
              |    The model function will be taken as a quadratic approximation of F (¬µ) with controllable errors
              | introduced into the value and gradient at the expansion point. For this purpose define
blank         | 
text          |                                                                1
              |       G(¬µ; ¬µÃÑ, , Œ¥) := F (¬µÃÑ) +  + (‚àáF (¬µÃÑ) + Œ¥1)T (¬µ ‚àí ¬µÃÑ) + (¬µ ‚àí ¬µÃÑ)T ‚àá2 F (¬µÃÑ)(¬µ ‚àí ¬µÃÑ),       (3.34)
              |                                                                2
blank         | 
text          | where the gradient and Hessian of F (¬µ) are
              |                                           "                                #
              |                                            ‚àí400¬µ1 (¬µ2 ‚àí ¬µ21 ) ‚àí 2(1 ‚àí ¬µ1 )
              |                                ‚àáF (¬µ) =
              |                                                     200(¬µ2 ‚àí ¬µ21 )
              |                                           "                           #                            (3.35)
              |                                 2          400(3¬µ21 ‚àí ¬µ2 ) + 2 ‚àí400¬µ1
              |                               ‚àá F (¬µ) =                                       .
              |                                                    ‚àí400¬µ1             200
blank         | 
text          | The gradient of G(¬µ; ¬µÃÑ, , Œ¥) is
blank         | 
text          |                           ‚àáG(¬µ; ¬µÃÑ, , Œ¥) = ‚àáF (¬µÃÑ) + Œ¥1 + ‚àá2 F (¬µÃÑ)(¬µ ‚àí ¬µÃÑ).                      (3.36)
blank         | 
text          | From the definition of G and its gradient, it is clear that , Œ¥ are the errors in the value and gradient,
              | respectively, at the expansion point ¬µÃÑ since the evaluation of G and ‚àáG at ¬µÃÑ gives
blank         | 
text          |                     G(¬µÃÑ; ¬µÃÑ, , Œ¥) = F (¬µÃÑ) +       ‚àáG(¬µÃÑ; ¬µÃÑ, , Œ¥) = ‚àáF (¬µÃÑ) + Œ¥1.             (3.37)
blank         | 
text          | The approximation model mk (¬µ) is taken as the (inexact) quadratic approximation of F (¬µ) at the
              | trust region center, the trust region constraint is taken based on the exact pointwise objective error,
              | and the gradient error indicator œïk (¬µ) is taken to be the exact gradient error, i.e.,
blank         | 
text          |                  mk (¬µ) := G(¬µ, ¬µk , k , Œ¥k )
              |                   œëk (¬µ) := |F (¬µ) ‚àí G(¬µ; ¬µk , k , Œ¥k )| + |F (¬µk ) ‚àí G(¬µk ; ¬µk , k , Œ¥k )|      (3.38)
              |                   œïk (¬µ) := k‚àáF (¬µ) ‚àí ‚àáG(¬µ; ¬µk , k , Œ¥k )k
blank         | 
text          | where the error terms k , Œ¥k must be chosen based on the requirements of the global convergence
              | theory. With these choices, the error bounds in (3.12)-(3.13) hold with Œ∂ = Œæ = 1. The value of
              | k and Œ¥k will be chosen to ensure the error conditions (3.14) and (3.15) hold. At the trust region
              | centers, the error indicators reduce to the following simple expressions
blank         | 
text          |                                               œëk (¬µk ) = 2k
              |                                                          ‚àö                                         (3.39)
              |                                               œïk (¬µk ) = 2Œ¥k
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            66
blank         | 
              | 
              | 
              | 
text          |                                1.0
blank         | 
              | 
              | 
              | 
text          |                                0.5
blank         | 
              | 
              | 
              | 
text          |                          ¬µ2
              |                                0.0
blank         | 
              | 
              | 
              | 
text          |                               ‚àí0.5
blank         | 
              | 
              | 
              | 
text          |                               ‚àí1.0
              |                                 ‚àí0.2   0.0    0.2   0.4        0.6   0.8   1.0
              |                                                           ¬µ1
blank         | 
              | 
              | 
text          | Figure 3.3: Trajectory of Algorithm 1 as applied to the Rosenbrock problem (3.33). The contours
              | represent the true function F (¬µ), the red dots indicate trust region centers, and the blue line is the
              | trajectory of the trust region subproblem.
blank         | 
              | 
text          | and the error conditions in (3.14)-(3.15) become
blank         | 
text          |                                             Œ∫œë
              |                                        k ‚â§     ‚àÜk
              |                                              2
              |                                             Œ∫œï                                                   (3.40)
              |                                        Œ¥k ‚â§ ‚àö min{k‚àámk (¬µk )k , ‚àÜk }
              |                                                2
blank         | 
text          | Since the right-hand side of the inequality for k is independent of k , admissible values are easily
              | determined and k = Œ∫œë ‚àÜk /2 will be used throughout. The right-hand side of the inequality for Œ¥k
              | depends on Œ¥k itself (through mk (¬µk )) and, in general, an iterative algorithm must be used. A simple
              | backtracking algorithm is used where any initial value of Œ¥k is chosen and reduced by a predefined
              | factor until (3.40) is satisfied.
              |    With the proposed definitions of mk (¬µ), œëk (¬µ), and œïk (¬µ) all ingredients necessary for the com-
              | plete description of Algorithm 1 have been prescribed. The trust region subproblem (3.4) is solved
              | using the BFGS interior-point method described in Section 3.1.2; the non-quadratic trust region
              | constraint eliminates the possibility of using standard trust region solvers such as Steihaug-Toint
              | CG. The trajectory of the optimization iterations‚Äîincluding the progress of the trust region centers
              | and the trajectory of each trust region subproblem‚Äîis shown in Figure 3.3. Figure 3.4 provides
              | additional insight to the Algorithm 1 by showing individual iterations, including the trust region
              | center, candidate step, and feasible region for the trust region subproblem. Notice the substantial
              | difference between the shape of the trust regions in Figure 3.4 and traditional trust regions that are
              | spheres or ellipsoids. These error-aware trust region allows progress to be made toward the optimal
              | solution by searching regions of the parameter space where the model is sufficiently accurate.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                                                                                                     67
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       ¬µ2
              |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       ‚àí0.5                                                          ‚àí0.5                                                   ‚àí0.5
blank         | 
text          |                                             Optimal solution
              |                                             TR center
              |                                             Candidate
              |       ‚àí1.0                                                          ‚àí1.0                                                   ‚àí1.0
              |         ‚àí0.2   0.0   0.2   0.4        0.6     0.8      1.0            ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0          ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  ¬µ1                                                            ¬µ1                                                     ¬µ1
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       ¬µ2
              |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       ‚àí0.5                                                          ‚àí0.5                                                   ‚àí0.5
blank         | 
              | 
              | 
              | 
text          |       ‚àí1.0                                                          ‚àí1.0                                                   ‚àí1.0
              |         ‚àí0.2   0.0   0.2   0.4        0.6     0.8      1.0            ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0          ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  ¬µ1                                                            ¬µ1                                                     ¬µ1
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       ¬µ2
              |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       ‚àí0.5                                                          ‚àí0.5                                                   ‚àí0.5
blank         | 
              | 
              | 
              | 
text          |       ‚àí1.0                                                          ‚àí1.0                                                   ‚àí1.0
              |         ‚àí0.2   0.0   0.2   0.4        0.6     0.8      1.0            ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0          ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  ¬µ1                                                            ¬µ1                                                     ¬µ1
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       ¬µ2
blank         | 
              | 
              | 
              | 
text          |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       ‚àí0.5                                                          ‚àí0.5                                                   ‚àí0.5
blank         | 
              | 
              | 
              | 
text          |       ‚àí1.0                                                          ‚àí1.0                                                   ‚àí1.0
              |         ‚àí0.2   0.0   0.2   0.4        0.6     0.8      1.0            ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0          ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  ¬µ1                                                            ¬µ1                                                     ¬µ1
blank         | 
              | 
              | 
              | 
text          |        1.0                                                           1.0                                                    1.0
blank         | 
              | 
              | 
              | 
text          |        0.5                                                           0.5                                                    0.5
              |  ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                                       ¬µ2
blank         | 
              | 
              | 
              | 
text          |        0.0                                                           0.0                                                    0.0
blank         | 
              | 
              | 
              | 
text          |       ‚àí0.5                                                          ‚àí0.5                                                   ‚àí0.5
blank         | 
              | 
              | 
              | 
text          |       ‚àí1.0                                                          ‚àí1.0                                                   ‚àí1.0
              |         ‚àí0.2   0.0   0.2   0.4        0.6     0.8      1.0            ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0          ‚àí0.2   0.0   0.2   0.4        0.6   0.8   1.0
              |                                  ¬µ1                                                            ¬µ1                                                     ¬µ1
blank         | 
              | 
              | 
              | 
text          | Figure 3.4: Trajectory of Algorithm 1 as applied to the Rosenbrock problem (3.33); iterations
              | proceed from left to right then top to bottom. The contours represent the true function F (¬µ), the
              | red dots indicate trust region centers ¬µk , the blue dots are the candidate for the next trust region
              | center ¬µÃÇk , and the green region indicates the feasible set for the trust region subproblem.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            68
blank         | 
              | 
              | 
              | 
text          |                   101
blank         | 
text          |                  10‚àí2
blank         | 
text          |                  10‚àí5
blank         | 
text          |                  10‚àí8
              |                         0         5           10               15     20          25
              |                                                    iteration
blank         | 
text          | Figure 3.5: Convergence history of the objective quantities using Algorithm 1: F (¬µk ) (  ), F (¬µÃÇk )
              | (    ), mk (¬µk ) (  ), mk (¬µÃÇk ) (  ). Steady progress is made toward the optimal solution, despite
              | the objective and model only agreeing at iteration 0.
blank         | 
              | 
text          |                   102
blank         | 
              | 
text          |                   100
blank         | 
              | 
text          |                  10‚àí2
blank         | 
              | 
text          |                  10‚àí4
              |                         0         5           10               15     20          25
              |                                                    iteration
blank         | 
text          | Figure 3.6: Convergence history of gradient quantities using Algorithm 1: k‚àáF (¬µk )k (         ),
              | k‚àáF (¬µÃÇk )k ( ), k‚àámk (¬µk )k (  ). The gradient of the true objective function decreases 6 orders
              | of magnitude.
blank         | 
              | 
text          |    Figures 3.5 and 3.6 show the convergence history of the objective and gradient quantities, re-
              | spectively. From Figure 3.5 it can be seen that the objective function F (¬µ) continually decreases as
              | the algorithm iterates, despite the fact that the model values mk (¬µ) do not agree well with F (¬µ) at
              | either the trust region centers ¬µk or candidates ¬µÃÇk . Figure 3.6 shows that the first-order optimality
              | condition decreases 6 orders of magnitude throughout the iterations. A more detailed report of the
              | convergence history is provided in Table 3.1.
              |              Table 3.1: Convergence history of Algorithm 1 applied to the Rosenbrock problem.
blank         | 
text          |   F (¬µk )       mk (¬µk )      F (¬µÃÇk )     mk (¬µÃÇk )   k‚àáF (¬µk )k        œÅk            ‚àÜk        Success?
              | 1.0100e+02    1.0150e+02    1.1002e+00   1.0017e-01    2.0001e+02      9.8521e-01   2.0000e+00    True
              | 1.1002e+00    2.1002e+00    9.1177e-01   2.0676e+00    2.1176e+00     5.7778e+00    4.0000e+00    True
              | 9.1177e-01    2.9118e+00    1.5584e+00   2.6152e+00    6.2403e+00    -2.1805e+00    2.6419e-01    False
              | 9.1177e-01    1.0439e+00    6.5172e-01   8.7534e-01    6.2403e+00     1.5431e+00    5.2838e-01    True
              | 6.5172e-01    9.1592e-01    4.8297e-01   8.4624e-01    2.5230e+00     2.4220e+00    1.0568e+00    True
              | 4.8297e-01    1.0114e+00    4.8755e-01   9.8450e-01    4.1851e+00     -1.7086e-01   1.2424e-01    False
              | 4.8297e-01    5.4508e-01    3.2345e-01   4.4402e-01    4.1851e+00     1.5783e+00    2.4847e-01    True
              | 3.2345e-01    4.4769e-01    2.2451e-01   4.0474e-01    2.9992e+00     2.3038e+00    4.9694e-01    True
              | 2.2451e-01    4.7298e-01    2.3107e-01   4.6842e-01    1.9897e+00    -1.4389e+00    5.9337e-02    False
              | 2.2451e-01    2.5418e-01    1.4907e-01   1.8817e-01    1.9897e+00     1.1428e+00    1.1867e-01    True
              | 1.4907e-01    2.0840e-01    8.2419e-02   1.5822e-01    6.8219e+00     1.3281e+00    2.3735e-01    True
              | 8.2419e-02    2.0109e-01    8.6494e-02   2.0100e-01    3.4986e-01    -4.4516e+01    2.8627e-02    False
              | 8.2419e-02    9.6733e-02    7.9495e-02   5.0868e-02    3.4986e-01     6.3763e-02    7.1567e-03    False
              | 8.2419e-02    8.5998e-02    4.3691e-02   3.6534e-02    3.4986e-01     7.8297e-01    1.4313e-02    True
              | 4.3691e-02    5.0847e-02    1.3241e-02   2.4755e-02    5.6732e+00     1.1670e+00    2.8627e-02    True
              | 1.3241e-02    2.7555e-02    4.3950e-03   2.3160e-02    2.5786e-01     2.0128e+00    5.7254e-02    True
              | 4.3950e-03    3.3022e-02    5.5308e-03   3.1229e-02    1.4895e+00     -6.3368e-01   6.4246e-03    False
              | 4.3950e-03    7.6073e-03    9.3057e-04   5.1524e-03    1.4895e+00     1.4112e+00    1.2849e-02    True
              | 9.3057e-04    7.3552e-03    5.4597e-04   7.2723e-03    2.4271e-01     4.6412e+00    2.5699e-02    True
              | 5.4597e-04    1.3395e-02    2.0254e-03   1.2934e-02    2.3541e-02    -3.2065e+00    2.7271e-03    False
              | 5.4597e-04    1.9095e-03    4.7041e-05   1.6024e-03    2.3541e-02     1.6243e+00    5.4543e-03    True
              | 4.7041e-05    2.7742e-03    1.2499e-04   2.7398e-03    1.3032e-01    -2.2700e+00    6.5371e-04    False
              | 4.7041e-05    3.7390e-04    1.2059e-06   3.3688e-04    1.3032e-01     1.2384e+00    1.3074e-03    True
              |                                                                                                             CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD
blank         | 
              | 
              | 
              | 
text          | 1.2059e-06    6.5492e-04    2.1025e-06   6.5473e-04    1.0560e-02    -4.8064e+00    1.6316e-04    False
              | 1.2059e-06    8.2784e-05    4.2021e-08   8.1934e-05    1.0560e-02     1.3691e+00    3.2631e-04    True
meta          |                                                                                                             69
              | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                              70
blank         | 
              | 
              | 
title         | 3.2       Nonlinearly Constrained Optimization
text          | This section extends the unconstrained generalized trust region method introduced in Section 3.1.1
              | to handle nonlinear equality constraints. Consider the nonlinear equality-constrained optimization
              | problem
              |                                          minimize     F (¬µ)
              |                                           ¬µ‚ààRN¬µ
              |                                                                                                   (3.41)
              |                                          subject to c(¬µ) = 0.
blank         | 
text          | This is the exact form of the reduced-space PDE-constrained optimization problem in (2.104) without
              | inequality constraints. The feasible set (Definition 3.4) is an important concept in constrained
              | optimization theory as it defines the set of all points that satisfy the constraints of (3.41).
blank         | 
text          | Definition 3.4 (Feasible set). The set of points that satisfy the constraints of the optimization
              | problem in (3.41)
              |                                       ‚Ñ¶ := {¬µ ‚àà RN¬µ | c(¬µ) = 0}                                   (3.42)
blank         | 
text          | is called the feasible set.
blank         | 
text          |    As in the unconstrained case, it is desirable to find the global minimum of (3.41), i.e., the point ¬µ‚àó
              | such that F (¬µ‚àó ) ‚â§ F (¬µ) for all ¬µ ‚àà ‚Ñ¶; however, due to the inherent difficulty of global optimization
              | we settle for local minima, as defined in Definition 3.5.
blank         | 
text          | Definition 3.5 (Constrained local minima). A point ¬µ‚àó is a local minima of (3.41) if ¬µ‚àó ‚àà ‚Ñ¶ and
              | there is a neighborhood N of ¬µ‚àó such that F (¬µ‚àó ) ‚â§ F (¬µ) for all ¬µ ‚àà N ‚à© ‚Ñ¶.
blank         | 
text          |    Before stating the first-order necessary optimality condition, two concepts must be introduced.
              | The first is the concept of constraint qualifications that provide conditions that must be satisfied for
              | the linearized feasible set to resemble the tangent cone [143]. In this work, we solely consider the
              | Linear Independence Constraint Qualifications (Definition 3.6) that requires linear independence of
              | the gradient of the constraints at a particular point.
blank         | 
text          | Definition 3.6 (Linear Independence Constraint Qualification (LICQ)). The LICQ holds at a point
              |                                                 ‚àÇc
              | ¬µ ‚àà RN¬µ if the rows of the constraint Jacobian,    (¬µ) are linearly independent.
              |                                                 ‚àÇ¬µ
              |    The second concept is the Lagrangian (Definition 3.7) that combines the objective and constraints
              | into a single function by introducing auxiliary variables known as Lagrange multipliers.
blank         | 
text          | Definition 3.7 (Lagrangian). The Lagrangian corresponding to the optimization problem in (3.41)
              | is defined as
              |                                       L(¬µ, œÑ ) = F (¬µ) ‚àí œÑ T c(¬µ),                                (3.43)
blank         | 
text          | where œÑ ‚àà RNc is a vector of Lagrange multipliers.
blank         | 
text          |    Equipped with these concepts, the first-order necessary optimality conditions for ¬µ‚àó to be a local
              | minima (in the sense of Definition 3.5) of (3.41) are stated in Theorem 3.3.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                           71
blank         | 
              | 
              | 
text          | Theorem 3.3 (First-order constrained optimality condition). Suppose ¬µ‚àó is a local minima of
              | (3.41), that F and ci are continuously differentiable, and LICQ holds at ¬µ‚àó. Then there is a
              | Lagrange multiplier vector Œª‚àó such that
blank         | 
text          |                                 ‚àá¬µ L(¬µ‚àó , Œª‚àó ) = 0
              |                                         c(¬µ‚àó ) = 0                                             (3.44)
              |                                     Œª‚àói ci (¬µ‚àó ) = 0   for i ‚àà {1, . . . , Nc }.
blank         | 
text          |    The first condition requires stationarity of the Lagrangian at a local minima and the second
              | requires the feasibility. The last condition is usually referred to as complementarity. There are also
              | second-order necessary and sufficient conditions for ¬µ‚àó to be a local minima of (3.41) that involve
              | the Hessian of the Lagrangian [143]. This will not be considered further as this work will primarily
              | be concerned with first-order optimality conditions.
blank         | 
              | 
title         | 3.2.1     Error-Aware Augmented Lagrangian Multifidelity Trust Region Method
text          | This section extends the multifidelity trust region framework introduced in Section 3.1.1 to handle
              | equality-constrained problems in (3.41). The proposed approach converts the constrained optimiza-
              | tion problem in (3.41) to a sequence of unconstrained problems using the the concept of the aug-
              | mented Lagrangian. The unconstrained multifidelity trust region method proposed in Section 3.1.1
              | is used to solve each unconstrained problem in the sequence for an efficient algorithm that leverages
              | inexpensive approximation models. The augmented Lagrangian corresponding to the optimization
              | problem in (3.41) is
blank         | 
text          |                 LœÑ (¬µ, Œª) := L(¬µ, Œª) + œÑ c(¬µ)T c(¬µ) = F (¬µ) ‚àí ŒªT c(¬µ) + œÑ c(¬µ)T c(¬µ),          (3.45)
blank         | 
text          | where œÑ > 0 is the penalty parameter. A standard result in constrained optimization theory states
              | that, under certain assumptions (Theorem 3.4), there exists a constant œÑÃÑ such that a local minima
              | of (3.41) is a local minima of LœÑ (¬µ, Œª‚àó ) for œÑ ‚â• œÑÃÑ , where Œª‚àó are the Lagrange multipliers at the
              | local minima.
blank         | 
text          | Theorem 3.4. Let ¬µ‚àó be a local minima of (3.41) with Lagrange multipliers Œª‚àó . If the LICQ holds
              | at ¬µ‚àó and the second-order sufficient-conditions hold at (¬µ‚àó , Œª‚àó ) (Theorem 12.6 of [143]), there
              | exists œÑÃÑ such that ¬µ‚àó is a local minima of LœÑ (¬µ, Œª‚àó ) for all œÑ ‚â• œÑÃÑ .
blank         | 
text          | Proof. See Theorem 17.5 of [143].
blank         | 
text          |    Therefore, the optimization problem in (3.41) reduces to a sequence of unconstrained optimization
              | problems of the form
              |                                           minimize LœÑp (¬µ, ŒªÃÇp )                               (3.46)
              |                                            ¬µ‚ààRN¬µ
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                           72
blank         | 
              | 
              | 
text          | for a sequence œÑp ‚Üí œÑ > œÑÃÑ , where ŒªÃÇp are Lagrange multiplier estimates, usually taken as
blank         | 
text          |                                        ŒªÃÇp = ŒªÃÇp‚àí1 ‚àí œÑp‚àí1 c(¬µ‚àóp‚àí1 ),                            (3.47)
blank         | 
text          | where ¬µ‚àóp‚àí1 is the solution of (3.46) at iteration p ‚àí 1. The augmented Lagrangian is employed
              | instead of, e.g., a quadratic penalty function, as equivalence between (3.41) and (3.46) is guaranteed
              | for a finite value of the penalty parameter.
              |    The generalized multifidelity trust region method of Section 3.1.1 applies, without modification,
              | to each unconstrained optimization problem in (3.46), i.e., for a fixed œÑp . In this case, the approx-
              | imation model, mk (¬µ), and constraint functions, œëk (¬µ) and œïk (¬µ), must be constructed such that
              | the conditions in (3.12)-(3.15), applied to the augmented Lagrangian in (3.45) hold, that is,
blank         | 
text          |          |LœÑp (¬µk , ŒªÃÇp ) ‚àí LœÑp (¬µ, ŒªÃÇp ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ Œ∂œëk (¬µ)   ¬µ ‚àà Rk
blank         | 
text          |                             ‚àá¬µ LœÑp (¬µk , ŒªÃÇp ) ‚àí ‚àámk (¬µk ) ‚â§ Œæœïk (¬µk )
              |                                                                                                 (3.48)
              |                                                      œëk (¬µk ) ‚â§ Œ∫œë ‚àÜk
              |                                                     œïk (¬µk ) ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }.
blank         | 
text          | Furthermore, the model and constraint functions must satisfy assumptions (AM1)‚Äì(AM4). The trust
              | region subproblem in (3.4), based on this model and constraint, is solved using the interior-point
              | method outlined in Section 3.1.2.
              |    Global convergence of this equality-constrained variant of the trust region method of Section 3.1.1
              | follows trivially from the global convergence of the generalized trust region method (Appendix A) and
              | Theorem 3.4 provided assumptions (AF1)‚Äì(AF2) hold for the augmented Lagrangian. Assumption
              | (AF1) holds since the objective F and constraint c are assumed twice-continuously differentiable on
              | RN¬µ and assumption (AF2) holds since (3.45) is bounded below provided F is bounded below.
blank         | 
              | 
title         | 3.2.2    Numerical Experiment: Contrived
text          | This section closes with the application of Algorithm 1, embedded in the augmented Lagrangian
              | framework of Section 3.2.1, to solve the following optimization problem with a single nonlinear
              | equality constraint
              |                                       minimize
              |                                            2
              |                                                    ¬µ1 + ¬µ2
              |                                         ¬µ‚ààR
              |                                                                                                 (3.49)
              |                                       subject to ¬µ21 + ¬µ22 ‚àí 2 = 0.
blank         | 
text          | The augmented Lagrangian corresponding to this problem, for a fixed penalty œÑ and Lagrange
              | multiplier estimate Œª, is
blank         | 
text          |                       LœÑ (¬µ, Œª) = ¬µ1 + ¬µ2 + (¬µ21 + ¬µ22 ‚àí 2) œÑ (¬µ21 + ¬µ22 ‚àí 2) ‚àí Œª
blank         |                                                                                  
text          |                                                                                                 (3.50)
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                            73
blank         | 
              | 
              | 
              | 
text          |                                   1.5
blank         | 
              | 
              | 
text          |                                   1.0
blank         | 
              | 
              | 
text          |                                   0.5
blank         | 
              | 
              | 
text          |                                   0.0
blank         | 
              | 
              | 
              | 
text          |                             ¬µ2
              |                                  ‚àí0.5
blank         | 
              | 
              | 
text          |                                  ‚àí1.0
blank         | 
              | 
              | 
text          |                                  ‚àí1.5
              |                                    ‚àí1.5   ‚àí1.0    ‚àí0.5   0.0   0.5   1.0   1.5
              |                                                          ¬µ1
blank         | 
              | 
              | 
text          | Figure 3.7: Trajectory of Algorithm 1 as applied to the constrained problem (3.49). The contours
              | represent the true function F (¬µ), the red dots indicate trust region centers, and the blue line is the
              | trajectory of the trust region subproblem.
blank         | 
              | 
text          | and the resulting sequence of unconstrained optimization problems are
blank         | 
text          |                                             minimize
              |                                                  2
              |                                                      LœÑj (¬µ, Œªj ),                              (3.51)
              |                                                  ¬µ‚ààR
blank         | 
              | 
text          | which will be solved using Algorithm 1. The model mk (¬µ), objective decrease error indicator œëk (¬µ),
              | and gradient error indicator œïk (¬µ) are identical to those in Section 3.1.3 with F (¬µ) replaced with
              | LœÑj (¬µ, Œªj ). The objective and gradient errors k and Œ¥k are similarly chosen using (3.40).
              |    With these definitions of mk (¬µ), œëk (¬µ), and œïk (¬µ) all ingredients necessary for the complete
              | description of Algorithm 1 are set. The trust region subproblem, for fixed œÑj and Œªj , is solved
              | using the BFGS interior-point method described in Section 3.1.2. The trajectory of the optimiza-
              | tion iterations‚Äîincluding the progress of the trust region centers and the trajectory of each trust
              | region subproblem‚Äîare shown in Figure 3.7. These iterations are aggregated over three augmented
              | Lagrangian iterations corresponding to œÑ0 = 10‚àí4 , œÑ1 = 10‚àí5 , and œÑ2 = 10‚àí6 , with Œªj updated
              | according to (3.47) and initialized with Œª0 = 0. Figure 3.8 provides additional insight to the Al-
              | gorithm 1 by showing (selected) individual iterations, including the trust region center, candidate
              | step, and feasible region for the trust region subproblem. Again, notice the substantial difference
              | between the shape of the trust regions in Figure 3.8 with traditional trust regions that are spheres
              | or ellipsoids. From both of these figures, it is clear the iterations converge to a feasible point, as
              | expected from the augmented Lagrangian framework.
              |    Figures 3.9 and 3.10 show the convergence history of the objective and gradient quantities,
              | respectively, aggregated over all three augmented Lagrangian iterations. A dashed vertical line
              | separates augmented Lagrangian iterations. From Figure 3.9 it can be seen that the objective
              | function LœÑj (¬µ, Œªj ) continually decreases within an augmented Lagrangian iteration, despite the
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                                                                                        74
blank         | 
              | 
              | 
text          |           1.5                                                 1.5                                                 1.5
blank         | 
              | 
              | 
text          |           1.0                                                 1.0                                                 1.0
blank         | 
              | 
              | 
text          |     ¬µ2    0.5                                                 0.5                                                 0.5
blank         | 
              | 
              | 
text          |           0.0
blank         | 
              | 
              | 
              | 
text          |                                                         ¬µ2
              |                                                               0.0
blank         | 
              | 
              | 
              | 
text          |                                                                                                             ¬µ2
              |                                                                                                                   0.0
blank         | 
              | 
              | 
text          |          ‚àí0.5                                                ‚àí0.5                                                ‚àí0.5
blank         | 
              | 
              | 
text          |          ‚àí1.0                                                ‚àí1.0                                                ‚àí1.0
blank         | 
              | 
              | 
text          |          ‚àí1.5                                                ‚àí1.5                                                ‚àí1.5
              |            ‚àí1.5   ‚àí1.0   ‚àí0.5   0.0   0.5   1.0   1.5          ‚àí1.5   ‚àí1.0   ‚àí0.5   0.0   0.5   1.0   1.5          ‚àí1.5   ‚àí1.0   ‚àí0.5   0.0   0.5   1.0   1.5
              |                                 ¬µ1                                                  ¬µ1                                                  ¬µ1
blank         | 
              | 
              | 
              | 
text          |           1.5                                                 1.5                                                 1.5
blank         | 
              | 
              | 
text          |           1.0                                                 1.0                                                 1.0
blank         | 
              | 
              | 
text          |           0.5                                                 0.5                                                 0.5
              |     ¬µ2
blank         | 
              | 
              | 
              | 
text          |           0.0
blank         | 
              | 
              | 
              | 
text          |                                                         ¬µ2
              |                                                               0.0
blank         | 
              | 
              | 
              | 
text          |                                                                                                             ¬µ2
              |                                                                                                                   0.0
blank         | 
              | 
              | 
text          |          ‚àí0.5                                                ‚àí0.5                                                ‚àí0.5
blank         | 
              | 
              | 
text          |          ‚àí1.0                                                ‚àí1.0                                                ‚àí1.0
blank         | 
              | 
              | 
text          |          ‚àí1.5                                                ‚àí1.5                                                ‚àí1.5
              |            ‚àí1.5   ‚àí1.0   ‚àí0.5   0.0   0.5   1.0   1.5          ‚àí1.5   ‚àí1.0   ‚àí0.5   0.0   0.5   1.0   1.5          ‚àí1.5   ‚àí1.0   ‚àí0.5   0.0   0.5   1.0   1.5
              |                                 ¬µ1                                                  ¬µ1                                                  ¬µ1
blank         | 
              | 
              | 
              | 
text          | Figure 3.8: Trajectory of Algorithm 1 as applied to the constrained problem (3.49) embedded in
              | the augmented Lagrangian framework. The contours represent the true function F (¬µ), the red dots
              | indicate trust region centers ¬µk , the blue dots are the candidate for the next trust region center ¬µÃÇk ,
              | and the green region indicates the feasible set for the trust region subproblem.
blank         | 
              | 
text          | fact that the model values mk (¬µ) do not agree with LœÑj (¬µ, Œªj ) at either the trust region centers
              | ¬µk or candidates ¬µÃÇk when the algorithm is far from convergence. Figure 3.10 shows that the first-
              | order optimality condition decreases 3 ‚àí 4 orders of magnitude throughout these iterations. A more
              | detailed report of the convergence history is provided in Table 3.2.
meta          | CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD                                        75
blank         | 
              | 
              | 
              | 
text          |                  101
blank         | 
              | 
              | 
              | 
text          |                  100
blank         | 
text          |                        0      5        10        15        20       25        30
              |                                              iterations
blank         | 
text          | Figure 3.9: Convergence history of the augmented Lagrangian objective quantities using Algorithm 1:
              | LœÑj (¬µk ) (    ), LœÑj (¬µÃÇk ) ( ), mk (¬µk ) (    ), mk (¬µÃÇk ) (  ). The three augmented Lagrangian
              | iterations are separated by a vertical dashed line with the following penalty parameters: œÑ0 = 10‚àí4
              | (iterations 0 ‚àí 9), œÑ1 = 10‚àí5 (iterations 10 ‚àí 19), œÑ2 = 10‚àí6 (iterations 20 ‚àí 29).
blank         | 
              | 
              | 
              | 
text          |                   102
blank         | 
text          |                   100
blank         | 
text          |                 10‚àí2
blank         | 
text          |                 10‚àí4
              |                         0      5        10        15       20        25        30
              |                                               iterations
blank         | 
text          | Figure 3.10: Convergence history of the augmented Lagrangian gradient quantities using Algo-
              | rithm 1: k‚àáLœÑj (¬µk )k (     ), k‚àáLœÑj (¬µÃÇk )k (   ), k‚àámk (¬µk )k (    ). The three augmented La-
              | grangian iterations are separated by a vertical dashed line with the following penalty parameters:
              | œÑ0 = 10‚àí4 (iterations 0 ‚àí 9), œÑ1 = 10‚àí5 (iterations 10 ‚àí 19), œÑ2 = 10‚àí6 (iterations 20 ‚àí 29). For
              | each augmented Lagrangian iteration, the gradient of the true augmented Lagrangian (for fixed œÑj )
              | decreases 3 ‚àí 4 orders of magnitude.
              | Table 3.2: Convergence history of Algorithm 1 applied to the constrained problem (3.49). Iterations 0 ‚àí 9: œÑ0 = 10‚àí4 , iterations 10 ‚àí 19:
              | œÑ1 = 10‚àí5 , iterations 20 ‚àí 29: œÑ2 = 10‚àí6 . The norm of the gradient of LœÑj (¬µ), for fixed œÑj , decreases 3 ‚àí 4 orders of magnitude throughout
              | the iterations despite the values of LœÑj (¬µk ) and mk (¬µk ) or LœÑj (¬µÃÇk ) and mk (¬µÃÇk ) not being close until near convergence.
blank         | 
text          |                  LœÑj (¬µk )      mk (¬µk )      LœÑj (¬µÃÇk )    mk (¬µÃÇk )    k‚àáLœÑj (¬µk )k        œÅk            ‚àÜk        Success?
              |                2.5312e+00     3.0312e+00    3.0781e+00     2.0781e+00    2.5125e+00      -5.7368e-01   2.5000e-01      False
              |                2.5312e+00     2.6562e+00    1.9946e+00     1.9971e+00    2.5125e+00      8.1405e-01    5.0000e-01      True
              |                1.9946e+00     2.2446e+00    1.6205e+00     2.0460e+00    2.5410e+00      1.8838e+00    1.0000e+00      True
              |                1.6205e+00     2.1205e+00    1.7166e+00     2.1042e+00    1.0140e+00     -5.9067e+00    9.6906e-02      False
              |                1.6205e+00     1.6689e+00    1.5149e+00     1.5999e+00    1.0140e+00      1.5304e+00    1.9381e-01      True
              |                1.5149e+00     1.6118e+00    1.5188e+00     1.6115e+00    3.1026e-01     -1.4365e+01    2.3193e-02      False
              |                1.5149e+00     1.5265e+00    1.5008e+00     1.5163e+00    3.1026e-01     1.3860e+00     4.6386e-02      True
              |                1.5008e+00     1.5240e+00    1.5013e+00     1.5239e+00    6.7233e-02     -6.9139e+00    5.6381e-03      False
              |                1.5008e+00     1.5036e+00    1.5000e+00     1.5031e+00    6.7233e-02     1.4075e+00     1.1276e-02      True
              |                1.5000e+00     1.5057e+00    1.5001e+00     1.5056e+00    1.2228e-02     -3.5785e+00    1.3905e-03      False
              |                5.1554e+01     5.2054e+01    2.5730e+01     2.4730e+01    1.9955e+02      9.4510e-01    2.0000e+00      True
              |                2.5730e+01     2.6730e+01    3.0107e+00     1.0107e+00    1.5760e+02      8.8336e-01    4.0000e+00      True
              |                3.0107e+00     5.0107e+00    3.4134e+00     -5.8657e-01   3.6765e+01      -7.1943e-02   1.0000e+00      False
              |                3.0107e+00     3.5107e+00    2.1365e+00     1.1365e+00    3.6765e+01      3.6822e-01    5.0000e-01      True
              |                2.1365e+00     2.3865e+00    2.0013e+00     2.2475e+00    1.4813e+01      9.7295e-01    1.0000e+00      True
              |                2.0013e+00     2.5013e+00    2.7229e+00     2.3393e+00    1.4232e+00     -4.4558e+00    9.5895e-02      False
              |                2.0013e+00     2.0492e+00    2.0018e+00     2.0454e+00    1.4232e+00      -1.4312e-01   1.0887e-02      False
              |                2.0013e+00     2.0067e+00    2.0000e+00     2.0054e+00    1.4232e+00      9.5556e-01    2.1775e-02      True
              |                2.0000e+00     2.0109e+00    2.0002e+00     2.0108e+00    3.4607e-02     -1.6948e+00    2.6365e-03      False
              |                2.0000e+00     2.0013e+00    2.0000e+00     2.0013e+00    3.4607e-02     1.6271e+00     5.2730e-03      True
              |                2.0000e+00     2.5000e+00    3.3861e+00     2.3862e+00     8.0931e-01    -1.2175e+01    2.5000e-01      False
              |                2.0000e+00     2.1250e+00    2.3588e+00     2.1088e+00     8.0931e-01    -2.2161e+01    6.2500e-02      False
              |                2.0000e+00     2.0313e+00    2.0165e+00     2.0300e+00     8.0931e-01    -1.3440e+01    3.3820e-03      False
              |                2.0000e+00     2.0017e+00    2.0000e+00     2.0017e+00     8.0931e-01     -3.8261e-02   4.2134e-04      False
              |                                                                                                                                                  CHAPTER 3. GENERALIZED MULTIFIDELITY TRUST REGION METHOD
blank         | 
              | 
              | 
              | 
text          |                2.0000e+00     2.0002e+00    2.0000e+00     2.0002e+00     8.0931e-01    1.0374e+00     8.4269e-04      True
              |                2.0000e+00     2.0004e+00    2.0000e+00     2.0004e+00     2.8971e-02     -8.1375e-01   1.0521e-04      False
              |                2.0000e+00     2.0001e+00    2.0000e+00     2.0001e+00     2.8971e-02      2.2565e-01   1.3148e-05      False
              |                2.0000e+00     2.0000e+00    2.0000e+00     2.0000e+00     2.8971e-02      9.5811e-01   2.6296e-05      True
              |                2.0000e+00     2.0000e+00    2.0000e+00     2.0000e+00     5.5167e-05      3.0124e-01   1.3148e-05      True
              |                2.0000e+00     2.0000e+00    2.0000e+00     2.0000e+00     8.5278e-05    6.9467e+00     2.6296e-05      True
meta          |                                                                                                                                                  76
title         | Chapter 4
blank         | 
title         | Projection-Based Model Reduction
blank         | 
text          | The trust region method introduced in the previous chapter is general in that any approximation
              | model equipped with error bounds (3.12) and (3.13) can be employed. Projection-based model
              | reduction has been shown to be a promising method to dramatically reduce the cost‚Äîin terms of
              | computational time and resources‚Äîof PDE simulations, while retaining a high degree of fidelity
              | [31, 198]. In this approach, the solution of the partial differential equation is sought in a well-
              | chosen, low-dimensional (possibly affine) trial subspace by solving a reduced representation of the
              | governing equations, usually a projection onto a test subspace. It has been shown to yield Reduced-
              | Order Models (ROMs) that are O(105 ) smaller (in terms of number of degrees of freedom) and faster
              | to solve than the original discretized PDE [31, 198], which will be called the High-Dimensional Model
              | (HDM). This makes reduced-order models a promising candidate for the trust region approximation
              | model from the previous chapter.
              |    This chapter provides background necessary to use projection-based reduced-order models as
              | the approximation model in the error-aware trust region method of Chapter 3. The discussion will
              | include the derivation of the primal, sensitivity, and adjoint reduced-order model and computable
              | error bounds. While some of this discussion is a review, there are novel contributions regarding
              | the formulation of a minimum-residual reduced-order model for sensitivity and adjoint equations
              | that guarantee the reduced quantities optimally approximate the HDM counterparts. When the
              | governing equations are nonlinear, a critical bottleneck exists in the evaluation of the ROM that will
              | destroy nearly all resource reduction potential. To eliminate this bottleneck, hyperreduction methods
              | [17, 175, 115, 41, 31, 59] have been developed that introduce an additional level of approximation.
              | Another contribution of this chapter is the extension of the minimum-residual formulation of the
              | primal, sensitivity, and adjoint reduced-order model to a specific hyperreduction method known as
              | collocation.
blank         | 
              | 
              | 
              | 
meta          |                                                  77
              | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                          78
blank         | 
              | 
              | 
title         | 4.1      Global Reduced-Order Models
text          | For simplicity, this section will consider a static, deterministic partial differential equation at the
              | discrete level
              |                                              r(u, ¬µ) = 0,                                          (4.1)
blank         | 
text          | where u ‚àà RNu is the state vector, ¬µ ‚àà RN¬µ is the parameter vector, and r : RNu √ó RN¬µ ‚Üí RNu
              | is the discrete PDE. Most of the developments will extend to the time-dependent case where r is
              | the governing equation and u is the state vector at a single time step. The fundamental ansatz of
              | (global) projection-based model reduction is that the state vector u can be well-approximated in a
              | single low-dimensional subspace
              |                                               u = Œ¶ur ,                                            (4.2)
blank         | 
text          | where Œ¶ ‚àà RNu √óku is the reduced-order basis (basis for the trial subspace), ur ‚àà Rku are the
              | reduced coordinates of u in the basis Œ¶, and ku  Nu . It is also common to consider an affine
              | expansion in (4.2); however, this generalization will not significantly contribute to the following
              | developments and is omitted for clarity. Subsequent sections will use this ansatz to arrive at the
              | primal, sensitivity, and adjoint form of the reduced-order model. A central focus will be the concept
              | of minimum-residual reduced-order models‚Äîdefined such that its solution coincides with the first-
              | order optimality condition of residual minimization over the trial subspace in some norm‚Äîas they
              | possess desirable properties such as monotonicity and interpolation.
blank         | 
              | 
title         | 4.1.1     Primal Formulation
text          | The general form of the projection-based reduced-order model is obtained by substituting the ansatz
              | (4.2) into the governing equation (4.1) and projecting the resulting overdetermined nonlinear system
              | of equations onto a test subspace spanned by the columns of the basis Œ® ‚àà RNu √óku
blank         | 
text          |                                    rr (ur , ¬µ) := Œ®T r(Œ¶ur , ¬µ) = 0,                               (4.3)
blank         | 
text          | where rr : Rku √ó RN¬µ ‚Üí Rku is a nonlinear system of equations with ku equations and unknowns.
              | Define ur (¬µ; Œ¶, Œ®) implicitly as the solution of rr ( ¬∑ , ¬µ) = 0‚Äîthe Implicit Function Theorem
              | (Theorem 2.1) guarantees the existence of such a function and its smoothness with respect to ¬µ. In
              | the remainder, the notation ur (¬µ; Œ¶, Œ®) will be simplified to ur (¬µ) when there is no risk of confusion
              | regarding the choice of test and trial basis. The reduced coordinates must be reconstructed in the
              | full space according to Œ¶ur prior to the evaluation of a quantity of interest, which leads to the
              | definition of the reduced quantity of interest fr : Rku √ó RN¬µ ‚Üí R as
blank         | 
text          |                                      fr (ur , ¬µ; Œ¶) := f (Œ¶ur , ¬µ).                                (4.4)
blank         | 
text          | The reduced quantity of interest becomes purely a function of ¬µ when the implicit definition
              | ur (¬µ; Œ¶, Œ®) is used in the above equation, i.e., when the reduced QoI is only evaluated at solutions
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                             79
blank         | 
              | 
              | 
text          | of the reduced-order model (4.3),
blank         | 
text          |                                  Fr (¬µ; Œ¶, Œ®) := f (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ).                               (4.5)
blank         | 
text          | The implicit function Fr ( ¬∑ ; Œ¶, Œ®) : RN¬µ ‚Üí R is the reduced-order model approximation of the
              | quantity of interest F : RN¬µ ‚Üí R, defined as F (¬µ) := f (u(¬µ), ¬µ), where u(¬µ) is the solution of
              | r( ¬∑ , ¬µ) = 0. The notation for the reduced quantity of interest in (4.4) and (4.5) will be simplified
              | to fr (ur , ¬µ) and Fr (¬µ), respectively, when there is no risk confusion.
              |    In the general case, the test basis may be non-constant, i.e., Œ® = Œ®(u, ¬µ). Two common choices
              | for the test basis are
              |                                                            ‚àÇr
              |                                Œ®=Œ¶            and     Œ®=      (Œ¶ur , ¬µ)Œ¶,                            (4.6)
              |                                                            ‚àÇu
              | which correspond to a Galerkin and Least-Squares Petrov-Galerkin [28, 31] projection, respectively.
              | At this point, there have been no restrictions placed on either the test or trial bases‚Äîaside from the
              | implicit requirement that they are valid bases, i.e., their columns are linearly independent‚Äînor has
              | any relationship between these bases been specified. Next, the concept of minimum-residual reduced-
              | order models will be introduced that equips the reduced-order models with desirable properties:
              | (1) monotonicity‚Äîthe quality of the solution can only improve (in some well-defined metric) as the
              | trial space is hierarchically refined and (2) interpolation‚Äîthe reduced-order model will recover the
              | HDM solution if it lies in the trial space.
blank         | 
text          | Definition 4.1 (Minimum-Residual Property). A reduced-order model possesses the minimum-
              | residual property if the solution satisfies the first-order optimality conditions of the following residual
              | minimization problem
              |                                                     1             2
              |                                       minimize        kr(Œ¶ur , ¬µ)kŒò                                  (4.7)
              |                                        ur ‚ààRku      2
              | for some symmetric positive-definite Œò ‚àà RNu √óNu .
blank         | 
text          | Proposition 4.1. Let (Œ¶, Œ®, Œò) define a minimum-residual reduced-order model whose solution
              | coincides with the global minimum of (4.7). Then, the following properties hold for any ¬µ ‚àà RN¬µ :
blank         | 
text          |    ‚Ä¢ (Optimality) For any u ‚àà col(Œ¶),
blank         | 
text          |                                    kr(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)kŒò ‚â§ kr(u, ¬µ)kŒò                               (4.8)
blank         | 
              | 
text          |    ‚Ä¢ (Monotonicity) Let (Œ¶0 , Œ®0 ) define a projection-based reduced-order model such that col(Œ¶0 ) ‚äÜ
              |       col(Œ¶), then
              |                            kr(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)kŒò ‚â§ r(Œ¶0 ur (¬µ; Œ¶0 , Œ®0 ), ¬µ)       Œò
              |                                                                                                      (4.9)
blank         | 
text          |    ‚Ä¢ (Interpolation) If u(¬µ) ‚àà col(Œ¶), then
blank         | 
text          |                          r(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) = 0        and     u(¬µ) = Œ¶ur (¬µ; Œ¶, Œ®)                (4.10)
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                             80
blank         | 
              | 
              | 
text          | Proof. Optimality follows trivially from the fact that ur (¬µ; Œ¶, Œ®) is the global minima of the opti-
              | mization problem in (4.7). A simple application of the optimality property to u = Œ¶0 ur (¬µ; Œ¶0 , Œ®0 ) ‚àà
              | col(Œ¶0 ) ‚äÜ col(Œ¶) leads to monotonicity. Finally, if the exact solution of r( ¬∑ , ¬µ) = 0 is contained in
              | the columnspace of Œ¶, i.e., u(¬µ) ‚àà col(Œ¶), the optimality property implies
blank         | 
text          |                             kr(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)kŒò ‚â§ kr(u(¬µ), ¬µ)kŒò = 0.                             (4.11)
blank         | 
text          | This result, along with the assumed uniqueness of solutions of r( ¬∑ , ¬µ) = 0 (Assumption 2.2), leads
              | to the interpolation property in (4.10).
blank         | 
text          | Remark. The results in Proposition 4.1 only hold if the solution of the minimum-residual reduced-
              | order model coincides with the global solution of the optimization problem in (4.7). In general this
              | is only guaranteed if the optimization problem is convex, which will be the case if the governing
              | equation is affine in its first argument, i.e., r(u, ¬µ) = A(¬µ)u + b(¬µ), where A(¬µ) ‚àà RNu √óNu and
              | b(¬µ) ‚àà RNu . When the optimization problem in (4.7) is non-convex, the stationary point that will be
              | found by the minimum-residual reduced-order model will not necessarily be the global minima of (4.7).
              | A heuristic that, in practice, is usually sufficient to lead to the results in Proposition 4.1 (optimality,
              | monotonicity, and interpolation) is to initialize the reduced-order model solver with a quality starting
              | point. Due to the required training phase in model reduction (Section 4.3), a reasonable starting point
              | can typically be obtained via interpolation of the training data [198].
blank         | 
text          |    From Appendix B, the approximation of the reduced quantity of interest is equipped with a
              | residual-based error bound (Lemma B.4) that takes the form
blank         | 
text          |                       |F (¬µ) ‚àí Fr (¬µ)| ‚â§ Œ∫ kr(Œ¶ur (¬µ), ¬µ)k ‚â§ Œ∫0 kr(Œ¶ur (¬µ), ¬µ)kŒò                    (4.12)
blank         | 
text          | for some constants Œ∫, Œ∫0 > 0, where the second inequality follows from norm equivalence in finite di-
              | mensions. The residual-based error bound illuminates one motivation behind the minimum-residual
              | formulation: it minimizes the error bound over the columnspace of Œ¶.
              |    A general relationship between projection-based reduced-order models and minimum-residual
              | reduced-order models (Definition 4.1) is established by matching terms in the first-order optimality
              | condition of (4.7), i.e.,
              |                                                      T
              |                                       ‚àÇr
              |                                          (Œ¶ur , ¬µ)Œ¶        Œòr(Œ¶ur , ¬µ) = 0                          (4.13)
              |                                       ‚àÇu
              | with the form of the projection-based reduced-order model in (4.3). From these two equations, the
              | relationship
              |                                                            ‚àÇr
              |                                          Œ®(u, ¬µ) = Œò          (u, ¬µ)Œ¶                               (4.14)
              |                                                            ‚àÇu
              | is sufficient for a general projection-based reduced-order model in (4.3) to possess the minimum-
              | residual property.
              |                                                              ‚àÇr
              |    The LSPG reduced-order model in (4.6), i.e., Œ®(u, ¬µ) =        (u, ¬µ)Œ¶, satisfies the condition in
              |                                                              ‚àÇu
              | (4.14) with Œò = I, where I is the Nu identity matrix, and therefore possesses the minimum-residual
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                         81
blank         | 
              | 
              | 
text          | property. For problems with symmetric positive-definite Jacobian matrices
blank         | 
text          |                                  ‚àÇr
              |                                     (u, ¬µ)  0     ‚àÄu ‚àà RNu , ¬µ ‚àà RN¬µ
              |                                  ‚àÇu
blank         | 
text          | the Galerkin reduced-order model in (4.6), i.e., Œ® = Œ¶, possesses the minimum-residual property
              | in the metric defined by the Jacobian inverse transpose evaluated at the (reconstructed) solution of
              | the Galerkin reduced-order model (Œ¶ur (¬µ; Œ¶, Œ¶)), i.e.,
blank         | 
text          |                                           ‚àÇr
              |                                     Œò=       (Œ¶ur (¬µ; Œ¶, Œ¶), ¬µ)‚àíT .                             (4.15)
              |                                           ‚àÇu
blank         | 
text          | This is a valid metric since: (1) the Jacobian is evaluated at a specific state and (2) the Jacobian
              | is symmetric positive-definite at any state (by assumption) and therefore its inverse transpose is
              | symmetric positive-definite. This metric reduces the first-order optimality conditions (4.13) of the
              | residual minimization problem in (4.7) to: find y ‚àà Rku such that
              |                                     T
              |                           ‚àÇr            ‚àÇr
              |                              (Œ¶y, ¬µ)Œ¶      (Œ¶ur (¬µ; Œ¶, Œ¶), ¬µ)‚àíT r(Œ¶y, ¬µ) = 0
              |                           ‚àÇu            ‚àÇu
blank         | 
text          | for a fixed ¬µ ‚àà RN¬µ . It is easily verified that the solution of the Galerkin reduced-order model
              | satisfies the above equation, i.e., with y = ur (¬µ; Œ¶, Œ¶). Therefore Galerkin reduced-order models
              | possess the minimum-residual property in the metric in (4.15) for problems with symmetric positive-
              | definite Jacobians.
blank         | 
text          | Remark. To this point, minimum-residual reduced-order models have been interpreted as a specific
              | projection of the governing equations
              |                                               r(u, ¬µ) = 0.
blank         | 
text          | Given the existence and uniqueness assumption (Assumption 2.2) regarding solutions of the above
              | equation, it can equivalently be formulated as the solution of the minimum-residual optimization
              | problem
              |                                                    1          2
              |                                         minimize     kr(u, ¬µ)kŒò ,                               (4.16)
              |                                          u‚ààRNu     2
              | where Œò is a symmetric, positive-definite matrix, with first-order optimality condition
blank         | 
text          |                                         ‚àÇr
              |                                            (u, ¬µ)T Œòr(u, ¬µ) = 0.                                (4.17)
              |                                         ‚àÇu
blank         | 
text          | From this equation and (4.13), it is clear that minimum-residual reduced-order models are equiva-
              | lently derived as a Galerkin projection, with Œ¶ as the test and trial basis, of the governing equations
              | in minimum-residual form (4.17).
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                      82
blank         | 
              | 
              | 
title         | 4.1.2        Exact and Minimum-Residual Sensitivity Formulation
text          | The intention of this work is to use the reduced-order models of the previous section in a gradient-
              | based, reduced-space PDE-constrained optimization setting, in an attempt to reduce the overall
              | computational cost. This requires a discussion regarding gradients of the reduced quantities of
              | interest. Both the sensitivity and adjoint approaches will be detailed for this purpose. Following
              | the procedure outlined in Section 2.3.3, the total derivative of (4.5) is expanded as
blank         | 
text          |                             ‚àÇf                      ‚àÇf                     ‚àÇur
              |       ‚àáFr (¬µ; Œ¶, Œ®) =          (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) +    (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)Œ¶     (¬µ; Œ¶, Œ®).                    (4.18)
              |                             ‚àÇ¬µ                      ‚àÇu                     ‚àÇ¬µ
blank         | 
text          |                          ‚àÇur
              | The reduced sensitivities    (¬µ; Œ¶, Œ®)1 are derived by considering the total variation of the
              |                           ‚àÇ¬µ
              | reduced-order model in (4.3) with respect to perturbations in ¬µ. In the general case where Œ®
              | is state- and parameter-dependent, the reduced sensitivities are defined as the solution of the linear
              | equations
              |                  Ô£Æ                           Ô£π        Ô£Æ                           Ô£π
              |                    Nu     ‚àÇ Œ®T ej          ‚àÇr    ‚àÇu
              |                                                           Nu     ‚àÇ   Œ® T
              |                                                                          ej        ‚àÇr
              |                                                    r
              |                   X                                      X
              |                  Ô£∞     rj           Œ¶ + Œ®T    Œ¶Ô£ª     = ‚àíÔ£∞     rj              + Œ®T    Ô£ª                      (4.19)
              |                    j=1
              |                              ‚àÇu            ‚àÇu    ‚àÇ¬µ       j=1
              |                                                                      ‚àÇ¬µ            ‚àÇ¬µ
blank         | 
              | 
text          | where all terms are evaluated at the reconstructed primal solution, Œ¶ur (¬µ; Œ¶, Œ®) of the reduced-
              | order model (4.3). In the special case where the primal reduced-order model is exact, i.e.,
              | r(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) = 0, or the test basis is constant, the expression in (4.19) reduces to
blank         |                                                  
text          |                                               ‚àÇr    ‚àÇur       ‚àÇr
              |                                            Œ®T    Œ¶      = ‚àíŒ®T    .                                           (4.20)
              |                                               ‚àÇu    ‚àÇ¬µ        ‚àÇ¬µ
blank         | 
text          | A Galerkin projection employs a constant test basis Œ® = Œ¶ and the equation for the reduced
              | sensitivity in (4.19) or (4.20) reduces to
blank         |                                                   
text          |                                              T ‚àÇr    ‚àÇur       ‚àÇr
              |                                             Œ¶     Œ¶      = ‚àíŒ¶T    .                                          (4.21)
              |                                                ‚àÇu    ‚àÇ¬µ        ‚àÇ¬µ
blank         | 
text          |                                                            ‚àÇr
              | A LSPG projection employs a non-constant test basis Œ® =       Œ¶ and the derivatives of the test basis
              |                                                            ‚àÇu
              | in (4.19) cannot be ignored in the general case where the primal solution is not exact. In this case,
              | the reduced sensitivities are the solution of the following equation
              |        Ô£Æ                                  Ô£π         Ô£Æ                               Ô£π
              |          Nu          2              T                 Nu          2            T
              |         X          ‚àÇ   rj        ‚àÇr   ‚àÇr    ‚àÇu r
              |                                                      X          ‚àÇ   rj      ‚àÇr   ‚àÇr
              |        Ô£∞     rj Œ¶T        Œ¶ + Œ¶T         Œ¶Ô£ª      = ‚àíÔ£∞     rj Œ¶T        + Œ¶T         Ô£ª.                       (4.22)
              |          j=1
              |                    ‚àÇu‚àÇu          ‚àÇu   ‚àÇu    ‚àÇ¬µ        j=1
              |                                                                 ‚àÇu‚àÇ¬µ        ‚àÇu   ‚àÇ¬µ
blank         | 
text          |    1 This                                                   ‚àÇur                          ‚àÇur
              |             notation is simplified from ‚àáFr (¬µ; Œ¶, Œ®) and       (¬µ; Œ¶, Œ®) to ‚àáFr (¬µ) and     (¬µ), respectively, when
              |                                                             ‚àÇ¬µ                           ‚àÇ¬µ
              | there is no risk of confusion.
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                          83
blank         | 
              | 
              | 
text          | Despite the many advantages of minimum-residual reduced-order models (Proposition 4.1), a major
              | disadvantage is that they complicate sensitivity analysis due to the required partial derivatives of the
              | test basis in (4.19). This is particularly true in the case of LSPG where the first-order sensitivities,
              | ‚àÇur
              |      , require second-order information about the partial differential equation‚Äîinformation that
              |  ‚àÇ¬µ
              | is rarely available is large-scale PDE implementations. For this reason, the theory of minimum-
              | residual primal reduced-order models is extended to the sensitivity equations in an attempt to avoid
              | terms involving derivatives of the test basis, while generating reduced sensitivities that optimally
              | reconstruct the HDM sensitivities. The main drawback of this approach is the computed sensitivities
              | will not be consistent with the reduced-order model to which they correspond since they will not
              | coincide with the solution of (4.19).
              |    Before embarking on the discussion of minimum-residual sensitivity analysis, recall the definition
              | of the sensitivity residual
blank         | 
text          |                                                  ‚àÇr          ‚àÇr
              |                               r ‚àÇ (u, w, ¬µ) =       (u, ¬µ) +    (u, ¬µ)w,                         (4.23)
              |                                                  ‚àÇ¬µ          ‚àÇu
blank         | 
text          | and the generalization of the gradient of the quantity of interest
blank         | 
text          |                                                  ‚àÇf          ‚àÇf
              |                                g ‚àÇ (u, w, ¬µ) =      (u, ¬µ) +    (u, ¬µ)w                          (4.24)
              |                                                  ‚àÇ¬µ          ‚àÇu
blank         | 
text          | introduced in Section 2.3.3. With this notation, the gradient of the reduced quantity of interest
              | takes the form                                                  
              |                                                        ‚àÇur
              |                               ‚àáFr (¬µ) = g ‚àÇ Œ¶ur (¬µ), Œ¶     (¬µ), ¬µ .                              (4.25)
              |                                                        ‚àÇ¬µ
              |                                                                      ‚àÇur
              | Instead of considering the reconstructed reduced sensitivity, Œ¶          , as an approximation for the
              |                                                                      ‚àÇ¬µ
              |                   ‚àÇu
              | HDM sensitivity      , an approximation of the form
              |                   ‚àÇ¬µ
blank         | 
text          |                                             ‚àÇu      ‚àÇu
              |                                                     dr
              |                                                = Œ¶‚àÇ                                              (4.26)
              |                                             ‚àÇ¬µ      ‚àÇ¬µ
blank         | 
text          | will be considered where Œ¶‚àÇ ‚àà RNu √óku is a reduced-order basis (linearly independent columns) for
              |                       ‚àÇu
              |                       dr
              | the sensitivities and    ‚àà Rku √óN¬µ are the reduced coordinates. The reduced coordinates will be
              |                       ‚àÇ¬µ
              | defined as the argument that minimizes the sensitivity residual
blank         | 
text          |                      ‚àÇu
              |                      dr                               1 ‚àÇ                      2
              |                         (¬µ; Œ¶‚àÇ , Œò‚àÇ , u) = arg min      r (u, Œ¶‚àÇ wr , ¬µ)            ,            (4.27)
              |                      ‚àÇ¬µ                   wr ‚ààRku √óN¬µ 2                        Œò‚àÇ
blank         | 
              | 
text          | where u ‚àà RNu is any linearization point, usually the reconstructed primal solution, i.e., u =
              | Œ¶ur (¬µ; Œ¶, Œ®) and Œò‚àÇ  0 is the metric defining the norm. The first-order optimality condition of
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                                  84
blank         | 
              | 
              | 
text          | the linear least-squares problem in (4.27) leads to the normal equations
              |                          T                                    d                   T
              |            ‚àÇr                                 ‚àÇr                  ‚àÇur      ‚àÇr                ‚àÇr
              |               (u, ¬µ)Œ¶‚àÇ           Œò   ‚àÇ
              |                                                  (u, ¬µ)Œ¶‚àÇ             =‚àí      (u, ¬µ)Œ¶‚àÇ
              |                                                                                           Œò‚àÇ    (u, ¬µ).                   (4.28)
              |            ‚àÇu                                 ‚àÇu                  ‚àÇ¬µ       ‚àÇu                ‚àÇ¬µ
blank         | 
text          | The following proposition parallels Proposition 4.1 for the minimum-residual sensitivity approxima-
              |                                                                           ‚àÇu
              |                                                                           dr
              | tion and provides conditions that result in the reduced sensitivities, Œ¶‚àÇ    , exactly reconstructing
              |                                                                           ‚àÇ¬µ
              |                        ‚àÇu
              | the HDM sensitivities,    .
              |                        ‚àÇ¬µ
              | Proposition 4.2. Let (Œ¶‚àÇ , Œò‚àÇ ) define a minimum-residual sensitivity reduced-order model. Then,
              | the following properties hold for any ¬µ ‚àà RN¬µ :
blank         | 
text          |    ‚Ä¢ (Optimality) For any u ‚àà RNu and w ‚àà col(Œ¶‚àÇ )
              |                                                              !
              |                                       ‚àÇu
              |                                       dr
              |                                               ‚àÇ‚àÇ  ‚àÇ
              |                            rk‚àÇ   u, Œ¶    (¬µ; Œ¶ , Œò , u)ek , ¬µ                               ‚â§ rk‚àÇ (u, w, ¬µ)   Œò‚àÇ
              |                                                                                                                           (4.29)
              |                                       ‚àÇ¬µ
              |                                                                                        Œò‚àÇ
blank         | 
              | 
text          |      for k = 1, . . . , N¬µ , where rk‚àÇ (u, w ¬∑ ek , ¬µ) := r ‚àÇ (u, weTk , ¬µ)ek and ek is the kth canonical
              |      unit vector.
              |                                           0         0
              |    ‚Ä¢ (Monotonicity) Let (Œ¶‚àÇ , Œò‚àÇ ) define a minimum-residual sensitivity reduced-order model such
              |                   0
              |      that col(Œ¶‚àÇ ) ‚äÜ col(Œ¶‚àÇ ), then
blank         | 
text          |                                                    rk‚àÇ (u, w, ¬µ)   Œò‚àÇ
              |                                                                         ‚â§ rk‚àÇ (u, w0 , ¬µ)       Œò‚àÇ
              |                                                                                                      ,                    (4.30)
blank         | 
text          |                           ‚àÇu
              |                           dr                                0 ‚àÇu
              |                                                               dr       0    0
              |      where w = Œ¶‚àÇ            (¬µ; Œ¶‚àÇ , Œò‚àÇ , u)ek and w0 = Œ¶‚àÇ      (¬µ; Œ¶‚àÇ , Œò‚àÇ , u)ek , for any u ‚àà RNu .
              |                           ‚àÇ¬µ                                  ‚àÇ¬µ
              |                              ‚àÇu
              |    ‚Ä¢ (Interpolation) If          (¬µ) ‚àà col(Œ¶‚àÇ ) for k ‚àà {1, . . . , N¬µ }, then
              |                              ‚àÇ¬µk
              |                                                                                                   !
              |                                                            ‚àÇu
              |                                                            dr
              |                                                             ‚àÇ
              |                                          rk‚àÇ       u(¬µ), Œ¶    (¬µ; Œ¶‚àÇ , Œò‚àÇ , u(¬µ))ek , ¬µ                  =0
              |                                                            ‚àÇ¬µ
              |                                                                                                                           (4.31)
              |                                                         ‚àÇu           ‚àÇu
              |                                                                      dr
              |                                                             (¬µ) = Œ¶‚àÇ    (¬µ; Œ¶‚àÇ , Œò‚àÇ , u(¬µ))ek .
              |                                                         ‚àÇ¬µk          ‚àÇ¬µ
blank         | 
              | 
text          |                                                        ‚àÇu
              |                                                        dr
              | Proof. Optimality follows trivially from the fact that Œ¶‚àÇ  (¬µ; Œ¶‚àÇ , Œò‚àÇ , u) is the (unique) minima
              |                                                         ‚àÇ¬µ
              | of the optimization problem in (4.27). Monotonicity follows directly from the optimality property
meta          |                       0
text          | since w0 ‚àà col(Œ¶‚àÇ ) ‚äÜ col(Œ¶‚àÇ ). Finally, if the exact solution of rk‚àÇ (u(¬µ), ¬∑ , ¬µ) = 0 is contained in
              |                               ‚àÇu
              | the columnspace of Œ¶‚àÇ , i.e.,     (¬µ) ‚àà col(Œ¶‚àÇ ), the optimality property implies
              |                               ‚àÇ¬µk
              |                                                                    !                                   
              |                    ‚àÇu
              |                    dr
              |                     ‚àÇ                                                                         ‚àÇu
              |     rk‚àÇ    u(¬µ), Œ¶    (¬µ; Œ¶‚àÇ , Œò‚àÇ , u(¬µ))ek , ¬µ                              ‚â§   rk‚àÇ    u(¬µ),     (¬µ), ¬µ           = 0.   (4.32)
              |                    ‚àÇ¬µ                                                                         ‚àÇ¬µk             Œò‚àÇ
              |                                                                         Œò‚àÇ
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                        85
blank         | 
              | 
              | 
text          | The interpolation property in (4.31) follows from this and the fact that for a given u ‚àà RNu and
              | ¬µ ‚àà RN¬µ , the solution of rk‚àÇ (u, ¬∑ , ¬µ) = 0 is unique (due to invertibility of the Jacobian and linear
              | independence of the columns of Œ¶‚àÇ ).
blank         | 
text          |    The proposed minimum-residual sensitivity approximation is used to reconstruct an approxima-
              | tion of the gradient of the quantity of interest as
              |                                                                                          !
              |                                                ‚àÇ
              |                         dr (¬µ; Œ¶, Œ®, Œ¶ , Œò ) := g      ‚àÇ    ‚àÇ        ‚àÇu
              |                                                                      dr
              |                                                                        ‚àÇ     ‚àÇ   ‚àÇ
              |                ‚àáF (¬µ) ‚âà ‚àáF                                      u, Œ¶    (¬µ; Œ¶ , Œò , u), ¬µ ,                     (4.33)
              |                                                                      ‚àÇ¬µ
blank         | 
text          | where u = Œ¶ur (¬µ; Œ¶, Œ®) is the reconstructed primal solution2 . From Appendix B, the approxima-
              | tion of the gradient of the reduced QoI is equipped with a residual-based error bound (Lemma B.4)
              | that takes the form
              |                                                                                                         !
              |           dr (¬µ; Œ¶, Œ®, Œ¶‚àÇ , Œò‚àÇ ) ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r ‚àÇ                           ‚àÇu
              |                                                                                  dr                
              |  ‚àáF (¬µ) ‚àí ‚àáF                                                               u, Œ¶‚àÇ      ¬µ; Œ¶‚àÇ , Œò‚àÇ , u , ¬µ
              |                                                                                  ‚àÇ¬µ
              |                                                                                                           !
              |                                                    0               0       ‚àÇ       ‚àÇ‚àÇu
              |                                                                                     dr 
              |                                                                                              ‚àÇ   ‚àÇ
blank         |                                                                                                      
text          |                                             ‚â§ Œ∫ kr(u, ¬µ)kŒò + œÑ         r       u, Œ¶      ¬µ; Œ¶ , Œò , u , ¬µ
              |                                                                                     ‚àÇ¬µ
              |                                                                                                              Œò‚àÇ
              |                                                                                                         (4.34)
              | for some constants Œ∫, Œ∫0 , œÑ, œÑ 0 > 0, where u = Œ¶ur (¬µ; Œ¶, Œ®) and the second inequality follows from
              | norm equivalence in finite dimensions. The residual-based error bound illuminates one motivation
              | behind the minimum-residual primal and sensitivity formulations: the minimum-residual primal
              | reduced-order model minimizes the first term in (4.34) over the columnspace of Œ¶ and the minimum-
              | residual sensitivity reduced-order model minimizes the second term over the columnspace of Œ¶‚àÇ .
              |                                                                                               ‚àÇu
              |     To this point, two different approximations of the high-dimensional model sensitivities      (¬µ)
              |                                                                                               ‚àÇ¬µ
              |                           ‚àÇur          ‚àÇu
              |                                        dr
              | have been introduced: Œ¶        and Œ¶‚àÇ     . Each leads to a different approximation of the gradient
              |                            ‚àÇ¬µ          ‚àÇ¬µ
              | of the quantity of interest: ‚àáFr (¬µ) and ‚àáF
              |                                           dr (¬µ). Proposition 4.3 states sufficient conditions under
              | which these two approximations are equal. Specifically, it requires the test basis for the primal ROM
              | (Œ®), the sensitivity optimality metric (Œò‚àÇ ), and sensitivity basis (Œ¶‚àÇ ) be related according to (4.35).
              |                                                                      ‚àÇur      ‚àÇu
              |                                                                                dr
              | Furthermore, these conditions also imply the reduced coordinates          and      themselves are equal.
              |                                                                      ‚àÇ¬µ        ‚àÇ¬µ
              | Proposition 4.3 is significant since it provides conditions under which the easily computed minimum-
              | residual sensitivities (since they do not require second derivatives of r) reduce to the desired reduced-
              | order model sensitivities (since they guarantee consistency of the gradients of reduced quantities of
              | interest).
blank         | 
text          | Proposition 4.3. Consider a primal reduced-order model defined by trial and test bases Œ¶ and Œ®,
              | respectively, and a minimum-residual sensitivity reduced-order model defined by basis Œ¶‚àÇ and metric
              | Œò‚àÇ . Suppose that either: (1) the primal solution of the reduced-order model exactly reconstructs the
              |   2 The   notation ‚àáF                              dr (¬µ; Œ¶, Œ®, Œ¶‚àÇ , Œò‚àÇ ) when there is no risk of confusion.
              |                    dr (¬µ) will be used in place of ‚àáF
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                        86
blank         | 
              | 
              | 
text          | HDM solution, i.e.,
              |                                            u(¬µ) = Œ¶ur (¬µ; Œ¶, Œ®)
blank         | 
text          | or (2) the test basis Œ® is constant. Then, for any u ‚àà RNu , the relationships
blank         | 
text          |                                               Œ¶‚àÇ = Œ¶
              |                                                         ‚àÇr                                     (4.35)
              |                                          Œ®(u, ¬µ) = Œò‚àÇ      (u, ¬µ)Œ¶‚àÇ
              |                                                         ‚àÇu
blank         | 
text          | guarantee the sensitivity of the primal reduced-order model (Œ¶, Œ®) coincides with the solution of the
              | minimum-residual sensitivity reduced-order model (Œ¶‚àÇ , Œò‚àÇ ) and the corresponding gradient approx-
              | imations match
              |                          ‚àÇur             ‚àÇu
              |                                          dr
              |                              (¬µ; Œ¶, Œ®) =    (¬µ; Œ¶‚àÇ , Œò‚àÇ , Œ¶ur (¬µ; Œ¶, Œ®))
              |                          ‚àÇ¬µ              ‚àÇ¬µ                                                    (4.36)
              |                                          dr (¬µ; Œ¶, Œ®, Œ¶‚àÇ , Œò‚àÇ ).
              |                          ‚àáFr (¬µ; Œ¶, Œ®) = ‚àáF
blank         | 
text          | Proof. Let Œ¶ur = Œ¶ur (¬µ; Œ¶, Œ®) denote the reconstructed primal solution of the projection-based
              | reduced-order model. If either the primal solution is exact or the test basis is constant, the general
              | form of the reduced-order model sensitivity equations in (4.19) reduce to the equations in (4.20),
              | where all terms are evaluated at the primal solution, i.e.,
blank         |                                           
text          |                               ‚àÇr             ‚àÇur                ‚àÇr
              |                   Œ®(Œ¶ur , ¬µ)T    (Œ¶ur , ¬µ)Œ¶      = ‚àíŒ®(Œ¶ur , ¬µ)T    (Œ¶ur , ¬µ).                  (4.37)
              |                               ‚àÇu             ‚àÇ¬µ                 ‚àÇ¬µ
blank         | 
text          | Conversely, the normal form of the minimum-residual sensitivity reduced-order model in (4.28)
              | reduces to                                 d
              |                               ‚àÇr             ‚àÇur                ‚àÇr
              |                   Œ®(Œ¶ur , ¬µ)T    (Œ¶ur , ¬µ)Œ¶      = ‚àíŒ®(Œ¶ur , ¬µ)T    (Œ¶ur , ¬µ),                  (4.38)
              |                               ‚àÇu             ‚àÇ¬µ                 ‚àÇ¬µ
              | when the relationships in (4.35) are enforced. Thus, under conditions (4.35), the governing equations
              |     ‚àÇur       ‚àÇu
              |                dr
              | for      and       are identical and the (unique) solutions must be equal, which establishes the first
              |     ‚àÇ¬µ         ‚àÇ¬µ
              | result in (4.36). The second result in (4.36) follows from the simple relation
blank         |                                                                     
text          |                            ‚àÇ                        ‚àÇur
              |        ‚àáFr (¬µ; Œ¶, Œ®) = g           Œ¶ur (¬µ; Œ¶, Œ®), Œ¶     (¬µ; Œ¶, Œ®), ¬µ
              |                                                     ‚àÇ¬µ
              |                                                                                          !
              |                                                      ‚àÇu
              |                                                      dr
              |                                                                                                (4.39)
              |                        = g‚àÇ        Œ¶ur (¬µ; Œ¶, Œ®), Œ¶‚àÇ     (¬µ; Œ¶‚àÇ , Œò‚àÇ , Œ¶ur (¬µ; Œ¶, Œ®)), ¬µ
              |                                                      ‚àÇ¬µ
              |                          dr (¬µ; Œ¶, Œ®, Œ¶‚àÇ , Œò‚àÇ ),
              |                        = ‚àáF
blank         | 
text          | where the first and last equality use the definition of g ‚àÇ and the second equality uses the identity
              | between the true and minimum-residual reduced sensitivities established in the first part.
blank         | 
text          |    To close this section, the specific form of the minimum-residual sensitivity equations in (4.19)
              | are discussed for the special cases of Galerkin and LSPG projections (4.6). For problems with
              | symmetric positive-definite Jacobians, the Galerkin sensitivity equations in (4.21) exactly match the
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                           87
blank         | 
              | 
              | 
text          | minimum-residual sensitivity equations with Œ® = Œ¶ = Œ¶‚àÇ and sensitivity metric
blank         | 
text          |                                            ‚àÇr
              |                                     Œò‚àÇ =      (Œ¶ur (¬µ; Œ¶, Œ¶))‚àíT .                                 (4.40)
              |                                            ‚àÇu
blank         | 
text          | Additionally, these choices satisfy (4.35), which further supports the claim. Thus, for such problems,
              | the true Galerkin sensitivities possess the minimum residual property (and therefore optimality,
              | monotonicity, and interpolation as defined in Proposition 4.2) and are easy to compute since they
              |                                                                                      ‚àÇu
              | do not rely on second derivatives of r. For the case of a LSPG projection (Œ®(u, ¬µ) =    (u, ¬µ)Œ¶),
              |                                                                                      ‚àÇ¬µ
              | the choices Œ¶‚àÇ = Œ¶ and Œò‚àÇ = I reduce the minimum-residual sensitivity equations to
blank         | 
text          |                                         ‚àÇr T ‚àÇr ‚àÇu
              |                                                  dr      ‚àÇr T ‚àÇr
              |                                    Œ¶T          Œ¶    = Œ¶T                                          (4.41)
              |                                         ‚àÇu ‚àÇu ‚àÇ¬µ         ‚àÇu ‚àÇ¬µ
blank         | 
text          | where all nonlinear terms are evaluated at the reconstructed primal solution Œ¶ur (¬µ; Œ¶, Œ®). The
              | above equation is identical to the LSPG sensitivity equations when the primal solution is exact and
              | thus the true and minimum-residual sensitivities agree. This is reaffirmed since the choices satisfy
              | (4.35).
blank         | 
              | 
title         | 4.1.3     Exact and Minimum-Residual Adjoint Formulation
text          | For optimization problems that involve more optimization variables than constraints, it is desirable
              | to employ the adjoint method to compute gradients of quantities of interest. The derivation of
              | the adjoint equations for the reduced-order model can apply any of the three procedures outline in
              | Section 2.3.4 to the governing equation in (4.3), i.e., rr (ur , ¬µ) = 0, and reduced quantity of interest
              | in (4.4), i.e., fr (Œ¶ur , ¬µ). For brevity, only the optimization approach is detailed. Consider the
              | auxiliary optimization problem
blank         | 
text          |                                     minimize     f (Œ¶ur , ¬µÃÇ)
              |                                      ur ‚ààRku
              |                                                                                                   (4.42)
              |                                     subject to Œ®T r(Œ¶ur , ¬µÃÇ) = 0
blank         | 
text          | for a fixed ¬µÃÇ and the corresponding Lagrangian
blank         | 
text          |                             Lr (ur , Œªr ) = f (Œ¶ur , ¬µÃÇ) ‚àí ŒªTr Œ®T r(Œ¶ur , ¬µÃÇ).                    (4.43)
blank         | 
text          | By comparing this expression for the Lagrangian with that in (2.98), it is clear that the HDM
              | Lagrange multipliers are reconstructed from the reduced Lagrange multipliers as
blank         | 
text          |                                                Œª = Œ®Œªr .                                          (4.44)
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                             88
blank         | 
              | 
              | 
text          | The stationarity of the Lagrangian with respect to ur leads to the reduced adjoint equations
blank         | 
text          |                           Ô£Æ                           Ô£πT
              |                            XNu     ‚àÇ Œ®T ej          ‚àÇr Ô£ª          ‚àÇf T
              |                           Ô£∞     rj           Œ¶ + Œ®T    Œ¶ Œª r = Œ¶T                                (4.45)
              |                             j=1
              |                                       ‚àÇu            ‚àÇu            ‚àÇu
blank         | 
              | 
text          | where all terms are evaluated at the reconstructed primal solution, Œ¶ur (¬µ; Œ¶, Œ®).
              |    For any ¬µ ‚àà RN¬µ and bases Œ¶, Œ®, the solution of the above equation is denoted Œªr (¬µ; Œ¶, Œ®).
              | The gradient of the quantity of interest is then reconstructed as
              |                                                  Ô£Æ                        Ô£π
              |            ‚àÇf                                     XNu     ‚àÇ Œ®T ej        ‚àÇr
              |  ‚àáFr (¬µ) =    (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) ‚àí Œªr (¬µ; Œ¶, Œ®)T Ô£∞     rj           + Œ®T    Ô£ª                           .
              |            ‚àÇ¬µ                                      j=1
              |                                                              ‚àÇ¬µ          ‚àÇ¬µ
              |                                                                                    (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)
              |                                                                                                  (4.46)
              | In the special case where the primal reduced-order model is exact, i.e., r(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) = 0, or
              | the test basis is constant, the expression in (4.45) reduces to
              |                                               T
              |                                                         ‚àÇf T
blank         |                                      
text          |                                         T ‚àÇr
              |                                       Œ®      Œ¶ Œª r = Œ¶T      ,                                   (4.47)
              |                                           ‚àÇu            ‚àÇu
blank         | 
text          | and the gradient of the QoI becomes
blank         | 
text          |                      ‚àÇf                                       ‚àÇr
              |          ‚àáFr (¬µ) =      (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) ‚àí Œªr (¬µ; Œ¶, Œ®)T Œ®T    (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ).             (4.48)
              |                      ‚àÇ¬µ                                       ‚àÇ¬µ
blank         | 
text          | In the special case where the primal reduced-order model employs a Galerkin projection (Œ® = Œ¶),
              | the test basis is state- and parameter-independent and the adjoint equations in (4.45) or (4.47)
              | become                                       T
              |                                                       ‚àÇf T
blank         |                                      
text          |                                          ‚àÇr
              |                                       Œ¶T    Œ¶ Œªr = Œ¶T                                            (4.49)
              |                                          ‚àÇu           ‚àÇu
              | and the gradient of the QoI is
blank         | 
text          |                      ‚àÇf                                       ‚àÇr
              |          ‚àáFr (¬µ) =      (Œ¶ur (¬µ; Œ¶, Œ¶), ¬µ) ‚àí Œªr (¬µ; Œ¶, Œ¶)T Œ¶T    (Œ¶ur (¬µ; Œ¶, Œ¶), ¬µ).             (4.50)
              |                      ‚àÇ¬µ                                       ‚àÇ¬µ
blank         | 
text          | In the special case of a LSPG projection, the adjoint equations in (4.45) become
              |                          Ô£Æ                              Ô£πT
              |                            Nu                      T
              |                           X           2
              |                                      ‚àÇ rj        ‚àÇr ‚àÇr Ô£ª          ‚àÇf T
              |                          Ô£∞     rj Œ¶T      Œ¶ + Œ¶T       Œ¶ Œª r = Œ¶T                                (4.51)
              |                            j=1
              |                                      ‚àÇu‚àÇu        ‚àÇu ‚àÇu            ‚àÇu
blank         | 
text          | and the QoI gradient is
              |                                               Ô£Æ                             Ô£π
              |                                                 Nu         2           T
              |           ‚àÇf                                   X          ‚àÇ rj      ‚àÇr ‚àÇr Ô£ª
              | ‚àáFr (¬µ) =    (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)‚àíŒªr (¬µ; Œ¶, Œ®)T Ô£∞     rj Œ¶T      + Œ¶T                                         .
              |           ‚àÇ¬µ                                    j=1
              |                                                           ‚àÇu‚àÇ¬µ      ‚àÇu   ‚àÇ¬µ
              |                                                                                      (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)
              |                                                                                                  (4.52)
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                     89
blank         | 
              | 
              | 
text          |    For cases where the test basis is non-constant, the adjoint equations and QoI gradient are diffi-
              | cult to compute due to the presence of derivatives of the test basis, which usually involves second
              | derivatives of the discrete PDE. These terms are rarely available in large-scale PDE implementations
              | and expensive to compute when available. Furthermore, the adjoint equations for the reduced-order
              | model are not developed such that the HDM adjoint variable will be optimally reconstructed and,
              | therefore, the gradient of the reduced QoI may not be a good approximation of the gradient of
              | the true QoI. For these reasons, minimum-residual adjoint equations will be formulated such that
              | the reconstructed reduced adjoint variable minimizes the HDM adjoint residual in some norm. For
              | generality, approximate the HDM adjoint variable in a reduced-order basis Œ¶Œª ‚àà RNu √óku , i.e.,
blank         | 
text          |                                                        Œª = Œ¶Œª ŒªÃÇr .                                          (4.53)
blank         | 
text          | In general, the adjoint basis may depend on the primal solution and parameter, i.e., Œ¶Œª (u, ¬µ). The
              | reduced coordinates ŒªÃÇr ‚àà Rku are defined as the solution of the linear residual minimization problem
              | (linear least-squares)
              |                                                   1 Œª                         2
              |                                     minimize        r (Œ¶ur , Œ¶Œª ŒªÃÇr , ¬µ)           .                         (4.54)
              |                                                   2                           ŒòŒª
blank         | 
text          | where u ‚àà RNu is any linearization point, usually the primal ROM solution. Expanding (4.54) with
              | the definition of r Œª in (2.101) leads to the following definition of ŒªÃÇr
blank         | 
text          |                                                                                                     2
              |                                                        1   ‚àÇf           ‚àÇr
              |                ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u) = arg min            ‚àí    (u, ¬µ)T +    (u, ¬µ)T Œ¶Œª zr                 ,   (4.55)
              |                                              zr ‚ààRku   2   ‚àÇu           ‚àÇu                          ŒòŒª
blank         | 
              | 
text          | for any u ‚àà RNu . The definition of ŒªÃÇr in (4.55) is equivalent to the solution of the normal equations
              |                                     !T                     !                      !T
              |                          ‚àÇr T Œª              Œª   ‚àÇr T Œª                ‚àÇr T Œª               ‚àÇf T
              |                              Œ¶           Œò           Œ¶         ŒªÃÇr =       Œ¶           ŒòŒª                    (4.56)
              |                          ‚àÇu                      ‚àÇu                    ‚àÇu                   ‚àÇu
blank         | 
text          | where the dependence on the linearization point (u) and parameter (¬µ) have been dropped.
              |    As with the primal and sensitivity minimum-residual reduced-order models, the minimum-
              | residual adjoint reduced-order models are guaranteed to be monotonic and interpolatory as defined
              | in Proposition 4.4. This result is relevant since it provides conditions under which the minimum-
              | residual adjoint reduced-order model solution monotonically approaches the HDM adjoint solution
              | and the requirement for these solutions to exactly match.
blank         | 
text          | Proposition 4.4. Let (Œ¶Œª , ŒòŒª ) define a minimum-residual adjoint reduced-order model. Then the
              | following properties hold for any ¬µ ‚àà RN¬µ
blank         | 
text          |    ‚Ä¢ (Optimality) For any u ‚àà RNu and z ‚àà col(Œ¶Œª )
blank         |                                                          
text          |                          r Œª u, Œ¶Œª ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u), ¬µ                  ‚â§ r Œª (u, z, ¬µ)        ŒòŒª
              |                                                                                                              (4.57)
              |                                                                        ŒòŒª
blank         | 
              | 
text          |                                 0        0
              |    ‚Ä¢ (Monotonicity) Let (Œ¶Œª , ŒòŒª ) define a minimum-residual adjoint reduced-order model such
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                               90
blank         | 
              | 
              | 
text          |                    0
              |         that col(Œ¶Œª ) ‚äÜ col(Œ¶Œª ), then
blank         | 
text          |                                     r Œª (u, z, ¬µ)   ŒòŒª
              |                                                          ‚â§ r Œª (u, z 0 , ¬µ)       ŒòŒª
              |                                                                                        ,               (4.58)
blank         | 
text          |                                                           0          0        0
              |         where z = Œ¶Œª ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u) and z 0 = Œ¶Œª ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u), for any u ‚àà RN¬µ .
blank         | 
text          |       ‚Ä¢ (Interpolatory) If Œª(¬µ) ‚àà col(Œ¶Œª ), then
blank         |                                                                        
text          |                                  r Œª u(¬µ), Œ¶Œª ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u(¬µ)), ¬µ = 0
              |                                                                                                        (4.59)
              |                                               Œª(¬µ) = Œ¶Œª ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u(¬µ)).
blank         | 
              | 
text          | Proof. Optimality follows trivially from the fact that Œ¶Œª ŒªÃÇ(¬µ; Œ¶Œª , ŒòŒª , u) is the (unique) minima
              | of the optimization problem in (4.54). Monotonicity follows directly from the optimality property
              |                        0
              | since z 0 ‚àà col(Œ¶Œª ) ‚äÜ col(Œ¶Œª ). Finally, if the exact solution of r Œª (u(¬µ), ¬∑ , ¬µ) = 0 is contained in
              | the columnspace of Œ¶Œª , i.e., Œª(¬µ) ‚àà col(Œ¶Œª ), the optimality property implies
blank         |                                                  
text          |            r Œª u(¬µ), Œ¶Œª ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u(¬µ)), ¬µ            ‚â§ r Œª (u(¬µ), Œª(¬µ), ¬µ)        ŒòŒª
              |                                                                                                 = 0.   (4.60)
              |                                                          ŒòŒª
blank         | 
              | 
text          | The interpolation property in (4.59) follows from this and the fact that for a given u ‚àà RNu and
              | ¬µ ‚àà RN¬µ , the solution of r Œª (u, ¬∑ , ¬µ) = 0 is unique (due to invertibility of the Jacobian and linear
              | independence of the columns of Œ¶Œª ).
blank         | 
text          |       Since ŒªÃÇr is chosen to optimally reconstruct the HDM adjoint variable Œª in the sense of the
              |   Œª
              | Œò -norm of the adjoint residual, the gradient of the QoI will be computed as
blank         |                                                                           
text          |                 dr (¬µ; Œ¶, Œ®, Œ¶Œª , ŒòŒª ) := g Œª u, Œ¶Œª ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u), ¬µ
              |        ‚àáF (¬µ) ‚âà ‚àáF
              |                                               ‚àÇf                                   T ‚àÇr
              |                                                                                                        (4.61)
              |                                           =      (u, ¬µ) ‚àí ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u)T Œ¶Œª      (u, ¬µ).
              |                                               ‚àÇ¬µ                                     ‚àÇ¬µ
blank         | 
text          | where u = Œ¶ur (¬µ; Œ¶, Œ®). From Appendix B, the approximation of the gradient of the reduced
              | quantity of interest is equipped with a residual-based error bound (Lemma B.4) that takes the form
blank         | 
              | 
              |                                                                                   
text          |            dr (¬µ; Œ¶, Œ®, Œ¶Œª , ŒòŒª ) ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r Œª u, Œ¶Œª ŒªÃÇr ¬µ; Œ¶Œª , ŒòŒª , u , ¬µ
              |   ‚àáF (¬µ) ‚àí ‚àáF
blank         |                                                                                         
text          |                                   ‚â§ Œ∫0 kr(u, ¬µ)kŒò + œÑ 0 r Œª u, Œ¶Œª ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , u), ¬µ
              |                                                                                                          ŒòŒª
              |                                                                                                        (4.62)
              | for some constants Œ∫, Œ∫0 , œÑ, œÑ 0 > 0, where u = Œ¶ur (¬µ; Œ¶, Œ®) and the second inequality follows
              | from norm equivalence in finite dimensions. The residual-based error bound illuminates one moti-
              | vation behind the minimum-residual primal and adjoint formulations: the minimum-residual primal
              | reduced-order model minimizes the first term in (4.62) over the columnspace of Œ¶ and the minimum-
              | residual adjoint reduced-order model minimizes the second term over the columnspace of Œ¶Œª .
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                          91
blank         | 
              | 
              | 
text          |      In an exact parallel with the previous section, Proposition 4.5 provides conditions under which the
              | two aforementioned approximations of the HDM gradient ‚àáF (¬µ), i.e., the approximation based on
              | the reduced-order model adjoint ‚àáFr (¬µ) and that based on the minimum-residual adjoint reduced-
              | order model ‚àáF
              |              dr (¬µ), exactly match. The main condition is the requirement (4.63) on the relation-
              | ship between the trial basis (Œ¶), adjoint basis (Œ¶Œª ), and adjoint optimality metric (ŒòŒª ). These
              | conditions also ensure the reduced coordinates Œªr and ŒªÃÇr match. These results are relevant as
              | they provide conditions under which the easily computed minimum-residual adjoint solutions (since
              | the computation does not require second-order derivatives of r) reduce to the desired reduced-order
              | model sensitivities (that guarantee consistency of the gradients of the reduced quantities of interest).
blank         | 
text          | Proposition 4.5. Consider a primal reduced-order model defined by trial and test bases Œ¶ and Œ®,
              | respectively, and a minimum-residual adjoint reduced-order model defined by basis Œ¶Œª and metric
              | ŒòŒª . Suppose that either: (1) the primal solution of the reduced-order model exactly reconstructs the
              | HDM solution, i.e.,
              |                                         u(¬µ) = Œ¶ur (¬µ; Œ¶, Œ®).
blank         | 
text          | or (2) the test basis Œ® is constant. Then, for any u ‚àà RNu , the relationships
              |                                                                 ‚àí1
              |                                                       ‚àÇr
              |                              Œ¶Œª (u, ¬µ) = Œ®(u, ¬µ) = ŒòŒª    (u, ¬µ)T     Œ¶                           (4.63)
              |                                                       ‚àÇu
blank         | 
text          | guarantee the adjoint solution of the primal reduced-order model (Œ¶, Œ®) matches the minimum-
              | residual adjoint reduced-order model (Œ¶Œª , ŒòŒª )
blank         | 
text          |                              Œªr (¬µ; Œ¶, Œ®) = ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , Œ¶ur (¬µ; Œ¶, Œ®))
              |                                                                                                  (4.64)
              |                                            dr (¬µ; Œ¶, Œ®, Œ¶Œª , ŒòŒª )
              |                            ‚àáFr (¬µ; Œ¶, Œ®) = ‚àáF
blank         | 
text          | Proof. Let Œ¶ur = Œ¶ur (¬µ; Œ¶, Œ®) denote the reconstructed primal solution of the projection-based
              | reduced-order model. If either the primal solution is exact or the test basis is constant, the general
              | form of the reduced-order model adjoint equations in (4.45) reduce to the equations in (4.47), where
              | all terms are evaluated at the primal solution, i.e.,
              |                                                  T
              |                                      ‚àÇr                    ‚àÇf
              |                          Œ®(Œ¶ur , ¬µ)T    (Œ¶ur , ¬µ)Œ¶ Œªr = Œ¶T    (Œ¶ur , ¬µ)T .                       (4.65)
              |                                      ‚àÇu                    ‚àÇu
blank         | 
text          | Conversely, the normal form of the minimum-residual adjoint reduced-order model in (4.56) reduces
              | to                                               T
              |                                      ‚àÇr                     ‚àÇf
              |                          Œ®(Œ¶ur , ¬µ)T    (Œ¶ur , ¬µ)Œ¶ ŒªÃÇr = Œ¶T    (Œ¶ur , ¬µ)T                        (4.66)
              |                                      ‚àÇu                     ‚àÇu
              | when the relationships in (4.63) are enforced. Thus, under the aforementioned conditions, the
              | governing equations for Œªr and ŒªÃÇr are identical and the (unique) solutions must be equal, which
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                            92
blank         | 
              | 
              | 
text          | establishes the first result in (4.64). The second result in (4.64) follows from the simple relation
blank         | 
text          |         ‚àáFr (¬µ; Œ¶, Œ®) = g Œª (Œ¶ur (¬µ; Œ¶, Œ®), Œ®Œªr (¬µ; Œ¶, Œ®), ¬µ)
blank         |                                                                                 
text          |                       = g Œª Œ¶ur (¬µ; Œ¶, Œ®), Œ¶Œª ŒªÃÇr (¬µ; Œ¶Œª , ŒòŒª , Œ¶ur (¬µ; Œ¶, Œ®)), ¬µ                (4.67)
              |                           dr (¬µ; Œ¶, Œ®, Œ¶Œª , ŒòŒª ),
              |                         = ‚àáF
blank         | 
text          | where the first and last equality use the definition of g Œª and the second equality uses the identity
              | between the true and minimum-residual reduced adjoints established in the first part.
blank         | 
text          |    The section closes with a discussion of the minimum-residual adjoint equations for the special
              | cases of Galerkin and LSPG projections (4.6). For problems with symmetric positive-definite Jaco-
              | bians, the Galerkin adjoint equations in (4.49) exactly match the minimum-residual adjoint equations
              | in (4.55) with Œ® = Œ¶Œª = Œ¶ and adjoint metric
blank         | 
text          |                                             ‚àÇr
              |                                     ŒòŒª =       (Œ¶ur (¬µ; Œ¶, Œ¶))‚àíT .                               (4.68)
              |                                             ‚àÇu
blank         | 
text          | Additionally, these choices satisfy (4.63), which further supports the claim. Thus, for such problems,
              | the true Galerkin adjoints possess the minimum residual property and are easy to compute since they
              |                                                                                       ‚àÇr
              | do not rely on second derivatives of r. For the case of a LSPG projection (Œ®(u, ¬µ) =     (u, ¬µ)Œ¶),
              |                                                                                      ‚àÇu
              |               Œª
              | the choices Œ¶ = Œ® and                    "         #‚àí1
              |                                      Œª     ‚àÇr T ‚àÇr
              |                                    Œò =                                                       (4.69)
              |                                            ‚àÇu ‚àÇu
              |                                                      (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)
blank         | 
text          | lead to the minimum-residual adjoint equations
blank         | 
text          |                                            ‚àÇr T ‚àÇr         ‚àÇf
              |                                       Œ¶T           Œ¶u = Œ¶T                                       (4.70)
              |                                            ‚àÇu ‚àÇu           ‚àÇu
blank         | 
text          | where all nonlinear terms are evaluated at Œ¶ur (¬µ; Œ¶, Œ®). The above equation is identical to the
              | LSPG adjoint equations when the primal solution is exact and thus the true and minimum-residual
              | adjoints agree. This is reaffirmed since the above choices satisfy (4.63).
blank         | 
              | 
title         | 4.2     Global Hyperreduced Models
text          | Despite the small size of the nonlinear system defining the reduced-order model
blank         | 
text          |                                            Œ®T r(Œ¶ur , ¬µ) = 0,
blank         | 
text          | in terms of the number of equations and unknowns (ku ), it may still be expensive to solve. The
              | major expense emanates from the large-scale operations required to evaluate the reduced residual
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                               93
blank         | 
              | 
              | 
text          | and Jacobian (neglecting higher derivatives if Œ® is not constant)
blank         | 
text          |                                                                       ‚àÇr
              |                                   Œ®T r(Œ¶ur , ¬µ)               Œ®T         (Œ¶ur , ¬µ)Œ¶,                                  (4.71)
              |                                                                       ‚àÇu
blank         | 
text          | i.e., the reconstruction of the full state from the reduced coordinates u = Œ¶ur and projection of the
              | full residual and Jacobian into the reduced space. Such bottlenecks do not arise in the case where r is
              | polynomial in the state and parameter as terms can be precomputed offline such that no large-scale
              | operations are required online; see Section 4.2.1 for additional details. To overcome such bottlenecks
              | when r is nonlinear in state or parameter, a slew of hyperreduction 3 techniques have been proposed
              | that introduce an additional layer of approximation on top of that in (4.3). Among the most popular
              | hyperreduction techniques are (1) polynomialization methods, such as Trajectory Piece-Wise Linear
              | (TPWL) approximation [165], where the governing nonlinear equations are replaced by a weighted
              | sum of the equations linearized about preselected points in parameter space and (2) gappy methods
              | [56, 17, 115, 41, 31, 59] where only a subset of the large-scale equations and degrees of freedom are
              | used in the computation of (4.71)‚Äîin the context of PDEs this amounts to only using a subset of
              | the mesh to assemble the reduced residual and Jacobian. This document will only consider gappy
              | methods since they maintain a strong connection to the underlying physics model and enable the
              | reduced residual and Jacobian to be evaluated without incurring operations that scale with the size
              | of the full mesh.
blank         | 
              | 
title         | 4.2.1       Precomputation for Polynomial Nonlinearities
text          | In the special case where the nonlinearity in the state and parameter is polynomial, the contraction
              | of the each monomial term with the reduced basis can be precomputed. As a result, each query
              | to the reduced residual and Jacobian will not involve operations that scale with Nu . Consider the
              | Taylor expansion of the governing equation of degree m in the state and degree n in the parameter
              |                m Xn
              |               X      1               ‚àÇ j+k r
              | r(u, ¬µ) =                                                             (u ‚àí uÃÇ)p1 ¬∑ ¬∑ ¬∑ (u ‚àí uÃÇ)pj (¬µ ‚àí ¬µÃÇ)q1 ¬∑ ¬∑ ¬∑ (¬µ ‚àí ¬µÃÇ)qk
              |               j=0
              |                     j!k! ‚àÇup1 ¬∑ ¬∑ ¬∑ ‚àÇupj ‚àÇ¬µq1 ¬∑ ¬∑ ¬∑ ‚àÇ¬µqk   (uÃÇ, ¬µÃÇ)
              |                   k=0
              |                                                                                                                       (4.72)
              |                Nu                N¬µ
              | where uÃÇ ‚àà R        and ¬µÃÇ ‚àà R        are the expansion points. If the governing equation is at most degree
              | m in the state and n in the parameter, this expansion is exact for any expansion points. Otherwise,
              | it is an approximation and its quality will be heavily dependent on uÃÇ and ¬µÃÇ. The remainder of this
              | section will primarily be concerned with the case where the governing equation is polynomial and
              | the expansion points will be taken as uÃÇ = 0Nu and ¬µÃÇ = 0N¬µ for simplicity. Define the following
              | j + k + 1-order tensor for j = 1, . . . , m and k = 1, . . . , n as the monomials arising in the above
              |   3a   term coined in [175]
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                               94
blank         | 
              | 
              | 
text          | expansion
blank         | 
text          |                                                                      ‚àÇ j+k r
              |                                        D jk (u, ¬µ) =                                              .                    (4.73)
              |                                                                  ¬∑ ¬∑ ‚àÇu} ‚àÇ¬µ ¬∑ ¬∑ ¬∑ ‚àÇ¬µ
              |                                                              | ¬∑{z
              |                                                              ‚àÇu
              |                                                                          | {z }
              |                                                                 j terms      k terms     (u, ¬µ)
blank         | 
text          | When arguments are omitted in the above definition, they are assumed to be zero, i.e., D jk =
              | D jk (0Nu , 0N¬µ ). With this definition, the expansion of the governing equation becomes
blank         | 
text          |                                              m Xn
              |                                             X      1    jk
              |                         r(u, ¬µ)i =                     Dip                 u ¬∑ ¬∑ ¬∑ upj ¬µq1 ¬∑ ¬∑ ¬∑ ¬µqk .
              |                                                            1 ¬∑¬∑¬∑pj q1 ¬∑¬∑¬∑qk p1
              |                                                                                                                        (4.74)
              |                                             j=0
              |                                                   j!k!
              |                                                    k=0
blank         | 
              | 
text          | Introduce the model reduction ansatz for both the state and parameter :
blank         | 
text          |                                                         u = Œ¶y            ¬µ = Œ•Œ∑                                       (4.75)
blank         | 
text          | where Œ¶ ‚àà RNu √óku and Œ• ‚àà RN¬µ √ók¬µ are the reduced bases for the state and parameter spaces,
              | y ‚àà Rku and Œ∑ ‚àà Rk¬µ are the corresponding reduced coordinates, and ku  Nu and k¬µ  N¬µ .
              | Substitution of these ansatz into the polynomial expansion of the governing equations in (4.72) and
              | subsequent projection onto the columnspace of Œ¶ (a Galerkin projection) leads to
blank         | 
text          |                                      m Xn
              |                                     X      1  jk 
              |                      [rr (y, Œ∑)]t =            Dr tr ¬∑¬∑¬∑r s ¬∑¬∑¬∑s yr1 ¬∑ ¬∑ ¬∑ yrj Œ∑s1 ¬∑ ¬∑ ¬∑ Œ∑sk                           (4.76)
              |                                     j=0
              |                                           j!k!      1    j 1    k
              |                                                   k=0
blank         | 
              | 
text          | where the reduced monomial terms that have been contracted with the reduced bases are
blank         | 
text          |                                                     jk
              |                      Drjk
blank         |                            
text          |                             tr1 ¬∑¬∑¬∑rj s1 ¬∑¬∑¬∑sk
              |                                                  = Dip1 ¬∑¬∑¬∑pj q1 ¬∑¬∑¬∑qk
              |                                                                        Œ¶it Œ¶p1 r1 ¬∑ ¬∑ ¬∑ Œ¶pj rj Œ•q1 s1 ¬∑ ¬∑ ¬∑ Œ•qk sk .   (4.77)
blank         | 
text          |                                                                                                     ‚àÇrr
              | From (4.76), the evaluation of the reduced residual rr (y, Œ∑) and Jacobian                              (y, Œ∑) are completely
              |                                                                                                     ‚àÇy
              | independent of the potentially large dimensions Nu and N¬µ                                 but scale poorly with the reduced
              |                                                                                        m+1 n
              | dimensions ku and k¬µ . For example, the evaluation of the reduced residual requires O(ku  k¬µ )
              | operations, which makes this feasible for only small polynomial orders m and n and reduced basis
              | sizes ku and k¬µ . Two common examples of nonlinear partial differential equations that possess poly-
              | nomial nonlinearities in the state and parameter are: the incompressible Navier-Stokes equations
              | (quadratic in the state) and the geometrically nonlinear structure with a St. Venant-Kirchhoff ma-
              | terial law (cubic in the state and linear in the material parameters). While these types of problems
              | arise in a number of important applications, the problems considered in this work will not possess
              | polynomial nonlinearities. Therefore, these methods will not be considered further and attention is
              | turned to the more general gappy methods.
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                           95
blank         | 
              | 
              | 
title         | 4.2.2    Mask and Sample Mesh
text          | The discussion of gappy methods in the context of partial differential equations begins with the
              | critical notion of a mask and sample mesh. Gappy methods are characterized by the distinguishing
              | feature that they only consider a subset of the governing equations, that is, only entries ri are
              | needed, where i ‚àà M ‚äÇ {1, . . . , Nu }. This subset M is called the mask. Let P ‚àà RNu √ó|M| be the
              | subset of the columns of the identity matrix that includes ei only if i ‚àà M. Then, the mask of the
              | governing residual r can be compactly represented as P T r. When r represents a discretized PDE,
              | the evaluation of ri will require the solution uj for all j ‚àà Si ‚äÇ {1, . . . , Nu }, where the Si depends
              | on the discretization scheme and the PDE under consideration. Define the sample mesh as the set
              |                                                     [
              |                                               S=         Si
              |                                                    i‚ààM
blank         | 
              | 
text          | and let PÃÑ ‚àà RNu √ó|S| be the subset of the columns of the identity matrix that include ei only if i ‚àà S.
              | Then, the restriction of the state vector u to the sample mesh is compactly written as PÃÑ T u. All of
              | the gappy-based hyperreduction methods considered in this document rely on the computation of
              | the masked reduced residual and Jacobian
blank         | 
text          |                                                          ‚àÇr
              |                      P T r(PÃÑ PÃÑ T Œ¶ur , ¬µ)         PT      (PÃÑ PÃÑ T Œ¶ur , ¬µ)PÃÑ PÃÑ T Œ¶            (4.78)
              |                                                          ‚àÇu
blank         | 
text          | as they are much less expensive to compute than the terms in (4.71) if |M|  Nu . While the
              | notation in (4.78) will prove convenient in later sections, it does not necessarily reveal the efficiency
              |                                                                                     ‚àÇr
              | of gappy methods. An efficient implementation will compute P T r and P T               PÃÑ directly from
              |                                                                                     ‚àÇu
              | PÃÑ Œ¶ur without reconstructing a Nu -vector padded with zeros as the notation PÃÑ PÃÑ T Œ¶ur suggests.
              |   T
blank         | 
text          | Furthermore, the restriction of the reduced-order basis to the mask, P T Œ¶, can be precomputed.
              | These implementation details enable the terms in (4.78) to be computed efficiently online, i.e.,
              | without incurring operations that scale with the full mesh O(Nu ).
              |    For brevity in the developments to follow, the following notation is introduced for the recon-
              | structed state vector restricted to the sample mesh and reduced residual restricted to the mask
blank         | 
              | 
text          |                          uh = PÃÑ PÃÑ T Œ¶ur          rh (uh , ¬µ) = P T r(uh , ¬µ).                   (4.79)
blank         | 
text          | Then, the masked Jacobians with respect to the state and parameter are
blank         | 
text          |          ‚àÇrh                ‚àÇr                             ‚àÇrh                ‚àÇr
              |              (uh , ¬µ) = P T    (uh , ¬µ)PÃÑ PÃÑ T Œ¶               (uh , ¬µ) = P T    (uh , ¬µ).        (4.80)
              |          ‚àÇuh                ‚àÇu                             ‚àÇ¬µ                 ‚àÇ¬µ
blank         | 
text          | Armed with this notation, the governing equations for gappy-based hyperreduction methods take
              | the general form
              |                                        A(uh , ¬µ)T rh (uh , ¬µ) = 0,                                (4.81)
blank         | 
text          | where it is assumed that A can be computed efficiently online or precomputed offline. Regardless
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                  96
blank         | 
              | 
              | 
text          | of the hyperreduced approach considered, the quantity of interest is defined according to (4.5), i.e.,
blank         | 
text          |                                            Fr (¬µ) = f (Œ¶ur (¬µ), ¬µ)
blank         | 
text          | where ur (¬µ) is the solution of the hyperreduced model. In many cases, the entire reconstructed state
              | Œ¶ur is not required to evaluate f , which commonly arises when f corresponds to a surface integral,
              | i.e., only entries corresponding to nodes on the surface are required. In these cases, only a subset of
              | the reconstructed vector PÃÉ T Œ¶ur are required and Fr can be computed efficiently. This implemen-
              | tation optimization will not be considered in the remainder as it complicates the exposition. The
              | industry-scale example of the shape optimization of a full aircraft configuration in Section 5.5.4 will
              | leverage this precise optimization since its optimization functionals involve forces integrated along
              | the surface. The next section provides specific examples of gappy-based hyperreduction method that
              | can be written in the general form (4.81).
blank         | 
              | 
title         | 4.2.3      Examples
text          | The simplest approach to gappy-based hyperreduction is to simply ignore information that is not
              | included in the mask. This approach is usually known as collocation and the general form of the
              | projection-based reduced-order model is
blank         | 
text          |                                           (P T Œ®)T rh (uh , ¬µ) = 0,                                      (4.82)
blank         | 
text          | which fits into the general form in (4.81) with A(u, ¬µ) = P T Œ®(u, ¬µ). While this approach is naive
              | in the sense that it makes no attempt to account for missing information, it is simple and robust,
              | provided a sufficiently large sample mesh is used.
              |    In contrast to the naive approach, a number of methods exist that attempt to account for
              | the missing information using ideas setforth in [56], which include (Discrete) Empirical Interpola-
              | tion Method ((D)EIM) [17, 41] and Gauss-Newton with Approximated Tensors (GNAT) [31]. The
              | (D)EIM approach assumes the residual lies in the low-dimensional subspace defined by the span of
              | a separate basis Œ¶r 4 , i.e.,
              |                                          r(Œ¶ur , ¬µ) = Œ¶r h(ur , ¬µ),                                      (4.83)
blank         | 
text          | where h : Rku √ó RN¬µ ‚Üí Rku are the reduced coordinates of the residual in the basis Œ¶. The reduced
              | coordinates are defined such that the representation in (4.83) exactly matches the true residual on
              | the mask
              |                                  P T Œ¶r h(Œ¶ur , ¬µ) = P T r(PÃÑ PÃÑ T Œ¶ur , ¬µ),                             (4.84)
blank         | 
text          | which leads to the following expression for the residual reduced coordinates h
blank         | 
text          |                                     h(Œ¶ur , ¬µ) = (P T Œ¶r )‚àí1 rh (ur , ¬µ)                                 (4.85)
              |   4 Usually the residual is separated into its linear and nonlinear components and (D)EIM is only applied to the
blank         | 
text          | nonlinear portion.
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                        97
blank         | 
              | 
              | 
text          | where the definition of rh in (4.80) was used. Combining (4.83) and (4.85) into the form of the
              | projection-based reduced-order model in (4.3), the (D)EIM governing equations are
              |                                            T            ‚àí1
              |                                     Œ¶Tr Œ®        P T Œ¶r           rh (uh , ¬µ) = 0.             (4.86)
blank         | 
text          | (D)EIM fits into the general form of gappy-based hyperreduced models in (4.81) with A(u, ¬µ) =
              |        T        ‚àí1
              |   Œ¶Tr Œ®    P T Œ¶r     .
              |    The GNAT method is a minor generalization of (D)EIM that also approximates the residual in
              | a separate low-dimensional subspace spanned by the basis Œ¶r ‚àà RNu √ókr (kr  Nu )
blank         | 
text          |                                        r(Œ¶ur , ¬µ) = Œ¶r h(ur , ¬µ),                              (4.87)
blank         | 
text          | and the reduced coordinates h : Rku √ó RN¬µ ‚Üí Rkr are defined such that the representation in (4.87)
              | matches, in a least-squares sense, the true residual on the mask
blank         | 
text          |                            h(ur , ¬µ) = arg min P T Œ¶r z ‚àí rh (uh , ¬µ)                2
              |                                                                                          .     (4.88)
              |                                              z‚ààRkr
blank         | 
              | 
text          | The GNAT governing equations follow from combining (4.87) and (4.88) into the form of the
              | projection-based reduced-order model in (4.3)
              |                                             T             ‚Ä†
              |                                      Œ¶Tr Œ®          P T Œ¶r        rh (uh , ¬µ) = 0.             (4.89)
blank         | 
text          | The GNAT equations fit into the general form of gappy-based hyperreduction models in (4.81) with
              |          T       ‚Ä†
              | A = Œ¶Tr Œ®     P T Œ¶r .
              |    From the above construction, a number of advantages and disadvantages of each approach emerge.
              | As previously mentioned, the (D)EIM and GNAT methods have the desirable property of attempting
              | to account for information missing from the mask by approximating the nonlinear residual in a low-
              | dimensional subspace, while collocation simply ignores missing information. However, due to the
              | nonlinearity of the residual r(u, ¬µ), it cannot be guaranteed (or even expected) to lie in a low-
              | dimensional subspace, even if the state vector u does. In fact, several works [31, 33] have shown
              | that, even to reproduce training data, the residual approximation in (4.87) requires kr  ku for the
              | resulting hyperreduced model to be sufficiently accurate. In practice, the (D)EIM and GNAT ansatz
              | in (4.83) and (4.87) cause the corresponding methods to exhibit classical over-fitting behavior, i.e.,
              | superb accuracy when reproducing training data and low accuracy at predictive points, particularly
              | when applied to real engineering applications [198]. A notable distinction between these methods is
              | that collocation only requires the trial basis (Œ¶), test basis (Œ®), and mask (P ), while (D)EIM and
              | GNAT require the construction of a separate residual basis (Œ¶r ) that involves substantial additional
              | offline training‚Äîusually requiring the collection and compression of residual snapshots [31]. It has
              | been observed [198] that residual snapshots are not necessarily amenable to compression, which
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                           98
blank         | 
              | 
              | 
text          | results large number of basis vectors (kr  1) and ultimately hurts the performance of the reduced-
              | order model.
              |    Given this discussion, only the collocation hyperreduction method will be considered in the
              | remainder. This is predominantly to avoid the overfitting behavior of the other approaches since
              | the hyperreduced model will be heavily used in predictive settings in subsequent chapters. The
              | remainder of this chapter will discuss a formulation of the primal, sensitivity, and adjoint collocation-
              | based hyperreduced model that results in optimal approximations, in the sense of minimizing the
              | residual in some norm on the mask.
blank         | 
              | 
title         | 4.2.4     Minimum-Residual Primal Formulation
text          | The form of the test basis Œ® in the gappy-accelerated projection-based reduced-order models in
              | (4.82), (4.86), (4.89) has remainder arbitrary to this point.        Given the desirable properties of
              | minimum-residual reduced-order models detailed in Sections 4.1.1‚Äì4.1.3, the test basis is defined
              | in accordance with a parallel concept in the collocation-based hyperreduction setting‚Äîthe masked
              | minimum-residual property. This property (Definition 4.2) requires the solution of the hyperreduced
              | model to minimize the residual over the mask in some norm. For the remainder of this document, the
              | solution of the general form of the collocation-based hyperreduced model in (4.81) will be denoted
              | uh (¬µ; Œ¶, Œ®, P ), i.e.,
              |                                        T
              |                                  PTŒ®        rh (uh (¬µ; Œ¶, Œ®, P ), ¬µ) = 0.
blank         | 
text          | Furthermore, the mask-reconstructed state vector uh is identified with its corresponding reduced
              | coordinates ur from (4.79), i.e., uh (¬µ; Œ¶, Œ®, P ) = PÃÑ PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®, P ). The sample mesh PÃÑ is
              | not included in the argument list since it is uniquely determined from the mask P and the structure
              | of the governing equation r. When there is no risk of confusion regarding the choice of mask, test
              | basis, and trial basis, the arguments will be dropped.
blank         | 
text          | Definition 4.2 (Masked Minimum-Residual Property). A hyperreduced model of the form (4.81)
              | possesses the masked minimum-residual property if the solution satisfies the first-order optimality
              | conditions of the following masked residual minimization problem
blank         | 
text          |                                               1                          2
              |                                  minimize       P T r(PÃÑ PÃÑ T Œ¶ur , ¬µ)   Œò
              |                                                                                                   (4.90)
              |                                   ur ‚ààRku     2
blank         | 
text          | for some symmetric positive-definite Œò ‚àà R|M|√ó|M| .
blank         | 
text          |    Masked minimum-residual hyperreduced models possess a similar monotonicity property as that
              | defined in Proposition 4.1 for minimum-residual projection-based reduced-order models. The inter-
              | polation property only holds if the solution of the minimum-residual hyperreduced model is unique
              | (guaranteed if the mask is full, i.e., M = {1, . . . , Nu } by Assumption 2.2). These properties are
              | stated precisely in Proposition 4.6.
blank         | 
text          | Proposition 4.6. Let (Œ¶, Œ®, Œò, P ) define masked minimum-residual hyperreduced model whose
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                        99
blank         | 
              | 
              | 
text          | solution coincides with the global minimum of (4.90). Then, the following properties hold for any
              | ¬µ ‚àà RN¬µ :
blank         | 
text          |    ‚Ä¢ (Optimality) For any u ‚àà RNu such that PÃÑ T u ‚àà col(PÃÑ T Œ¶),
blank         | 
text          |                           P T r(PÃÑ PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ)      Œò
              |                                                                        ‚â§ P T r(PÃÑ PÃÑ T u, ¬µ)    Œò
              |                                                                                                     .           (4.91)
blank         | 
              | 
text          |    ‚Ä¢ (Monotonicity) Let (Œ¶0 , Œ®0 , P ) define a hyperreduced model such that col(PÃÑ T Œ¶0 ) ‚äÜ col(PÃÑ T Œ¶),
              |       then
blank         | 
text          |              P T r(PÃÑ PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ)   Œò
              |                                                        ‚â§ P T r(PÃÑ PÃÑ T Œ¶0 ur (¬µ; Œ¶0 , Œ®0 , P ), ¬µ)      Œò
              |                                                                                                             .   (4.92)
blank         | 
              | 
text          |    ‚Ä¢ (Interpolatory) If PÃÑ T u(¬µ) ‚àà col(PÃÑ T Œ¶), then
blank         | 
text          |                                      P T r(PÃÑ PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ) = 0.                                   (4.93)
blank         | 
              | 
text          | Proof. Optimality follows from the fact that ur (¬µ; Œ¶, Œ®, P ) is the global minima of the optimization
              | problem in (4.90): for PÃÑ T u ‚àà col(PÃÑ T Œ¶), there exists y ‚àà Rku such that PÃÑ T u = PÃÑ T Œ¶y and since
              | ur is the global minima of (4.90), we have
blank         | 
text          |                       P T r(PÃÑ PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ)     Œò
              |                                                                   ‚â§ P T r(PÃÑ PÃÑ T Œ¶y, ¬µ)    Œò
              |                                                                                                 .               (4.94)
blank         | 
text          | A simple application of the optimality property to PÃÑ T Œ¶0 ur (¬µ; Œ¶0 , Œ®0 ) ‚àà col(PÃÑ T Œ¶0 ) ‚äÜ col(PÃÑ T Œ¶)
              | leads to monotonicity. Finally, if the solution of P T r( ¬∑ , ¬µ) = 0 is contained in the columnspace of
              | PÃÑ T Œ¶, i.e., PÃÑ T u(¬µ) ‚àà col(PÃÑ T Œ¶), the optimality property implies
blank         | 
text          |                     P T r(PÃÑ PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)     Œò
              |                                                             ‚â§ P T r(PÃÑ PÃÑ T u(¬µ), ¬µ)    Œò
              |                                                                                             = 0,                (4.95)
blank         | 
text          | which is precisely the interpolation property in (4.93).
blank         | 
text          |    The first-order optimality condition of (4.90) is
blank         | 
text          |                                       ‚àÇrh
              |                                           (uh , ¬µ)T Œòrh (uh , ¬µ) = 0                                            (4.96)
              |                                       ‚àÇuh
blank         | 
text          | and, therefore, the masked test basis must satisfy
blank         | 
text          |                                                              ‚àÇrh
              |                                       P T Œ®(uh , ¬µ) = Œò          (uh , ¬µ)                                       (4.97)
              |                                                              ‚àÇuh
blank         | 
text          | for the collocation-based projection-based hyperreduced model (4.82) to possess the masked minimum-
              | residual property. The special case of a LSPG projection satisfies (4.97) with
blank         | 
text          |                                                                        ‚àÇrh
              |                               Œò=I              P T Œ®(uh , ¬µ) =             (uh , ¬µ).                            (4.98)
              |                                                                        ‚àÇuh
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                       100
blank         | 
              | 
              | 
text          | where I is the |M| √ó |M| identity matrix and therefore possess the masked minimum-residual
              | property. The special case of a Galerkin projection (P T Œ® = P T Œ¶) is more cumbersome to interpret
              | as a special case of the optimality conditions in (4.97) in the hyperreduced setting. Unlike the pure
              |                                                                    ‚àÇrh
              | projection setting of Section 4.1.1, the reduced Jacobian matrix        is not square, in general, and
              |                                                                    ‚àÇuh
              | cannot define a valid norm. From (4.97), for a Galerkin projection to possess the masked minimum-
              | residual property, Œò must be selected such that the following constrained linear system of equations
              | (linear in Œò) is satisfied
blank         | 
text          |                                    ‚àÇrh
              |                                Œò       = PTŒ¶        subject to      Œò  0.                     (4.99)
              |                                    ‚àÇuh
blank         | 
text          | In general, there is no guarantee this constrained system of equations has a solution. The next
              | section derives the sensitivity equations corresponding to the collocation-based hyperreduced models
              | introduced in this section and develops a minimum-residual variant.
blank         | 
              | 
title         | 4.2.5     Exact and Minimum-Residual Sensitivity Formulation
text          | The sensitivity analysis for the hyperreduced model parallels the exposition for the reduced-order
              | models in Section 4.1.2. The total derivative of the quantity of interest is expanded as
blank         | 
text          |                ‚àÇf                          ‚àÇf                         ‚àÇur
              |    ‚àáFr (¬µ) =      (Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ) +    (Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ)Œ¶     (¬µ; Œ¶, Œ®, P )       (4.100)
              |                ‚àÇ¬µ                          ‚àÇu                         ‚àÇ¬µ
blank         | 
text          | where ur (¬µ; Œ¶, Œ®, P ) are the reduced coordinates corresponding to the solution of the hyperreduced
              |                      ‚àÇur
              | model in (4.82) and      (¬µ; Œ¶, Œ®, P ) is the corresponding sensitivity. The reduced sensitivities are
              |                      ‚àÇ¬µ
              | derived by differentiating the governing hyperreduced model in (4.82). In the general case where
              | P T Œ® is state- and parameter-dependent, the reduced sensitivities are defined as the solution of the
              | linear system of equations
              |                      Ô£Æ                                                        Ô£π
              |                        |M|            T   T
blank         |                                                
text          |                                 ‚àÇ (P    Œ®)  e                           ‚àÇr
              |                                              j                             h  Ô£ª ‚àÇur =
              |                       X
              |                      Ô£∞ (P T r)j                  PÃÑ PÃÑ T Œ¶ + (P T Œ®)T
              |                        j=1
              |                                       ‚àÇu                                ‚àÇu  h   ‚àÇ¬µ
              |                                   Ô£Æ                                                   Ô£π       (4.101)
              |                                     |M|                  T   T
blank         |                                                                    
text          |                                    X             ‚àÇ   (P    Œ®)  e j               ‚àÇr h
              |                                 ‚àí Ô£∞ (P T r)j                         + (P T Œ®)T       Ô£ª
              |                                     j=1
              |                                                          ‚àÇ¬µ                      ‚àÇ¬µ
blank         | 
              | 
text          | where all terms are evaluated at the primal solution uh (¬µ; Œ¶, Œ®, P ). In the special case where the
              | masked primal solution is exact on the mask, i.e., rh (uh (¬µ), ¬µ) = 0, or the masked test basis is
              | constant, the expression in (4.101) reduces to
blank         |                                             
text          |                                           ‚àÇrh ‚àÇur             ‚àÇrh
              |                                  (P T Œ®)T         = ‚àí(P T Œ®)T     .                           (4.102)
              |                                           ‚àÇuh ‚àÇ¬µ              ‚àÇ¬µ
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                           101
blank         | 
              | 
              | 
text          | A Galerkin projection uses a constant test basis P T Œ® = P T Œ¶ and the hyperreduced sensitivity
              | equations takes the form                           
              |                                           T   T ‚àÇrh   ‚àÇur             ‚àÇrh
              |                                         (P Œ¶)             = ‚àí(P T Œ¶)T     .                        (4.103)
              |                                                 ‚àÇuh ‚àÇ¬µ                ‚àÇ¬µ
              |                                                                          ‚àÇrh
              | A LSPG projection employs the non-constant test basis P T Œ®(u, ¬µ) =          (u, ¬µ) and derivatives
              |                                                                          ‚àÇuh
              | of the test basis cannot be ignored. The resulting hyperreduced sensitivity equations are
              |                      Ô£Æ                                                             Ô£π
              |                        |M|                      2
              |                                                                            T
              |                       X
              |                      Ô£∞ (P T r)j (PÃÑ T Œ¶)T PÃÑ T ‚àÇ   r j                 ‚àÇr h   ‚àÇrh  Ô£ª ‚àÇur =
              |                                                        PÃÑ (PÃÑ T Œ¶) +
              |                        j=1
              |                                                ‚àÇu‚àÇu                    ‚àÇu h   ‚àÇu h     ‚àÇ¬µ
              |                                       Ô£Æ                                                     Ô£π      (4.104)
              |                                         |M|                         2
              |                                                                                     T
              |                                        X                           ‚àÇ   rj      ‚àÇr  h   ‚àÇr h
              |                                    ‚àí Ô£∞ (P T r)j (PÃÑ T Œ¶)T PÃÑ T               +              Ô£ª
              |                                         j=1
              |                                                                   ‚àÇu‚àÇ¬µ         ‚àÇu  h    ‚àÇ¬µ
blank         | 
text          |    The difficulty associated with computing derivatives of the test basis, as well as the merits of
              | minimum-residual formulations discussed in Section 4.1, motivate the introduction of a collocation-
              | based equivalent of the minimum-residual sensitivity reduced-order model of Section 4.1.2. For
              | generality, consider the low-dimensional approximation of the high-dimensional model sensitivity
blank         | 
text          |                                                    ‚àÇu      ‚àÇu
              |                                                            dr
              |                                                       = Œ¶‚àÇ    ,                                    (4.105)
              |                                                    ‚àÇ¬µ      ‚àÇ¬µ
blank         | 
text          |                                                                    ‚àÇu
              |                                                                    dr
              | where Œ¶‚àÇ ‚àà RNu √óku is the reduced-order basis for the sensitivities andare the corresponding
              |                                                                     ‚àÇ¬µ
              | reduced coordinates. The reduced coordinates are defined as the argument that minimizes the
              | sensitivity residual on the mask, i.e.,
blank         | 
text          |                    ‚àÇu
              |                    dr                                    1                                  2
              |                       (¬µ; Œ¶‚àÇ , Œò‚àÇ , P , u) = arg min       P T r ‚àÇ (u, PÃÑ PÃÑ T Œ¶‚àÇ wr , ¬µ)          (4.106)
              |                    ‚àÇ¬µ                       wr ‚ààR ku √óN¬µ 2                                  Œò‚àÇ
blank         | 
              | 
text          | where u ‚àà RNu is any linearization point, usually the reconstructed primal solution, i.e., u =
              | PÃÑ PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®, P ) and Œò‚àÇ  0 is the metric defining the norm. The first-order optimality
              | condition of the linear least-squares problem in (4.106) leads to the normal equations
              |                             T                        d                       T
              |              ‚àÇr                          ‚àÇr              ‚àÇur        ‚àÇr                      ‚àÇr
              |         PT      PÃÑ PÃÑ T Œ¶‚àÇ        Œò‚àÇ P T    PÃÑ PÃÑ T Œ¶‚àÇ       = ‚àí PT    PÃÑ PÃÑ T Œ¶‚àÇ    Œò‚àÇ P T    .   (4.107)
              |              ‚àÇu                          ‚àÇu              ‚àÇ¬µ         ‚àÇu                      ‚àÇ¬µ
blank         | 
text          | where all terms are evaluated at the linearization point. A variant of the monotonicity and inter-
              | polation properties of Proposition 4.2 hold for masked minimum-residual sensitivity hyperreduced
              | model. In this case, monotonicity is guaranteed with respect to a fixed metric and mask and in-
              | terpolation requires a sufficiently large mask such that P T r(u, ¬µ) = 0 =‚áí r(u, ¬µ) = 0. These
              | results are stated and proved in Proposition 4.7.
blank         | 
text          | Proposition 4.7. Let (Œ¶‚àÇ , Œò‚àÇ , P ) define a masked minimum-residual sensitivity reduced-order
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                                       102
blank         | 
              | 
              | 
text          | model. Then the following properties hold for any ¬µ ‚àà RN¬µ
blank         | 
text          |    ‚Ä¢ (Optimality) For any u ‚àà RNu , w ‚àà RNu , and PÃÑ T w ‚àà col(PÃÑ T Œ¶‚àÇ ), then
              |                                                                                    !
              |                                       ‚àÇu
              |                                       dr
              |                                               ‚àÇ
              |                  T
              |                      rk‚àÇ             T
              |                                          (¬µ; Œ¶‚àÇ , Œò‚àÇ , P , u)ek , ¬µ                             ‚â§ P T rk‚àÇ u, PÃÑ PÃÑ T w, ¬µ
blank         |                                                                                                                          
text          |              P             u, PÃÑ PÃÑ Œ¶                                                                                        Œò‚àÇ
              |                                       ‚àÇ¬µ
              |                                                                                         Œò‚àÇ
              |                                                                                                                              (4.108)
              |        for k = 1, . . . , N¬µ , where                rk‚àÇ (u,                   ‚àÇ         T
              |                                                               w ¬∑ ek , ¬µ) := r (u, we , ¬µ)ek and ek is the kth canonical
              |        unit vector.
              |                                               0       0
              |    ‚Ä¢ (Monotonicity) Let (Œ¶‚àÇ , Œò‚àÇ , P ) define a masked minimum-residual sensitivity reduced-order
              |                                           0
              |        model such that col(Œ¶‚àÇ ) ‚äÜ col(Œ¶‚àÇ ), then
blank         | 
text          |                                                   P T rk‚àÇ (u, w, ¬µ)   Œò‚àÇ
              |                                                                            ‚â§ P T rk‚àÇ (u, w0 , ¬µ)      Œò‚àÇ
              |                                                                                                            ,                 (4.109)
blank         | 
text          |                           ‚àÇu
              |                           dr                                     0 ‚àÇu
              |                                                                    dr       0    0
              |        where w = Œ¶‚àÇ           (¬µ; Œ¶‚àÇ , Œò‚àÇ , P , u)ek and w0 = Œ¶‚àÇ      (¬µ; Œ¶‚àÇ , Œò‚àÇ , P , u)ek , for k =
              |                           ‚àÇ¬µ                                       ‚àÇ¬µ
              |        1, . . . , N¬µ and any u ‚àà RN¬µ .
              |                                          ‚àÇu
              |    ‚Ä¢ (Interpolation) If PÃÑ T                 (¬µ) ‚àà col(PÃÑ T Œ¶‚àÇ ), then
              |                                          ‚àÇ¬µk
              |                                                                                                 !
              |                                                                    ‚àÇu
              |                                                                    dr
              |                                    P T rk‚àÇ           u, PÃÑ PÃÑ T Œ¶‚àÇ    (¬µ; Œ¶‚àÇ , Œò‚àÇ , P , u)ek , ¬µ = 0.                        (4.110)
              |                                                                    ‚àÇ¬µ
blank         | 
              | 
text          |                                             ‚àÇu
              |                                             dr
              | Proof. Optimality follows from the fact that     (¬µ; Œ¶‚àÇ , Œò‚àÇ , P , u) is the (unique) minima of the
              |                                             ‚àÇ¬µ
              | optimization problem in (4.106). A simple application of the optimality property to
blank         | 
text          |                                      0 ‚àÇu
              |                                        dr                     0   0                         0
              |                            PÃÑ T Œ¶‚àÇ                 (¬µ; Œ¶‚àÇ , Œò‚àÇ , P , u) ‚àà col(PÃÑ T Œ¶‚àÇ ) ‚äÜ col(PÃÑ T Œ¶‚àÇ )
              |                                          ‚àÇ¬µ
blank         | 
text          | leads to monotonicity. Finally, if a solution of P T r ‚àÇ (PÃÑ PÃÑ T u(¬µ), ¬∑ , ¬µ) = 0 is contained in the
              |                                     ‚àÇu
              | columnspace of PÃÑ T Œ¶‚àÇ , i.e., PÃÑ T    (¬µ) ‚àà col(PÃÑ T Œ¶‚àÇ ), the optimality property implies
              |                                     ‚àÇ¬µ
              |                                                                               !                                   
              |                           ‚àÇu
              |                           dr
              |                                ‚àÇ                                                                        ‚àÇ uÃÇ
              |    T
              |  P r   ‚àÇ
              |            uÃÇ(¬µ), PÃÑ PÃÑ Œ¶  T
              |                              (¬µ; Œ¶‚àÇ , Œò‚àÇ , P , uÃÇ(¬µ)), ¬µ                               ‚â§ P T r ‚àÇ uÃÇ(¬µ),      (¬µ), ¬µ               = 0,
              |                           ‚àÇ¬µ                                                                            ‚àÇ¬µ                   Œò‚àÇ
              |                                                                                   Œò‚àÇ
              |                                                                                               (4.111)
              |                            T ‚àÇ uÃÇ             T ‚àÇu
              | where uÃÇ(¬µ) = PÃÑ PÃÑ u(¬µ) and      (¬µ) = PÃÑ PÃÑ      (¬µ), which is precisely the interpolation property
              |                              ‚àÇ¬µ                 ‚àÇ¬µ
              | in (4.110).
blank         | 
text          |    In addition to monotonicity and interpolation, conditions exist (Proposition 4.8) that guarantee
              | the two types of sensitivities introduced in this section, i.e., the sensitivities of the hyperreduced
              | model and the masked minimum-residual hyperreduced sensitivities, agree. These conditions also
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                              103
blank         | 
              | 
              | 
text          | guarantee that the reconstruction of these sensitivities in the full space yield the same approximation
              | of the high-dimensional model sensitivities. Among these conditions is a required relationship (4.112)
              | between the trial (Œ¶) and test basis (Œ®) for the primal hyperreduced model, the sensitivity metric
              | (Œò), and the sensitivity basis (Œ¶‚àÇ ). The result of Proposition 4.8 is significant since it provides
              | conditions under which the easily computed masked minimum-residual hyperreduced sensitivities
              | (independent of second derivatives of r) match the desired hyperreduction sensitivities (guarantee
              | consistency of gradient computations).
blank         | 
text          | Proposition 4.8. Consider a primal hyperreduced model defined by trial and test bases Œ¶ and Œ®,
              | respectively, and mask P and a minimum-residual sensitivity hyperreduced model defined by basis
              | Œ¶‚àÇ , mask P , and metric Œò‚àÇ . Suppose that either: (1) the primal solution of the hyperreduced model
              | exactly reconstructs the HDM solution on the sample mesh, i.e.,
blank         | 
text          |                                          PÃÑ T u(¬µ) = PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®)
blank         | 
text          | or (2) the masked test basis P T Œ® is constant. Then, for any u ‚àà RNu , the relationships
blank         | 
text          |                                           P T Œ¶‚àÇ = P T Œ¶
              |                                                            ‚àÇr                                        (4.112)
              |                                     P T Œ®(u, ¬µ) = Œò‚àÇ P T      (u, ¬µ)PÃÑ PÃÑ T Œ¶‚àÇ
              |                                                            ‚àÇu
blank         | 
text          | guarantee the sensitivity of the primal hyperreduced model (Œ¶, Œ®, P ) coincides with the solution of
              | the minimum-residual sensitivity hyperreduced model (Œ¶‚àÇ , Œò‚àÇ , P )
blank         | 
text          |                       ‚àÇur                 ‚àÇu
              |                                           dr
              |                           (¬µ; Œ¶, Œ®, P ) =    (¬µ; Œ¶‚àÇ , Œò‚àÇ , P , Œ¶ur (¬µ; Œ¶, Œ®, P )).                   (4.113)
              |                       ‚àÇ¬µ                  ‚àÇ¬µ
blank         | 
text          | Proof. Let PÃÑ T Œ¶ur = PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®, P ) denote the reconstructed primal solution of the projection-
              | based reduced-order model, restricted to the primal mesh. If either the primal solution is exact
              | (P T r(PÃÑ PÃÑ T Œ¶ur , ¬µ) = 0) or the test basis is constant, the general form of the reduced-order model
              | sensitivity equations in (4.101) reduces to the equation in (4.102), where all terms are evaluated at
              | the primal solution, i.e.,
blank         |                                                     
text          |                              T ‚àÇrh                    ‚àÇur                           T ‚àÇrh
              |     P T Œ®(PÃÑ PÃÑ T Œ¶ur , ¬µ)          (PÃÑ PÃÑ T Œ¶ur , ¬µ)      = ‚àí P T Œ®(PÃÑ PÃÑ T Œ¶ur , ¬µ)       (PÃÑ PÃÑ T Œ¶ur , ¬µ).
              |                                 ‚àÇuh                    ‚àÇ¬µ                               ‚àÇ¬µ
              |                                                                                                       (4.114)
              | Conversely, the normal form of the minimum-residual sensitivity reduced-order model in (4.107)
              | reduces to
              |                                                   d
              |                           T ‚àÇrh                    ‚àÇur                           T ‚àÇrh
              |     P T Œ®(PÃÑ PÃÑ T Œ¶ur , ¬µ)       (PÃÑ PÃÑ T Œ¶ur , ¬µ)      = ‚àí P T Œ®(PÃÑ PÃÑ T Œ¶ur , ¬µ)       (PÃÑ PÃÑ T Œ¶ur , ¬µ).
              |                              ‚àÇuh                    ‚àÇ¬µ                               ‚àÇ¬µ
              |                                                                                                    (4.115)
              | when the relationships in (4.112) are enforced. Thus, under the aforementioned conditions, the
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                            104
blank         | 
              | 
              | 
text          |                            ‚àÇur     ‚àÇu
              |                                    dr
              | governing equations for        and    are identical and the (unique) solutions must be equal, which
              |                            ‚àÇ¬µ      ‚àÇ¬µ
              | establishes (4.113).
blank         | 
              | 
title         | 4.2.6     Adjoint Formulation
text          | The adjoint equations for the collocation-based hyperreduced model are derived using the optimiza-
              | tion procedure, outlined in Section 2.3.4, applied to the governing equation (P T Œ®)T rh (PÃÑ PÃÑ T Œ¶ur , ¬µ) =
              | 0 and reduced quantity of interest f (Œ¶ur , ¬µ). Consider the auxiliary optimization problem
blank         | 
text          |                                     minimize       f (Œ¶ur , ¬µÃÇ)
              |                                       ur ‚ààRku
              |                                                                                                     (4.116)
              |                                                       T    T
              |                                     subject to     (P Œ®) rh (uh , ¬µÃÇ) = 0
blank         | 
text          | for a fixed ¬µÃÇ and the corresponding Lagrangian
blank         | 
text          |                         Lr (ur , Œªr ) = f (Œ¶ur , ¬µÃÇ) ‚àí ŒªTr (P T Œ®)T rh (PÃÑ PÃÑ T Œ¶ur , ¬µÃÇ).          (4.117)
blank         | 
text          | by comparing this expression for the Lagrangian with that in (2.98), it is clear that the masked
              | HDM Lagrange multipliers are reconstructed from the reduced Lagrange multipliers as
blank         | 
text          |                                                    Œª = Œ®Œªr .                                        (4.118)
blank         | 
text          | The stationarity of the Lagrangian with respect to ur leads to the reduced adjoint equations
              |                  Ô£Æ                                                Ô£π
              |                    |M|
              |                                                                              ‚àÇf T
              |                                 T  T
blank         |                                        
text          |                   X         ‚àÇ (P Œ®) ej                        ‚àÇrh Ô£ª
              |                  Ô£∞ (P T r)j              PÃÑ PÃÑ T Œ¶ + (P T Œ®)T       Œª r = Œ¶T                        (4.119)
              |                    j=1
              |                                 ‚àÇu                            ‚àÇuh            ‚àÇu
blank         | 
              | 
text          | where all terms are evaluated at the reconstructed primal solution, PÃÑ PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®, P ). For any
              | ¬µ ‚àà RN¬µ , bases Œ¶, Œ®, and mask P , the solution of the above equation is denoted Œªr (¬µ; Œ¶, Œ®, P ).
              | The gradient of the quantity of interest is reconstructed as
blank         | 
text          |                        ‚àÇf
              |          ‚àáFr (¬µ) =         (Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ)‚àí
              |                        ‚àÇ¬µ
              |                                          Ô£Æ                                  Ô£π
              |                                            |M|      ‚àÇ Œ®T e j                                        (4.120)
              |                                           X                               ‚àÇrh
              |                        Œªr (¬µ; Œ¶, Œ®, P )T Ô£∞ (P T r)j            + (P T Œ®)T     Ô£ª
              |                                            j=1
              |                                                        ‚àÇ¬µ                 ‚àÇ¬µ
              |                                                                                          (uh , ¬µ)
blank         | 
              | 
text          | where uh = PÃÑ PÃÑ T Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ). In the special case where the primal solution is exact on
              | the mask, in the sense that rh (uh , ¬µ) = 0, or the masked test basis is constant, the adjoint equations
              | in (4.119) reduce to
              |                                                   T
              |                                                              ‚àÇf T
blank         |                                       
text          |                                             T ‚àÇrh
              |                                          T
              |                                        (P Œ®)         Œªr = Œ¶T                                        (4.121)
              |                                               ‚àÇuh            ‚àÇu
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                              105
blank         | 
              | 
              | 
text          | and the gradient of the QoI becomes
blank         |                                                                             
text          |                   ‚àÇf                                                     ‚àÇrh
              |       ‚àáFr (¬µ) =      (Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ) ‚àí Œªr (¬µ; Œ¶, Œ®, P )T (P T Œ®)T                              (4.122)
              |                   ‚àÇ¬µ                                                     ‚àÇ¬µ (uh , ¬µ)
blank         | 
text          |                             ‚àÇf T
              |    In general, the term Œ¶T        requires O(Nu ) works and memory to evaluate. However, in many
              |                             ‚àÇu
              | applications f corresponds to an integral over a surface or small portion of the domain and therefore
              |                                                             ‚àÇf T                  ‚àÇf T
              | ‚àÇf /‚àÇu is sparse. Then this terms is exactly equal to Œ¶T         = (PÃÇ T Œ¶)T PÃÇ T      , where PÃÇ is the
              |                                                             ‚àÇu                    ‚àÇu
              | subset of columns of the identity matrix that exactly restricts ‚àÇf /‚àÇu to its nonzero entries and can
              | be computed without requiring large-scale operations. This is an important implementation detail;
              | however, for simplicity, the additional notation will not be continued.
              |    In the special case where the primal hyperreduced model employs a Galerkin projection (P T Œ® =
              | P T Œ¶), the test basis is state- and parameter-independent and the adjoint equations in (4.119) or
              | (4.121) become
              |                                                      T
              |                                                                 ‚àÇf T
blank         |                                        
text          |                                                  ‚àÇrh
              |                                         (P T Œ¶)T        Œªr = Œ¶T                                       (4.123)
              |                                                  ‚àÇuh            ‚àÇu
              | and the gradient of the QoI is
blank         |                                                                             
text          |                   ‚àÇf                                                     ‚àÇrh
              |       ‚àáFr (¬µ) =      (Œ¶ur (¬µ; Œ¶, Œ¶, P ), ¬µ) ‚àí Œªr (¬µ; Œ¶, Œ¶, P )T (P T Œ¶)T                              (4.124)
              |                   ‚àÇ¬µ                                                     ‚àÇ¬µ (uh , ¬µ)
blank         | 
text          | In the special case of a LSPG projection, the adjoint equations in (4.119) become
              |          Ô£Æ                                                       Ô£π
              |            |M|                                              T
              |                                      2
              |                                                                             ‚àÇf T
blank         |                                            
text          |           X                        ‚àÇ   rj               ‚àÇrh   ‚àÇrhÔ£ª
              |          Ô£∞ (P T r)j (PÃÑ T Œ¶)T PÃÑ T        PÃÑ (PÃÑ T Œ¶) +            Œª r = Œ¶T                           (4.125)
              |            j=1
              |                                    ‚àÇu‚àÇu                 ‚àÇuh ‚àÇuh             ‚àÇu
blank         | 
text          | and the gradient of the QoI is
blank         | 
text          |              ‚àÇf
              |   ‚àáFr (¬µ) =      (Œ¶ur (¬µ; Œ¶, Œ®, P ), ¬µ)‚àí
              |              ‚àÇ¬µ
              |                                Ô£Æ                                                 Ô£π
              |                                  |M|                      2
              |                                                                                                      (4.126)
              |                                 X                        ‚àÇ   rj              ‚àÇrh
              |              Œªr (¬µ; Œ¶, Œ®, P )T Ô£∞ (P T r)j (PÃÑ T Œ¶)T PÃÑ T          + (P T Œ®)T     Ô£ª                .
              |                                  j=1
              |                                                          ‚àÇu‚àÇ¬µ                ‚àÇ¬µ
              |                                                                                        (uh , ¬µ)
blank         | 
              | 
text          |    The introduction and derivation of the masked minimum-residual hyperreduced adjoint model
              | will be deferred to future work. There are special considerations that rise when considering the
              | minimization problem
blank         | 
text          |                                                                 ‚àÇf T      ‚àÇr
              |   minimize    P T r Œª (u, Œ¶Œª zr , ¬µ)        = minimize   ‚àíP T        + PT    (u, ¬µ)T Œ¶Œª zr            (4.127)
              |    zr ‚ààRku                             ŒòŒª      zr ‚ààRku          ‚àÇu        ‚àÇu
              |                                                                                              ŒòŒª
blank         | 
text          |                      ‚àÇr
              | since the term P T      (u, ¬µ)T Œ¶Œª zr requires a separate restriction matrix PÃÇ (subset of the columns
              |                      ‚àÇu
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                         106
blank         | 
              | 
              | 
text          | of the identity matrix) for the following identity to hold
blank         | 
text          |                                   ‚àÇr                  ‚àÇr
              |                              PT      (u, ¬µ)T Œ¶Œª = P T    (u, ¬µ)T PÃÇ PÃÇ T Œ¶Œª                     (4.128)
              |                                   ‚àÇu                  ‚àÇu
blank         | 
text          | and lead to operations independent of the large dimension Nu . This is the exact type of imple-
              | mentation optimization discussed in Section 4.2.2 that lead to the efficient computations with the
              | masked reduced Jacobian (no transpose)
blank         | 
text          |                                     ‚àÇr               ‚àÇr
              |                                PT      (u, ¬µ)Œ¶ = P T    (u, ¬µ)PÃÑ PÃÑ T Œ¶.                        (4.129)
              |                                     ‚àÇu               ‚àÇu
blank         | 
              | 
title         | 4.3     Construction of Reduced-Order Basis and Residual Mask
text          | The present exposition on projection-based reduced-order models has focused on the formulation of
              | the governing equations that guarantee desirable properties for a fixed trial basis Œ¶ and mask P ;
              | however, there has been no mention of the origin of these quantities, a process usually called training.
              | The specific training strategy will vary for the various applications encountered in Chapters 5‚Äì
              | 6 and an in-depth discussion will be deferred to the appropriate chapter. This section details
              | commonalities between the training methods employed in those chapters to facilitate the discussion.
              | Additionally, a general discussion is provided on training concepts used to enforce conditions required
              | for Propositions 4.1, 4.2, 4.4 to hold.
              |    A ubiquitous theme in all reduced-order model training algorithms considered in this document is
              | the method of snapshots [183]. This is the idea of building the reduced-order basis Œ¶ from solutions,
              | or snapshots, of the high-dimensional model. In addition to building the basis from fully converged
              | solutions (individual time steps for unsteady problems [183] or steady states for steady problems),
              | unconverged nonlinear iterations [198], unconverged linear system iterates [198], sensitivities [87, 86,
              | 32, 85, 52, 210, 198], and adjoint solutions [57, 74] have also been used. These various snapshots
              | are combined into the columns of a snapshot matrix with Ns columns. This approach ensures the
              | reduced-order basis includes relevant, information-rich basis vectors that incorporate physics from
              | the underlying PDE and, in many cases, even a small reduced-order basis can result in an accurate
              | reduced-order model.
              |    When the number of snapshots becomes large, it is desirable to apply a compression method
              | to retain most of the original information contained in the snapshots. Among the most popular
              | methods is the Proper Orthogonal Decomposition (POD), Algorithm 4, also known as the truncated
              | Singular Value Decomposition (SVD), Karhunen-LoeÃÄve (KL) decomposition, and Principal Compo-
              | nent Analysis (PCA). POD possess the desirable property of ordering the potential basis vectors
              | according to energy, or importance with regard to reconstructing the snapshot matrix. Therefore the
              | reduced-order basis is taken as the first ku vectors, where ku is chosen based on the singular value
              | decay or naively according to a desired basis size. The latter approach is described in Algorithm 4
              | and the operation of applying POD to build a reduced-order basis Œ¶ from the snapshot matrix X
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                          107
blank         | 
              | 
              | 
text          | will be denoted Œ¶ = POD(X).
blank         | 
title         | Algorithm 4 Proper Orthogonal Decomposition
blank         | 
text          |                                                Œ¶ = POD(X)
              | Input: Snapshot matrix X ‚àà RNu √óks and reduced-order basis size ku
              | Output: Reduced-order basis Œ¶                                                        
              |  1: Compute the thin SVD of X: X = U Œ£V T , where U = u1 u2 ¬∑ ¬∑ ¬∑               uks
blank         |                         
text          |  2: Œ¶ = u1 u2 ¬∑ ¬∑ ¬∑ uku
blank         | 
              | 
text          |    POD is well-known to be susceptible to bias when there is substantial variation in the scale of
              | the columns of the snapshot matrix. This will occur, for example, when a heterogeneous collection
              | of snapshots are used, i.e., states and sensitivities, since the units of the columns will not be consis-
              | tent. The result is sub-optimal compression that favors snapshots with the largest size. Following
              | the work in [210], this is remedied by partitioning the heterogeneous snapshot matrix X into homo-
              | geneous snapshot matrices Y and Z according to X = [Y , Z]. Each homogeneous snapshot matrix
              | is optimally compressed using POD and the results are combined via concatenation to yield the
              | reduced-order basis, i.e., Œ¶ = [POD(Y ), POD(Z)]. This algorithm, denoted Œ¶ = PODH(Y , Z), is sum-
              | marized in Algorithm 5 and includes a final step that employs a QR factorization to orthogonalize the
              | basis. An alternate approach, known as Compact Proper Orthogonal Decomposition [32], to remove
              | the potential bias of POD, specifically when states and sensitivity snapshot are used, weights the
              | sensitivity snapshots according to the magnitude of parameter perturbations. The former approach
              | based on compression of homogeneous submatrices is preferred in this work due to its generality
              | in handling any types of snapshots, flexibility in handling more than two types of snapshots, and
              | optimality in compressing individual snapshot types (since the compression is POD-based).
blank         | 
text          | Algorithm 5 Proper Orthogonal Decomposition for Heterogeneous Data
blank         | 
text          |                                           Œ¶ = PODH(Y , Z)
blank         |                                                   
text          | Input: Heterogeneous snapshot matrix, X = Y Z and truncation sizes ky and kz
              | Output: Reduced-order basis Œ¶
              |  1: Compute the thin SVD of Y : Y = UY Œ£Y VYT
              |  2: Compute the thin SVD of Z: Z = UZ Œ£Z VZT
              |  3: Form matrix of dominant singular vectors      
              |  4: W = (uY )1 ¬∑ ¬∑ ¬∑ (uY )ky (uZ )1 ¬∑ ¬∑ ¬∑ (uZ )kz
              |  5: Orthogonalize columns of W via QR factorization, Œ¶R = W
blank         | 
              | 
text          |    Another desirable property that POD does not possess is the exact preservation of a particular
              | subset snapshots in the span of the reduced basis. The interpolation property of minimum-residual
              | reduced-order models motivates such a property. For some ¬µ ‚àà RN¬µ , suppose X = [u(¬µ), X2 ]
              | where u(¬µ) is the exact solution of the high-dimensional model and X2 contains other snapshots.
              | If u(¬µ) ‚àà span(Œ¶), the resulting (minimum-residual) reduced-order model will exactly recover this
              | solution (Proposition 4.1). This property will prove particularly important in Chapters 5‚Äì6, where
              | a certain level of accuracy is required at trust region centers. However, if POD is applied to X,
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                        108
blank         | 
              | 
              | 
text          | u(¬µ) 6‚àà span(Œ¶), in general, even if u(¬µ) ‚àà span(X). To enhance POD to exactly preserve a
              | subsets of the snapshots in the reduced subspace, consider the decomposition of the snapshots as
              | X = [X1 , X2 ], where X1 contains the snapshots to be preserved. POD compression is applied
              | only to the snapshot matrix X2 and the reduced-order basis is defined as Œ¶ = [X1 , POD(X2 )]. This
              | algorithm, denoted Œ¶ = PODSP(X1 , X2 ), is summarized in Algorithm 6 and includes a final step
              | that employs a QR factorization to orthogonalize the basis.
blank         | 
text          | Algorithm 6 Proper Orthogonal Decomposition with Span Preservation
blank         | 
text          |                                      Œ¶ = PODSP(X1 , X2 )
              |                                  Nu √óks
blank         |                                                        
text          | Input: Snapshot matrix X ‚àà R        where X = X1 X2 and truncation size kx
              | Output: Reduced-order basis Œ¶ such that span X1 ‚äÇ span Œ¶                
              |  1: Compute the thin SVD of X2 : X2 = U Œ£V T , where U = u1 u2 ¬∑ ¬∑ ¬∑ uks
blank         |                            
text          |  2: W = X1 u1 ¬∑ ¬∑ ¬∑ ukx
              |  3: Orthogonalize columns of W via QR factorization, Œ¶R = W
blank         | 
              | 
text          |    In many cases, heterogeneous snapshots are encountered and certain subsets of each homoge-
              | neous snapshot collection must be preserved in the span of the basis. For example, the minimum-
              | residual sensitivity reduced-order models of Section 4.1.2 exactly recover the exact sensitivities at
              |                                                                     ‚àÇu
              | a parameter configuration ¬µ ‚àà RN¬µ if u(¬µ) ‚àà span(Œ¶) and                 (¬µ) ‚àà span(Œ¶). In this situa-
              |                                                                     ‚àÇ¬µ
              | tion, it is desirable to utilize both state and sensitivity snapshots and preserve the exact state and
              | sensitivity corresponding to parameter configuration ¬µ. This will have important implications in
              | the context of the trust region method introduced in Chapter 5 that requires a certain level of
              | accuracy, in both the objective and gradient, at trust region centers. In such situations, it is de-
              | sirable to combine the basic enhancements to POD introduced in Algorithms 5 and 6. For this
              | purpose, decompose the heterogeneous snapshot matrix X into homogeneous snapshot matrices
              | Y and Z. Further decompose these snapshot matrices according to the subset that must be pre-
              | served in the reduced subspace, i.e., Y = [Y1 , Y2 ] and Z = [Z1 , Z2 ] where the columns of Y1 and
              | Z1 must be contained in the span of Œ¶. This yields the decomposition of the original snapshot
              | matrix as X = [Y1 , Y2 , Z1 , Z2 ] and the basis is defined via POD-based compression to Y2 and
              | Z2 only, i.e., Œ¶ = [Y1 , Z1 , POD(Y2 ), POD(Z2 )] = [Y1 , Z1 , PODH(Y2 , Z2 )]. This algorithm, denoted
              | Œ¶ = PODHSP(Y1 , Y2 , Z1 , Z2 ), is summarized in Algorithm 7 and includes a final step that employs
              | a QR factorization to orthogonalize the basis.
              |    At the core of POD, and all the variants introduced in this section, lies a singular value decom-
              | position, which remains among the most expensive matrix factorizations. In many large-scale PDE
              | applications, particularly time-dependent applications, the snapshot matrix that is passed to POD
              | for compression may have O(108 ) rows and O(103 ) columns, which requires a substantial amount of
              | computational resources and will be extremely time- and memory-intensive. To reduce the burden
              | of the large-scale SVD computation, a low-rank approximation of the singular value decomposition
              | [81] will be employed for the large-scale CFD problems encountered in Section 5.5.4. The random-
              | ized, low-rank SVD, summarized in Algorithm 8, computes the standard SVD of original matrix
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                         109
blank         | 
              | 
              | 
text          | Algorithm 7 Proper Orthogonal Decomposition for Heterogeneous Data with Span Preservation
blank         | 
text          |                                     Œ¶ = PODHSP(Y1 , Y2 , Z1 , Z2 )
blank         |                                                                                                 
text          | Input: Heterogeneous snapshot matrix X = Y Z , where Y = Y1 Y2 and Z = Z1                          Z2 ,
              |     and truncation sizes, ky and kz
              | Output: Reduced-order basis Œ¶ such that span Y1 ‚äÇ span Œ¶ and span Z1 ‚äÇ span Œ¶
              |  1: Compute the thin SVD of Y2 : Y2 = UY Œ£Y VYT
              |  2: Compute the thin SVD of Z2 : Z2 = UZ Œ£Z VZT
              |  3: Form matrix of dominant singular vectors
blank         |                                                                         
text          |                      W = Y1 Z1 (uY )1 ¬∑ ¬∑ ¬∑ (uY )ky (uZ )1 ¬∑ ¬∑ ¬∑ (uZ )kz
blank         | 
text          |  4:   Orthogonalize columns of W via QR factorization, Œ¶R = W
blank         | 
              | 
text          | projected into a low-dimensional subspace that is constructed through random linear combinations
              | of the matrix. Since a SVD computation scales linearly with the number of rows and quadratically
              | in the number of columns, a substantial performance improvement comes with performing the SVD
              | in the reduced space.
              |       Another bottleneck encountered with all variants of POD is that even low-rank modifications to
              | the underlying snapshots, in general, requires re-computing the SVD from scratch. This is significant
              | since appending new snapshots to the snapshot matrix or re-centering the snapshot matrix cannot
              | necessarily re-use the previous singular factors. A series of papers by Brand [25, 26] changed this
              | landscape as they introduced a series of algorithms for low-rank updates to the SVD. This algorithm,
              | summarized in Algorithm 9 for the case of appending new snapshots to the snapshot matrix and
              | Algorithm 10 for the case for re-centering the snapshot matrix, enables the singular factors of the
              | original SVD to be re-used to compute the SVD of the low-rank update to the snapshot matrix. The
              | cost is mostly independent of operations that scale with the size of the original snapshot matrix.
blank         | 
title         | Algorithm 8 Low-Rank Probabilistic SVD Approximation
blank         | 
text          |                                         U , Œ£, V = ProbSVD(X, k, q)
              | Input: A ‚àà Rm√ón (usually n  m), approximation rank k, and number of power iterations q
              | Output: Approximate SVD of A ‚âà UŒ£VT
              |  1: Generate n √ó 2k Gaussian test matrix ‚Ñ¶
              |  2: Form Y = (AAT )q A‚Ñ¶
              |  3: Compute QR factorization of Y: Y = QR
              |  4: Form B = QT A
              |  5: Compute SVD of B = UÃÉŒ£VT
              |  6: Set U = QUÃÉ
blank         | 
              | 
text          |       This completes the discussion of the algorithms that will prove useful in defining the trial basis
              | Œ¶ from snapshot data. Chapters 5‚Äì6 will provide specific training methods that collect snapshots
              | according to the requirements of the trust region-based optimization algorithm. This section closes
              | with a brief note on the construction of the mask P when collocation-based hyperreduction is
              | employed. The sample mesh PÃÑ will not be discussed since it is determined uniquely from the mask,
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                          110
blank         | 
              | 
              | 
              | 
text          | Algorithm 9 Brand‚Äôs Algorithm for low-rank SVD updates: appending vector
blank         | 
text          |                            UÃÑ , Œ£ÃÑ, VÃÑ = BrandAppendSVD(U , Œ£, V , Y )
              | Input: Data matrix X ‚àà Rm√ón of rank r, thin SVD of data matrix X = U Œ£V T , and full-rank
              |     matrix of vectors Y ‚àà Rm√ók .                
              | Output: SVD of updated data matrix: X Y = UÃÑ Œ£ÃÑVÃÑ T
              |  1: Compute M = U T Y ‚àà Rr√ók
              |  2: Compute PÃÑ = Y ‚àí U M ‚àà Rm√ók
              |  3: Compute QR  decomposition
              |                                 of PÃÑ = P RA , where P ‚àà Rm√ók , RA ‚àà Rk√ók
              |                 Œ£ M
              |  4: Form K =               ‚àà R(r+k)√ó(r+k)
              |                  0 RA
              |  5: Compute SVD of K = CSD T , where C, S, D ‚àà R(r+k)√ó(r+k)
              |  6:                                                                   
              |                                                                 V 0
              |                           UÃÑ = U P C            Œ£ÃÑ = S      VÃÑ =         D
              |                                                                    0 I
blank         | 
              | 
              | 
              | 
text          | Algorithm 10 Brand‚Äôs Algorithm for low-rank SVD updates: translating columns
blank         | 
text          |                           UÃÑ , Œ£ÃÑ, VÃÑ = BrandTranslateSVD(U , Œ£, V , a)
              | Input: Data matrix X ‚àà Rm√ón of rank r, thin SVD of data matrix X = U Œ£V T , and desired
              |     translation vector, a ‚àà Rm
              | Output: SVD of updated data matrix: X + a1T = UÃÑ Œ£ÃÑVÃÑ T
              |  1: Compute n = V T 1, q = 1 ‚àí V n, q = kqk2 , and Q = 1q q
              |  2: Compute m = U T a ‚àà Rr
              |  3: Compute p = a ‚àí U m ‚àà Rm
              |  4: Define rÃÇ ‚àà R and v ‚àà RN such that: p = rÃÇv where kvk2 = 1
              |                  Œ£ + mnT qm
blank         |                                 
text          |  5: Form K =                       ‚àà R(r+1)√ó(r+1)
              |                      rÃÇn     rÃÇq
              |  6: Compute SVD of K = CSD T , where C, S, D ‚àà R(r+1)√ó(r+1)
              |  7:                                                               
              |                        UÃÑ = U    v C        Œ£ÃÑ = S        VÃÑ = V    Q D
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                        111
blank         | 
              | 
              | 
text          | as discussed in Section 4.2.2. The mask is constructed according to the DEIM algorithm introduced
              | in [41] and generalized in [31, 198]. The variant introduced in [198] will be employed in this work
              | due to its proven robustness in handling vector-valued PDE solutions where the variables in each
              | component have different scales and the flexibility afforded by injecting expert knowledge, which
              | proved crucial in large-scale CFD applications [198].
blank         | 
              | 
title         | 4.4     Summary
text          | With all of the ingredients for efficient and optimal projection-based model reduction introduced in
              | Sections 4.1‚Äì4.3, this section provides an overview of the overall framework and makes important
              | connections between the various components that will be leveraged in Chapters 5‚Äì6. The general
              | form of projection-based reduced-order models was introduced in (4.3)
blank         | 
text          |                                find ur ‚àà Rku such that Œ®T r(Œ¶ur , ¬µ) = 0.                      (4.130)
blank         | 
text          | It is uniquely defined by a trial basis Œ¶ and test basis Œ®, which may be chosen arbitrarily and
              | independently, in general. In order to ensure the reduced-order model possesses the minimum-
              | residual property, the test and trial basis must be related to one another and the optimality metric
              | Œò according to (4.14), i.e.,
              |                                                      ‚àÇr
              |                                        Œ®(u, ¬µ) = Œò      (u, ¬µ)Œ¶.                               (4.131)
              |                                                      ‚àÇu
              | The minimum-residual property is a desirable since it guarantees the approximation generated by
              | the reduced-order model monotonically improves (in terms of the residual norm in some metric)
              | as the trial basis is expanded and exactly reconstructs training data. These properties are known
              | as monotonicity and interpolation (Proposition 4.1). The interpolation property requires u(¬µ) ‚àà
              | col(Œ¶), where u(¬µ) is the solution of r( ¬∑ , ¬µ) = 0, which will be guaranteed using the span-preserving
              | variant of POD (PODSP in Algorithm 6). However, it will not be efficient or even practical to
              | require the reduced-order model be interpolatory at every ¬µ ‚àà RN¬µ . Instead, n interpolation points
              | {¬µ1 , . . . , ¬µn } are selected and a snapshot matrix is constructed as
              |                                          h                      i
              |                                       X = u(¬µ1 ) ¬∑ ¬∑ ¬∑    u(¬µn ) .                             (4.132)
blank         | 
text          | Additionally, let X 0 be any collection of primal snapshot to be used to construct the trial basis
              | whose columns will not necessarily be preserved in the span of the trial space. The trial basis is
              | then constructed as
              |                                           Œ¶ = PODSP(X, X 0 ).                                  (4.133)
blank         | 
text          | In Chapter 5, X will consist of the high-dimensional snapshot at the trust region center, i.e., u(¬µk ),
              | since conditions (3.14) and (3.15) require a prescribed level of accuracy at the center.
              |    With the primal reduced-order model constructed, the sensitivity or adjoint methods introduced
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                              112
blank         | 
              | 
              | 
text          | in Sections 4.1.2‚Äì4.1.3 can be used to derive the gradients of any reduced quantities of interest. How-
              | ever, it was shown that for general minimum-residual reduced-order models, these computations will
              | require second derivatives of the governing residual r, which are expensive to compute and rarely
              | available in large-scale PDE implementations. An alternative that will break discrete consistency,
              | i.e., the computed gradient of the reduced quantity of interest will not match the true gradient of
              | the quantity, is to employ minimum-residual reduced-order models directly for the high-dimensional
              | sensitivity and adjoint equations. Despite breaking discrete consistency, these quantities are com-
              | putable since they do not require second derivatives of r and also possess the minimum-residual
              | properties of monotonicity and interpolation.
              |     The minimum-residual sensitivities, defined in (4.28), are uniquely defined through the specifi-
              | cation of a sensitivity basis Œ¶‚àÇ and optimality metric Œò‚àÇ , i.e.,
              |                                       T            d            T
              |          ‚àÇu
              |          dr
              |                ku √óN¬µ             ‚àÇr ‚àÇ      ‚àÇ   ‚àÇr ‚àÇ ‚àÇu  r      ‚àÇr ‚àÇ       ‚àÇr
              |     find    ‚ààR        such that      Œ¶    Œò        Œ¶       =‚àí      Œ¶    Œò‚àÇ                           (4.134)
              |          ‚àÇ¬µ                       ‚àÇu            ‚àÇu     ‚àÇ¬µ       ‚àÇu         ‚àÇ¬µ
blank         | 
text          | where all terms are evaluated at the (reconstructed) solution of the primal reduced-order model
              | Œ¶ur (¬µ; Œ¶, Œ®). Proposition 4.3 guarantees these two choices for the reduced sensitivities match
              | when the primal solution is exact or the test basis is constant, provided the relationships in (4.35)
              | hold, i.e.,
              |                                               Œ¶‚àÇ = Œ¶
              |                                                        ‚àÇr                                            (4.135)
              |                                        Œ®(u, ¬µ) = Œò‚àÇ       (u, ¬µ)Œ¶‚àÇ ,
              |                                                        ‚àÇu
              | which will be enforced in the remainder. As with the primal ROM, the minimum-residual property is
              | desirable since it ensures the reduced sensitivity model is monotonic and interpolatory. Interpolation
              |          ‚àÇu                        ‚àÇu
              | requires     (¬µ) ‚àà col(Œ¶), where      (¬µ) is the solution of r ‚àÇ (u(¬µ), ¬∑ , ¬µ) = 0 and the requirement
              |          ‚àÇ¬µ                        ‚àÇ¬µ
              | Œ¶‚àÇ = Œ¶ has been imposed. This condition, along with the requirement for interpolation of the
              | primal solution (u(¬µ) ‚àà col(Œ¶)), will be enforced using the heterogeneous span-preserving variant
              | of POD. Define the sensitivity snapshot matrix
blank         |                                                                     
text          |                                          ‚àÇu                  ‚àÇu
              |                                      Y =    (¬µ ) ¬∑ ¬∑ ¬∑          (¬µ ),                                (4.136)
              |                                          ‚àÇ¬µ 1                ‚àÇ¬µ n
blank         | 
text          | where {¬µ1 , . . . , ¬µn } are the interpolation points previously defined, and let Y 0 be any other collection
              | of sensitivity snapshots. Then, the trial basis is defined according to
blank         | 
text          |                                       Œ¶ = PODHSP(X, X 0 , Y , Y 0 ).                                 (4.137)
blank         | 
text          | This guarantees the primal and sensitivity reduced-order models will be interpolatory if they both
              |                                                                    ‚àÇu
              | possess the minimum-residual property since u(¬µi ) ‚àà col(Œ¶) and       (¬µ ) ‚àà col(Œ¶). In Chapter 5, X
              |                                                                    ‚àÇ¬µ i
              | and Y will consist of the high-dimensional primal and sensitivity snapshots at the trust region center,
              |                  ‚àÇu
              | i.e., u(¬µk ) and    (¬µ ), since conditions (3.14) and (3.15) require a prescribed level of accuracy at
              |                  ‚àÇ¬µ k
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                                      113
blank         | 
              | 
              | 
text          | the center.
              |     The minimum-residual adjoint reduced-order model, defined in (4.56), is uniquely defined through
              | the specification of an adjoint basis Œ¶Œª and optimality metric ŒòŒª , i.e.,
              |                                                  !T                     !                    !T
              |                                         ‚àÇr T Œª            Œª    ‚àÇr T Œª               ‚àÇr T Œª             ‚àÇf T
              |         find ŒªÃÇr ‚àà R   ku
              |                             such that       Œ¶         Œò            Œ¶        ŒªÃÇr =       Œ¶         ŒòŒª          (4.138)
              |                                         ‚àÇu                     ‚àÇu                   ‚àÇu                 ‚àÇu
blank         | 
text          | where all terms are evaluated at the (reconstructed) solution of the primal reduced-order model
              | Œ¶ur (¬µ; Œ¶, Œ®). Proposition 4.5 guarantees these two choices for the reduced adjoints match when
              | the primal solution is exact or the test basis is constant, provided the relationships in (4.63) hold,
              | i.e.,
              |                                                                ‚àí1
              |                                                      ‚àÇr
              |                                         Œ¶Œª = Œ® = Œò Œª    (u, ¬µ)T     Œ¶                                         (4.139)
              |                                                      ‚àÇu
              | which will be enforced in the remainder. As with the primal ROM, the minimum-residual property
              | is desirable since it ensures the reduced adjoint model is monotonic and interpolatory. Interpolation
              | requires Œª(¬µ) ‚àà col(Œ®) where Œª(¬µ) is the solution of r Œª (u(¬µ), ¬∑ , ¬µ) = 0 and the requirement
              | Œ¶Œª = Œ® has been imposed. Due to the relationship between Œ®, Œ¶Œª , Œ¶, and ŒòŒª imposed in (4.63)
              | from Proposition 4.5, the following equivalence holds
blank         | 
text          |                                                               ‚àÇr
              |                                  Œª(¬µ) ‚àà col(Œ®) ‚áê‚áí ŒòŒª             (u(¬µ), ¬µ)T Œª(¬µ) ‚àà col(Œ¶)                     (4.140)
              |                                                               ‚àÇu
blank         | 
text          | This condition, along with the requirement for interpolation of the primal solution (u(¬µ) ‚àà col(Œ¶)),
              | will be enforced using the heterogeneous span-preserving variant of POD to construct the trial basis
              | Œ¶. Define the (modified) adjoint snapshot matrix
blank         |                                                                                              
text          |                         ‚àÇr       Œª                                     ‚àÇr
              |                                                                         Œª
              |                    Z= Œò    (u(¬µ1 ), ¬µ1 )T Œª(¬µ1 ) ¬∑ ¬∑ ¬∑               Œò                 T
              |                                                                           (u(¬µn ), ¬µn ) Œª(¬µn )                (4.141)
              |                         ‚àÇu                                             ‚àÇu
blank         | 
text          | where {¬µ1 , . . . , ¬µn } are the interpolation points previously defined, and let Z 0 be any other collection
              | of (modified) adjoint snapshots. Then, the trial basis is defined according to
blank         | 
text          |                                           Œ¶ = PODHSP(X, X 0 , Z, Z 0 ).                                       (4.142)
blank         | 
text          | This guarantees the primal and adjoint reduced-order models will be interpolatory if they both
              | possess the minimum-residual property since u(¬µi ) ‚àà col(Œ¶) and Œª(¬µi ) ‚àà col(Œ®). In Chapter 5, X
              | and Y will consist of the high-dimensional primal and adjoint snapshots at the trust region center,
              | i.e., u(¬µk ) and Œª(¬µk ), since conditions (3.14) and (3.15) require a prescribed level of accuracy at
              | the center.
              |     There may be cases where it is desirable for the reduced-order model to be monotonic and
              | interpolatory in the primal, sensitivity, and adjoint states. The logical extension of the previous de-
              | velopment employs a minimum-residual reduced-order model for the primal, sensitivity, and adjoint
meta          | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                      114
blank         | 
              | 
              | 
text          | and defines the trial basis according to
blank         | 
text          |                                 Œ¶ = PODHSP(X, X 0 , Y , Y 0 , Z, Z 0 )                       (4.143)
blank         | 
text          | where the above is the obvious extension in Algorithm 7 to three types of snapshots.
              |    Before closing this section, the abstract discussion regarding minimum-residual reduced-order
              | models is made concrete by considering the special case of a Galerkin and LSPG projection. Reduced-
              | order models based on a Galerkin projection take the test basis to be the same as the trial basis
blank         | 
text          |                                               Œ®(u, ¬µ) = Œ¶
blank         | 
text          | for any u ‚àà RNu and ¬µ ‚àà RN¬µ , rendering the test basis constant and immediately qualifying such
              | reduced-order models for the results of Propositions 4.3 and 4.5. Galerkin reduced-order models
              | possess the minimum-residual property in the metric
blank         | 
text          |                                            ‚àÇr
              |                                   Œò=          (Œ¶ur (¬µ; Œ¶, Œ¶), ¬µ)‚àíT ,                         (4.144)
              |                                            ‚àÇu
blank         | 
text          | provided the PDE Jacobian is symmetric, positive definite. Given this relation between test and
              | trial basis and requirement that the PDE Jacobian is SPD, the minimum-residual sensitivity and
              | adjoint reduced-order models follow from the choices
blank         | 
text          |                                                         ‚àÇr
              |                      Œ¶‚àÇ = Œ¶Œª = Œ¶           Œò‚àÇ = Œò Œª =      (Œ¶ur (¬µ; Œ¶, Œ¶), ¬µ)‚àíT              (4.145)
              |                                                         ‚àÇu
blank         | 
text          | This relations also ensure (4.35) and (4.63) of Propositions 4.3 and 4.5 are satisfied, which implies
              | the true Galerkin sensitivities and adjoint match the minimum-residual counterparts. In contrast,
              | reduced-order models based on the Least-Squares Petrov-Galerkin projection take the test basis
              | according to
              |                                                      ‚àÇr
              |                                        Œ®(u, ¬µ) =        (u, ¬µ)Œ¶                              (4.146)
              |                                                      ‚àÇu
              | for any u ‚àà RNu and ¬µ ‚àà RN¬µ , resulting in a non-constant test basis and the results of Proposi-
              | tions 4.3 and 4.5 will only hold when the primal solution of the reduced-order model is exact, i.e.,
              | u(¬µ) = Œ¶ur (¬µ; Œ¶, Œ®). The LSPG reduced-order model possesses the minimum-residual property
              | by construction in the metric Œò = I and therefore applies in the most general case, i.e., without
              | requiring SPD Jacobians. Given this relationship between the test and trial basis and the following
              | requirements from Propositions 4.3 and 4.5
blank         | 
text          |                                        Œ¶‚àÇ = Œ¶         Œ¶Œª = Œ®,                                (4.147)
blank         | 
meta          | the minimum-residual sensitivity and adjoint reduced-order models for the LSPG projection follow
              | CHAPTER 4. PROJECTION-BASED MODEL REDUCTION                                                       115
blank         | 
              | 
              | 
text          | from the choices                               "        #‚àí1
              |                                              ‚àÇr T ‚àÇr
              |                            Œò‚àÇ = I       ŒòŒª =                                   .              (4.148)
              |                                              ‚àÇu ‚àÇu
              |                                                           (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)
blank         | 
text          | The above relationship satisfy all conditions in Propositions 4.3 and 4.5, thereby ensuring the true
              | and minimum-residual sensitivities and adjoints agree when the primal solution is exact.
              |    Most of the developments detailed in this section extend to the case where collocation-based
              | hyperreduced models are used in place of the pure projection-based reduced-order models. In par-
              | ticular, the relationship between the various bases and optimality metrics, when imposed only on
              | the hyperreduction mask, lead to a weaker form of the minimum-residual property, i.e., the masked
              | minimum-residual property (Definition 4.2). This ultimately leads to a weaker form of monotonicity
              | and interpolation that only holds under stricter assumptions on solutions of the discrete PDE. In this
              | work, the mask P is constructed solely from the primal reduced-order basis Œ¶ and problem-specific
              | information following the approach in [198].
title         | Chapter 5
blank         | 
title         | Optimization via Model Reduction
              | and Residual-Based Trust Regions
blank         | 
text          | With the globally convergent, multifidelity trust region method introduced in Chapter 3 and projection-
              | based reduced-order models introduced in Chapter 4, these technologies are combined to yield an
              | efficient algorithm for deterministic PDE-constrained optimization. The approximation model will
              | be taken as the quantity of interest evaluated at the reconstructed reduced-order model solution and
              | residual-based error bounds (Appendix B) will define the objective and gradient error bounds that
              | are required for global convergence of the multifidelity trust region method. In addition to exploiting
              | inexpensive reduced-order (hyperreduced) models in the trust region subproblem, the flexible mul-
              | tifidelity trust region framework of Section 3.1.1 allows for several other opportunities for efficiency.
              | First, the objective accuracy condition (3.14), restated here for convenience,
blank         | 
text          |                                    œëk (¬µk ) ‚â§ Œ∫œë ‚àÜk      Œ∫œë ‚àà (0, 1),
blank         | 
text          | implies the reduced-order model does not need to be exact at the trust region centers. This is
              | exploited by using partially converged solutions to build the reduced-order basis. Similarly, the
              | gradient accuracy condition (3.15)
blank         | 
text          |                            œïk (¬µk ) ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }         Œ∫œï > 0
blank         | 
text          | allows for the use of partially converged sensitivity or adjoint snapshots. Partially converged pri-
              | mal and dual solutions can substantially reduce the burden of collecting snapshots, particularly in
              | large-scale applications encountered in computational fluid dynamics that require slowly converging
              | nonlinear solvers such as pseudo-transient continuation [104, 105] for robust convergence behavior.
              | Partially converged primal solutions are also used to efficiently evaluate the performance of a trust
              | region subproblem using the concepts outlined in Section 3.1.1. Sections 5.2 and 5.3 detail the use
blank         | 
              | 
              | 
meta          |                                                   116
              | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      117
blank         | 
              | 
              | 
text          | of partially converged solutions for these purposes. While most of this chapter focuses on approxi-
              | mation models based on projection-based reduced-order models without hyperreduction, Section 5.4
              | discusses the extension to collocation-based hyperreduced models. Finally, Section 5.5 provides sev-
              | eral numerical examples from various computational mechanics disciplines, including the large-scale
              | industrial demonstration of shape optimization of a full aircraft configuration, to study the proposed
              | approach.
blank         | 
              | 
title         | 5.1     Residual-Based Trust Region Method
text          | Consider the fully discrete partial differential equation r(u, ¬µ) = 0, where u ‚àà RNu is the state
              | vector and ¬µ ‚àà RN¬µ are the design or control parameters and f : RNu √ó RN¬µ ‚Üí R is a quantity of
              | interest to be optimized. The reduced-space approach to PDE-constrained optimization (Section 2.3)
              | considers the optimization problem
              |                                            minimize F (¬µ),                                          (5.1)
              |                                              ¬µ‚ààRN¬µ
blank         | 
text          | where F (¬µ) = f (u(¬µ), ¬µ) and u(¬µ) is the solution of r( ¬∑ , ¬µ) = 0. Due to the large expense associ-
              | ated with the evaluation of F (¬µ) and ‚àáF (¬µ), the multifidelity trust region method and projection-
              | based model reduction techniques are combined to efficiently solve (5.1). The multifidelity trust
              | region method of Chapter 3 was completely specified in terms of the approximation model mk (¬µ),
              | the objective error indicator œëk (¬µ) that satisfies (3.12), the gradient error indicator œïk (¬µ) that sat-
              | isfies (3.13), and the inexact objective model œàk (¬µ) and error indicator Œ∏k (¬µ) that satisfy (3.21).
              | Therefore the focus of this section is the specification of these functions using projection-based
              | reduced-order (hyperreduced) models and error indicators from Chapter 4 and Appendix B, respec-
              | tively. From the overview of the multifidelity trust region method provided in Section 3.1.1, there
              | are two other critical pieces required to fully prescribe the method such that global convergence is
              | guaranteed: a trust region subproblem solver that ensures the fraction of Cauchy decrease (A.9)
              | is obtained and a refinement mechanism for mk (¬µ), œàk (¬µ) and the associated error indicators to
              | ensure the error conditions (3.14), (3.15), (3.22) are met. Due to the significant cost separation
              | between reduced-order (hyperreduced) models and the high-dimensional model, the trust region
              | subproblem is solved exactly to guarantee the FCD is satisfied. The error conditions will be met
              | through construction of the reduced-order basis, which will be detailed in Section 5.1.2.
blank         | 
              | 
title         | 5.1.1      Multifidelity Trust Region Ingredients
text          | At the kth iteration, the approximation model based on projection-based reduced-order models takes
              | the form
              |                                   mk (¬µ) = f (Œ¶k ur (¬µ; Œ¶k , Œ®k ), ¬µ),                              (5.2)
blank         | 
text          | where Œ¶k is the reduced-order basis used at the kth iteration of the trust region method‚Äîdetails
              | pertaining to the construction of Œ¶k will be deferred to Section 5.1.2 as they will be intimately
              | linked to the error conditions in (3.14), (3.15) and therefore the global convergence theory‚Äîand
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     118
blank         | 
              | 
              | 
text          | ur (¬µ; Œ¶k , Œ®k ) is the solution of the reduced-order model
blank         | 
text          |                                           Œ®Tk r(Œ¶k ur , ¬µ) = 0
blank         | 
text          | with test basis Œ®k . The test basis is chosen according to (4.14) to ensure the reduced-order model
              | possesses the minimum-residual property, which in turn ensures it is monotonic and interpolatory.
              | The gradient of the model in (5.2) will be computed using the exact reduced sensitivity or adjoint
              | method if the test basis is constant. This ensures the gradients can be computed without requiring
              | second derivatives of r and will be consistent with the corresponding function. The minimum-
              | residual variants will be used if the test basis is not constant; however, the requirements between
              | the sensitivity/adjoint basis and optimality metric in (4.35) and (4.63) will be enforced to ensure
              | the minimum-residual gradients match the true gradients at any point where the primal reduced-
              | order model solution is exact. In Section 5.1.2, the reduced-order basis Œ¶k will be constructed such
              | that the primal and sensitivity/adjoint reduced-order model is exact at the trust region center ¬µk .
              | Following this discussion, the construction of the trial basis Œ¶k and selection of minimum-residual
              | optimality metrics is sufficient to completely define the remaining ingredients of the reduced-order
              | model, i.e., the test basis Œ®k , sensitivity basis Œ¶‚àÇk = Œ¶k , and adjoint basis Œ¶Œª
              |                                                                                  k = Œ®k .
              |    There are two natural choices for the trust region constraint function. The first is the standard
              | Euclidean distance
              |                                           œëk (¬µ) := k¬µ ‚àí ¬µk k
blank         | 
text          | which leads to a traditional trust region algorithm and recovers a method similar to the original Trust
              | Region Proper Orthogonal Decomposition (TRPOD) method [10]. As discussed in Section 3.1.1, this
              | choice automatically satisfies requirements in (3.12) and (3.14), provided a gradient error indicator
              | œïk (¬µ) is chosen that satisfies (3.13) and (3.15). Another choice for the trust region constraint that
              | was proposed in the author‚Äôs previous research [210], and earlier in [208] in the context of linear
              | PDEs, is the norm of the residual evaluated at the reconstructed ROM solution
blank         | 
text          |              œëk (¬µ) := kr(Œ¶k ur (¬µk ; Œ¶k , Œ®k ), ¬µk )kŒòk + kr(Œ¶k ur (¬µ; Œ¶k , Œ®k ), ¬µ)kŒòk ,         (5.3)
blank         | 
text          | which leads to an error-aware trust region. With this choice of œëk (¬µ), the bound in (3.12) is verified
              | as follows
blank         | 
text          |              |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ |F (¬µk ) ‚àí mk (¬µk )| + |F (¬µ) ‚àí mk (¬µ)|
              |                                                   ‚â§ Œ∂ (kr(u, ¬µk )k + kr(u, ¬µ)k)
blank         |                                                                                       
text          |                                                   ‚â§ Œ∂ÃÇ kr(u, ¬µk )kŒòk + kr(u, ¬µ)kŒòk
              |                                                   = Œ∂ÃÇœëk (¬µ),
blank         | 
text          |                                                                                    ‚àí1/2
              | where u = Œ¶k ur (¬µk ; Œ¶k , Œ®k ), Œ∂ > 0 is an arbitrary constant, and Œ∂ÃÇ = Œòk              Œ∂ is a related
              | constant. The second inequality uses Lemma B.4 that bounds errors in quantities of interest by
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                              119
blank         | 
              | 
              | 
text          |                                                                                           ‚àí1/2
              | the primal residual norm and the third inequality invokes the identity kxk = Œòk                  x        and the
              |                                                                                                      Œòk
              | triangle inequality (or simply norm equivalence). The function used for the gradient error indicator
              | is also a residual-based quantity, but the specific form depends on whether the sensitivity or adjoint
              | method is employed in the gradient computation.
blank         | 
text          | Remark. Some of the optimality metrics‚Äî Œò, Œò‚àÇ , ŒòŒª ‚Äî introduced in this document are parameter-
              | dependent. This is not an issue in Chapter 4 since the residual minimization problem was only posed
              | over the state space for a fixed parameter. In the context of PDE-constrained optimization, the met-
              | ric must be valid over the entire state and parameter space. Therefore, all parameter-dependent
              | metrics are fixed at the trust region center ¬µk . For a Galerkin projection, the optimality metrics
              | become
              |                                      ‚àÇr
              |                                Œòk =     (Œ¶k ur (¬µk ; Œ¶k , Œ¶k ), ¬µk )‚àíT
              |                                      ‚àÇu
              |                                      ‚àÇr
              |                                Œò‚àÇk =    (Œ¶k ur (¬µk ; Œ¶k , Œ¶k ), ¬µk )‚àíT
              |                                      ‚àÇu
              |                                      ‚àÇr
              |                                ŒòŒª
              |                                 k =     (Œ¶k ur (¬µk ; Œ¶k , Œ¶k ), ¬µk )‚àíT .
              |                                      ‚àÇu
              | For a LSPG projection, the optimality metrics become
blank         | 
text          |                                Œòk = I
              |                                Œò‚àÇk = I
              |                                      "             #‚àí1
              |                                     ‚àÇr T ‚àÇr
              |                                ŒòŒª
              |                                 k =                                                   .
              |                                     ‚àÇu ‚àÇu
              |                                                        (Œ¶k ur (¬µk ; Œ¶k , Œ®k ), ¬µk )
blank         | 
              | 
text          |    In this work, the minimum-residual sensitivity and adjoint models are used to compute (approx-
              | imate) gradients of the projection-based reduced-order model that comprises the approximation
              | model. For the sensitivity method, the approximate gradient, denoted ‚àám
              |                                                                      [k (¬µ), is computed ac-
              | cording to                                                                   !
              |                                                        ‚àÇu
              |                                                        dr
              |                           ‚àám
              |                           [k (¬µ) = g    ‚àÇ
              |                                             u,   Œ¶‚àÇk           ‚àÇ    ‚àÇ
              |                                                           (¬µ; Œ¶k , Œòk , u), ¬µ ,                             (5.4)
              |                                                        ‚àÇ¬µ
blank         | 
text          |        ‚àÇu
              |        dr
              | where       is the solution of the minimum-residual sensitivity reduced-order model in (4.28). For
              |        ‚àÇ¬µ
              | the adjoint method, the approximate gradient is computed according to
blank         | 
text          |                             [k (¬µ) = g Œª (u, Œ¶Œª
              |                             ‚àám                         Œª    Œª
              |                                               k ŒªÃÇ(¬µ; Œ¶k , Œòk , u), ¬µ).                                     (5.5)
blank         | 
text          | where ŒªÃÇr is the solution of the minimum-residual adjoint reduced-order model in (4.56). In both of
              | the above expressions, u = Œ¶k ur (¬µ; Œ¶k , Œ®k ) is the solution of the primal reduced-order model. The
              | relationships in (4.35) and (4.63) between Œ¶, Œ®, Œ¶‚àÇ , Œ¶Œª , Œò‚àÇk , and ŒòŒª
              |                                                                       k are employed to guarantee,
              | by Propositions 4.3 and 4.5, that ‚àám[k (¬µ) = ‚àámk (¬µ) whenever Œ®k is constant, i.e., a Galerkin
              | projection, or the primal solution is exact. Unfortunately, unless one of these criteria is satisfied
              | ‚àám
              | [k (¬µ) 6= ‚àámk (¬µ), which may cause convergence issues for the trust region subproblem. To ensure
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                   120
blank         | 
              | 
              | 
text          | these subproblems terminate, a maximum number of iterations is imposed on the solver, which may
              | slightly slow convergence of the overall trust region method. Once the optimization algorithm reaches
              | the vicinity of a local minima (of F (¬µ)), the reduced-order model is sufficiently accurate at (and
              | near) the new trust region center and the primal reduced-order model solution will be sufficiently
              | accurate that the two gradients closely match.
              |    For sensitivity-based gradient computations, the gradient error indicator is
              |                                                                                         !
              |                                                                   ‚àÇu
              |                                                                   dr
              |               œïk (¬µ) := Œ±1 kr(u, ¬µ)kŒòk + Œ±2 r     ‚àÇ
              |                                                        u,   Œ¶‚àÇk           ‚àÇ    ‚àÇ
              |                                                                      (¬µ; Œ¶k , Œòk , u), ¬µ         (5.6)
              |                                                                   ‚àÇ¬µ
              |                                                                                             Œò‚àÇ
              |                                                                                              k
blank         | 
              | 
              | 
text          |                                                                       ‚àÇu
              |                                                                       dr
              | where u = Œ¶k ur (¬µ; Œ¶k , Œ®k ) is the reconstructed primal solution and     is the solution of the
              |                                                                       ‚àÇ¬µ
              | minimum-residual reduced sensitivity equations (4.28). The primal trial basis Œ¶k is also used as
              | the sensitivity basis Œ¶‚àÇk to ensure the minimum-residual sensitivities agree with the true reduced-
              | order model sensitivities when the primal solution is exact or Œ®k is constant (Proposition 4.3). For
              | adjoint-based gradient computations, the gradient error indicator is
blank         |                                                                              
text          |                œïk (¬µ) := Œ±1 kr(u, ¬µ)kŒòk + Œ±2 r Œª u, Œ¶Œª          Œª    Œª
              |                                                      k ŒªÃÇr (¬µ; Œ¶k , Œòk , u), ¬µ                   (5.7)
              |                                                                                             ŒòŒª
              |                                                                                              k
blank         | 
              | 
              | 
text          | where u = Œ¶k ur (¬µ; Œ¶k , Œ®k ) is the reconstructed primal solution and ŒªÃÇr is the solution of the
              | minimum-residual reduced adjoint equations (4.56). The primal test basis Œ®k is used as the adjoint
              | basis Œ¶Œª
              |        k to ensure the minimum-residual adjoints agree with the true reduced-order model adjoints
              | when the primal solution is exact or Œ®k is constant (Proposition 4.5). In (5.6) and (5.7), Œ±1 , Œ±2 > 0
              | are user-defined constants intended to balance the contribution of the primal and dual residuals.
              | From Lemma B.7 and B.8, there exists a constant Œæ > 0 such that
blank         | 
text          |                                   ‚àáF (¬µk ) ‚àí ‚àám
              |                                              [k (¬µk ) ‚â§ Œæœïk (¬µk ),                               (5.8)
blank         | 
text          | holds regardless of the values of Œ±1 and Œ±2 (provided they are positive) for both the sensitivity and
              | adjoint form of the gradient error indicator. An error bound of this form is a critical ingredient in
              | the global convergence theory of the proposed trust region method, as well as in related methods
              | [93, 108].
blank         | 
text          | Remark. The objective decrease condition (3.14) introduced in the proposed generalized trust region
              | method is considerably weaker than the conditions required for previous methods. The work by
              | Alexander introduced a trust region framework to manage the use of general approximation models to
              | solve constrained and unconstrained optimization problems with expensive optimization functionals
              | [4, 6, 5]. The trust region model management framework required the approximation model possess
              | first-order consistency at trust region centers
blank         | 
text          |                              mk (¬µk ) = F (¬µk )       ‚àámk (¬µk ) = ‚àáF (¬µk ).                      (5.9)
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      121
blank         | 
              | 
              | 
text          | The Trust Region Proper Orthogonal Decomposition (TRPOD) method introduced in [10] and studied
              | extensively thereafter [57, 170, 186], removed the zeroth-order condition entirely and weakened the
              | first-order condition by replacing it with the Carter condition (see discussion to follow). Unlike the
              | present work, TRPOD strictly employed a tradition trust region constraint of the form k¬µ ‚àí ¬µk k ‚â§
              | ‚àÜk . The work in [208] generalized the TRPOD method to an error-aware trust region that required
              | a pointwise error bound on the objective at the trust region center (3.8), which is a considerably
              | stronger requirement than the objective decrease condition (3.14), as discussed in Chapter 3.
blank         | 
text          | Remark. The gradient condition (3.15) leveraged in the proposed generalized trust region method
              | was originally proposed in [93] and extensively used in [108, 109, 92, 166]. It is substantially more
              | flexible than the Carter condition [35]
blank         | 
text          |                         k‚àáF (¬µk ) ‚àí ‚àámk (¬µk )k ‚â§ Œ∑ k‚àámk (¬µk )k             Œ∑ ‚àà (0, 1)              (5.10)
blank         | 
text          | that was used in the original TRPOD method [10] and a related method proposed that uses generalized
              | trust regions [208]. Global convergence is predicated on construction of a model that satisfies this
              | bound with any value of Œ∑ that satisfies 0 < Œ∑ < 1. Since global convergence relies critically on
              | value of Œ∑ being in this range, it does not permit the use of error indicators since they are only
              | bounds when multiplied by an arbitrary constant. Therefore, ‚àáF (¬µk ) must be computed along with
              | ‚àámk (¬µk ) corresponding to an increasingly refined basis until (5.10) is met.
blank         | 
text          |    An opportunity for efficiency afforded by the flexible trust region framework introduced in Sec-
              | tion 3.1.1 is the use of an approximation model to compute the ratio of actual-to-predicted ratio,
              | œÅk . The true expression for œÅk can be replaced with
blank         | 
text          |                                                   œà(¬µk ) ‚àí œà(¬µÃÇk )
              |                                           œÅk =                                                     (5.11)
              |                                                  mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
text          | where œàk : RN¬µ ‚Üí R is an approximation model that satisfies
blank         | 
text          |                 |F (¬µk ) ‚àí F (¬µÃÇk ) + œàk (¬µÃÇk ) ‚àí œàk (¬µk )| ‚â§ œÉŒ∏k (¬µÃÇk )
              |                                                                                                    (5.12)
              |                                                   Œ∏kœâ (¬µÃÇk ) ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk },
blank         | 
text          | without destroying global convergence of the overall algorithm. In (5.12), œÉ > 0 is an arbitrary
              | constant, rk ‚Üí 0 is a forcing sequence, Œ∑ < min{Œ∑1 , 1 ‚àí Œ∑2 }, and 0 < Œ∑1 < Œ∑2 < 1 and œâ ‚àà (0, 1)
              | are algorithmic constant. The error bound in (5.12) is identical to the required relationship between
              | mk (¬µ) and œëk (¬µ) in (3.12). Thus, a natural and efficient choice is
blank         | 
text          |                               œàk (¬µ) = mk (¬µ)         and        Œ∏k (¬µ) = œëk (¬µ),                  (5.13)
blank         | 
text          | which implies
              |                                   œà(¬µk ) ‚àí œà(¬µÃÇk )      mk (¬µk ) ‚àí mk (¬µÃÇk )
              |                           œÅk =                        =                      = 1.                  (5.14)
              |                                  mk (¬µk ) ‚àí mk (¬µÃÇk )   mk (¬µk ) ‚àí mk (¬µÃÇk )
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      122
blank         | 
              | 
              | 
title         | Therefore, if the error condition
blank         | 
text          |                           œëœâ           œâ
              |                            k (¬µÃÇk ) = Œ∏k (¬µÃÇk ) ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }                (5.15)
blank         | 
text          | is satisfied for the reduced-order basis used during the kth iteration, the step can automatically
              | be accepted and trust region radius increased without referring to the high-dimensional model. If
              | this condition is not satisfied, the exact expression for ratio of actual-to-predicted reduction (3.9) is
              | used, i.e., œàk (¬µ) = F (¬µ) and Œ∏k (¬µ) = 0. Section 5.3 introduces another choice for œàk and Œ∏k that
              | leverages partially converged solutions for enhanced efficiency.
              |     The choice of objective and gradient error indicators in (5.3) and (5.6), (5.7) provides a strong
              | connection to the minimum-residual theory of Chapter 4 since the norms are taken to exactly coincide
              | with the optimality metrics defining the minimum-residual reduced-order model. As a result, the
              | optimality property and monotonicity hold (Propositions 4.1, 4.2, 4.4). Optimality implies that
blank         | 
text          |                                       œëk (¬µk ) ‚â§ 2 kr(Œ¶k x, ¬µk )kŒòk                               (5.16)
blank         | 
text          | for any x ‚àà Rku . A similar statement holds for œïk (¬µ)
blank         |                                                                                   
text          |           œïk (¬µk ) ‚â§ Œ±1 kr(Œ¶k y, ¬µk )kŒòk + Œ±2 r ‚àÇ Œ¶k ur (¬µk ; Œ¶k , Œ®k ), Œ¶‚àÇk w, ¬µk
              |                                                                                        Œò‚àÇ
              |                                                                                         k
              |                                                                                                 (5.17)
              |           œïk (¬µk ) ‚â§ Œ±1 kr(Œ¶k y, ¬µk )kŒòk + Œ±2 r Œª Œ¶k ur (¬µk ; Œ¶k , Œ®k ), Œ¶Œªk z, ¬µk
              |                                                                                        ŒòŒª
              |                                                                                         k
blank         | 
              | 
              | 
text          | for any y, z ‚àà Rku and w ‚àà Rku √óN¬µ . Notice that this bound requires the sensitivity and adjoint
              | residual to be defined (linearized) about the primal reduced-order model solution, i.e., Œ¶k ukr (¬µ).
              | Monotonicity means that hierarchically refining Œ¶k can only reduce œë(¬µ), provided Œò is independent
              | of Œ¶k . The same statement does not hold for œïk (¬µ) since monotonicity, as defined in Proposition 4.2,
              | 4.4 requires linearization about a fixed primal solution and hierarchically refining either Œ¶‚àÇk or Œ¶Œª
              |                                                                                                     k.
              | Given the relation between Œ¶k , Œ¶‚àÇk , and Œ¶Œª                                                   ‚àÇ
              |                                            k in (4.35) and (4.63), it is impossible to refine Œ¶k or
              | Œ¶Œª
              |  k without also modifying Œ¶k and therefore changing the primal reduced-order model solution (the
              | linearization point). For these reasons, the choice of œëk (¬µ) and œïk (¬µ) in (5.3) and (5.6), (5.7) are
              | highly desirable. On the other hand, these norms may be difficult to compute if the metric requires
              | computation of the Jacobian of r or its inverse, which will be the case for Galerkin reduced-order
              | models. In such cases, it is desirable to simply use the I-norm to define all terms in œëk (¬µ) and
              | œïk (¬µ), i.e.,
blank         | 
text          |    œëk (¬µ) := kr(Œ¶k ur (¬µk ; Œ¶k , Œ®k ), ¬µk )k + kr(Œ¶k ur (¬µ; Œ¶k , Œ®k ), ¬µ)k
blank         |                                                                                               
text          |                                                     ‚àÇ                      ‚àÇur
              |    œïk (¬µ) := Œ±1 kr(Œ¶k ur (¬µ; Œ¶k , Œ®k ), ¬µ)k + Œ±2 r Œ¶k ur (¬µ; Œ¶k , Œ®), Œ¶k       (¬µ; Œ¶k , Œ®k ), ¬µ
              |                                                                            ‚àÇ¬µ
              |    œïk (¬µ) := Œ±1 kr(Œ¶k ur (¬µ; Œ¶k , Œ®k ), ¬µ)k + Œ±2 r Œª (Œ¶k ur (¬µ; Œ¶k , Œ®k ), Œ®k Œªr (¬µ; Œ¶k , Œ®k ), ¬µ)
              |                                                                                                   (5.18)
              | since the norms are trivial to evaluate given the corresponding residual. Fortunately for the case
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      123
blank         | 
              | 
              | 
text          | of LSPG-based reduced-order models, the primal and sensitivity optimality metrics are taken as
              | identity (Sections 4.1.1 and 4.1.2) and the definitions of œëk (¬µ) and œïk (¬µ) in (5.3)-(5.7) and (5.18)
              | agree. However, even when LSPG is used, the adjoint optimality metric is not the identity matrix
              | (Section 4.1.3). Furthermore, note that the interpolation property of minimum-residual reduced-
              | order models does not depend on the metric used in the residual norm, due to the equivalence
              | of norms in finite dimensions, i.e., for the definition of œëk (¬µ) in (5.3) or (5.18), œëk (¬µ) = 0 if
              | u(¬µ) ‚àà col(Œ¶k ), and similarly for œïk (¬µ). This implies that the choice of œëk (¬µ) and œïk (¬µ) still
              | possesses this critical property that will be used in the next section. Finally, due to the equivalence
              | of norms in finite dimensions, the bounds in (3.12) and (3.13) will still hold (with different constants)
              | and therefore the choice of œëk (¬µ) and œïk (¬µ) in (5.18) will not destroy global convergence.
blank         | 
              | 
title         | 5.1.2    Basis Construction via Proper Orthogonal Decomposition and the
text          |          Method of Snapshots
              | The use of reduced-order models in the context of optimization has predominantly employed an
              | offline-online procedure [17, 149, 173] where expensive operations involving the HDM are performed
              | in the offline phase to build the reduced-order basis Œ¶, i.e., train the reduced-order model, and
              | the inexpensive reduced-order model is employed in the online optimization phase. A number of
              | drawbacks to this approach exist, the most critical ones being that global convergence can only
              | be established for relatively simple partial differential equations and it is difficult to train a robust
              | ROM in a high-dimensional parameter space. TRPOD [10] was among the first methods to break
              | the offline-online barrier and guarantee global convergence in a general setting. In TRPOD and the
              | many variants to follow [57, 1, 186], the reduced-order basis is constructed during the optimization
              | procedure such that conditions on the objective and gradient accuracy at trust region centers are
              | met, thereby avoiding the issue of sampling in possibly high-dimensional parameter spaces. This is
              | also the approach taken here.
              |    For the remainder of this chapter, only the residual-based constraint function is considered. From
              | the previous section, the choice of the reduced-order model approximation mk (¬µ) and residual-based
              | error indicators œëk (¬µ) and œïk (¬µ) satisfy the error bounds in (3.12), (3.13), regardless of the choice
              | of reduced-order basis. However, the accuracy criterion in (3.14) and (3.15)
blank         | 
text          |                                  œëk (¬µk ) ‚â§ Œ∫œë ‚àÜk
              |                                                                                                   (5.19)
              |                                  œïk (¬µk ) ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }
blank         | 
text          | depend critically on the choice of reduced-order basis.
              |    At each iteration k, the reduced-order bases Œ¶k , Œ¶‚àÇk , Œ¶Œª
              |                                                             k are constructed to ensure
blank         | 
text          |                                           ‚àÇu
              |                    u(¬µk ) ‚àà col(Œ¶k )         (¬µ ) ‚àà col(Œ¶‚àÇk )     Œª(¬µk ) ‚àà col(Œ¶Œª
              |                                                                                 k ).              (5.20)
              |                                           ‚àÇ¬µ k
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                          124
blank         | 
              | 
              | 
text          | The interpolation property of minimum-residual reduced-order models ensures the reconstructed so-
              | lutions exactly recover the high-dimensional counterparts. In turn, this ensures œëk (¬µk ) = œïk (¬µk ) = 0
              | and therefore the error conditions (3.14), (3.15) are trivially satisfied and global convergence is
              | guaranteed. As discussed in the previous section, the sensitivity and adjoint bases are chosen in
              | accordance with Proposition 4.3 and 4.5, i.e., Œ¶‚àÇk = Œ¶k and Œ¶Œª
              |                                                              k = Œ®k , and the requirements in
              | (5.20) reduce to
blank         | 
text          |                                            ‚àÇu
              |                      u(¬µk ) ‚àà col(Œ¶k )        (¬µ ) ‚àà col(Œ¶k )        Œª(¬µk ) ‚àà col(Œ®k ).               (5.21)
              |                                            ‚àÇ¬µ k
blank         | 
text          | The condition between the test basis Œ®k and adjoint optimality metric ŒòŒª required in Proposition 4.5
              | reduces (5.21) to
blank         | 
text          |                           ‚àÇu                                         ‚àÇr
              |    u(¬µk ) ‚àà col(Œ¶k )         (¬µ ) ‚àà col(Œ¶k )      ŒòŒª (u(¬µk ), ¬µk )      (u(¬µk ), ¬µk )T Œª(¬µk ) ‚àà col(Œ¶k ).
              |                           ‚àÇ¬µ k                                       ‚àÇu
              |                                                                                                      (5.22)
blank         | 
text          | Remark. In the case of a Galerkin projection (for problems with SPD Jacobians) with adjoint
              |                        ‚àÇr
              | optimality metric ŒòŒª =    (Œ¶ur (¬µ; Œ¶, Œ¶))‚àíT , the adjoint snapshots reduce to
              |                        ‚àÇu
blank         | 
text          |                                               ‚àÇr
              |                                ŒòŒª (u(¬µ), ¬µ)      (u(¬µ), ¬µ)T Œª(¬µ) = Œª(¬µ).
              |                                               ‚àÇu
              |                                                                                "          #‚àí1
              |                                                                     ‚àÇr T ‚àÇr
              |                                                                           Œª
              | In the case of a LSPG projection with adjoint optimality metric Œò =                                             ,
              |                                                                     ‚àÇu ‚àÇu
              |                                                                                            (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)
              | the adjoint snapshots reduce to
blank         | 
text          |                                       ‚àÇr                   ‚àÇr
              |                        ŒòŒª (u(¬µ), ¬µ)      (u(¬µ), ¬µ)T Œª(¬µ) =    (u(¬µ), ¬µ)‚àí1 Œª(¬µ).
              |                                       ‚àÇu                   ‚àÇu
blank         | 
text          |     The above requirements reveal the nature of the snapshots that should be used in the construction
              | of the trial basis Œ¶k . In practice, sensitivities and adjoints are rarely required simultaneously.
              | Usually the sensitivity method is employed when the number of constraints is larger than the number
              | of optimization variables and vice versa for the adjoint method. To generalize the notation such
              | that the sensitivity method and adjoint method can be considered simultaneously, define v(¬µ) as the
              | sensitivity or adjoint state, depending on which method is used to compute gradients of quantities
              | of interest, i.e.,                     Ô£±
              |                                        Ô£≤ ‚àÇu (¬µ) sensitivity method
              |                                        Ô£¥
              |                                  v(¬µ) = ‚àÇ¬µ                                                            (5.23)
              |                                        Ô£≥Œª(¬µ)
              |                                        Ô£¥
              |                                                 adjoint method
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     125
blank         | 
              | 
              | 
text          | and let vÃÇ(¬µ) denote the corresponding snapshot, i.e.,
blank         | 
text          |                              ‚àÇu
              |                            Ô£±
              |                            Ô£¥
              |                            Ô£≤    (¬µ)                         sensitivity method
              |                     vÃÇ(¬µ) = ‚àÇ¬µ                                                                   (5.24)
              |                            Ô£≥ŒòŒª (u(¬µ), ¬µ) ‚àÇr (u(¬µ), ¬µ)T Œª(¬µ) adjoint method.
              |                            Ô£¥
              |                                          ‚àÇu
blank         | 
text          | With this notation, the requirements in (5.22) are weakened to
blank         | 
text          |                                   u(¬µk ) ‚àà col(Œ¶k )     vÃÇ(¬µk ) ‚àà col(Œ¶k )                       (5.25)
blank         | 
text          | while still guaranteeing œëk (¬µk ) = œïk (¬µk ) = 0 where it is understood that œïk (¬µ) corresponds to (5.6)
              | if the sensitivity method is employed and (5.7) for the adjoint method. The conditions in (3.14)
              | and (3.15) will be guaranteed using the heterogeneous span-preserving variant of POD (Section 4.3).
              | Define snapshot matrices at iteration k consisting of u(¬µ) and vÃÇ(¬µ) at the trust region centers of
              | all previous iterations, i.e.,             h                       i
              |                                       Uk = u(¬µ0 ) ¬∑ ¬∑ ¬∑ u(¬µk‚àí1 )
              |                                            h                       i                             (5.26)
              |                                       VÃÇk = vÃÇ(¬µ0 ) ¬∑ ¬∑ ¬∑ vÃÇ(¬µk‚àí1 ) .
blank         | 
text          | and define the reduced-order basis as
blank         | 
text          |                                   Œ¶k = PODHSP(u(¬µk ), Uk , vÃÇ(¬µk ), VÃÇk ).                       (5.27)
blank         | 
text          | where PODHSP is defined in Algorithm 7. By construction, the conditions in (3.14) and (3.15) are
              | satisfied since u(¬µk ) and vÃÇ(¬µk ) are preserved in the columnspace of Œ¶k , which implies œëk (¬µk ) =
              | œïk (¬µk ) = 0 and global convergence is guaranteed. Even though the in the information in Uk and
              | Vk is not necessarily useful in satisfying the trust region error conditions, i.e., the information in
              | u(¬µk ) and v(¬µk ) is sufficient to do so, it provides the reduced-order model with additional fidelity,
              | which is useful in improving its robustness away from ¬µk .
blank         | 
text          | Remark. There may be instances where the reduced-order basis defined at iteration k‚àí1 is sufficient
              | to satisfy the error conditions (3.14), (3.15) at iteration k, i.e.,
blank         | 
text          |                                  œëk‚àí1 (¬µk ) ‚â§ Œ∫œë ‚àÜk
              |                                                                                                  (5.28)
              |                                  œïk‚àí1 (¬µk ) ‚â§ Œ∫œï min{k‚àámk‚àí1 (¬µk )k , ‚àÜk }.
blank         | 
text          | This is likely to occur when the initial trust region radius ‚àÜ0 is chosen too small. In this situation,
              | there is no need to update the reduced-order basis and the same model and error indicators are used,
              | i.e.,
              |                    mk (¬µ) := mk‚àí1 (¬µ)       œëk (¬µ) := œëk‚àí1 (¬µ)      œïk (¬µ) := œïk‚àí1 (¬µ).          (5.29)
blank         | 
text          | This choice saves queries to the expensive high-dimensional model and still guarantees global con-
              | vergence when (5.28) is satisfied.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     126
blank         | 
              | 
              | 
text          |    As written, the above approach to compute Œ¶k requires the singular value decomposition of the
              | snapshot matrices Uk and Vk that have an increasing number of columns. This quickly becomes
              | prohibitively expensive since the cost of the SVD scales quadratically in the number of columns [75].
              | However, the snapshot matrices satisfy the simple relation
              |                                            h                   i
              |                                       Uk = Uk‚àí1       u(¬µk‚àí1 )
              |                                            h                   i                                 (5.30)
              |                                       VÃÇk = VÃÇk‚àí1     vÃÇ(¬µk‚àí1 ) ,
blank         | 
text          | and therefore the thin SVD updates in Algorithm 9 can be used to compute the SVD of Uk from the
              | SVD of Uk‚àí1 . Only the QR decomposition of the compressed snapshot matrices must be recomputed
              | at each iteration.
blank         | 
text          | Remark. For time-dependent problems, exact preservation of u(¬µk ) in the column space of Œ¶k may
              | be unrealistic since u(¬µk ) corresponds to an entire time history. In this case, POD (Algorithm 4)
              | can be applied to u(¬µk ) and vÃÇ(¬µk ) with the level of compression set such that (3.14) and (3.15) are
              | satisfied. Then the basis can be defined as
blank         | 
text          |                           Œ¶k = PODHSP(POD(u(¬µk )), Uk , POD(vÃÇ(¬µk )), VÃÇk ).
blank         | 
text          | This is similar to the original TRPOD method [10] that constructs the reduced basis according to
              | Œ¶k = POD(u(¬µk )) or the extension presented in [57] that also constructs a reduced-order model for
              | the adjoint that constructs the basis according to Œ¶vk = POD(v(¬µk )).
blank         | 
text          |    To close this section, global convergence of Algorithm 11 is established based on Theorem A.1.
              | Suppose Assumptions (AF1)‚Äì(AF2) and (AM1)‚Äì(AM4) (Appendix A) hold and let {¬µk } denote
              | the sequence of iterations produced by Algorithm 11. To apply Theorem A.1 and conclude that
              | this algorithm is globally convergent, the choice of mk (¬µ), œëk (¬µ), œïk (¬µ) in (5.2), (5.3), (5.6)-(5.7)
              | must satisfy the error bounds in (3.12), (3.13) and the conditions in (3.14), (3.15). The objective
              | and gradient error bounds are established based on the residual-based error bounds detailed in
              | Appendix B. The construction of the reduced-order basis Œ¶k in (5.27) combined with the fact
              | that Œ®k is defined according to (4.14) to ensure the reduced-order model possesses the minimum-
              | residual property guarantees the objective (3.14) and gradient (3.15) error conditions. Therefore,
              | by Theorem A.1, the sequences of iterates produced by Algorithm 11 satisfies
blank         | 
text          |                                        lim inf k‚àáF (¬µk )k = 0.                                   (5.31)
              |                                            k‚Üí‚àû
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                         127
blank         | 
              | 
              | 
text          | Algorithm 11 Residual-based trust region method based on reduced-order models
              |  1:   Initialization: Given
              |                       ¬µ0 , U‚àí1 = ‚àÖ, VÃÇ‚àí1 = ‚àÖ, ‚àÜ0 , 0 < Œ≥ < 1, ‚àÜmax > 0, 0 < Œ∑1 < Œ∑2 < 1,
              |                             0 < Œ∫œë < 1, 0 < Œ∫œï , 0 < œâ < 1, {rk }‚àû
              |                                                                  k=0 such that rk ‚Üí 0
blank         | 
text          |  2:   Model and constraint update: If previous model and constraint are sufficient for convergence
blank         | 
text          |                      œëk‚àí1 (¬µk ) ‚â§ Œ∫œë ‚àÜk                œïk‚àí1 (¬µk ) ‚â§ Œ∫œï min{k‚àámk‚àí1 (¬µk )k , ‚àÜk },
blank         | 
text          |       re-use for the current iteration: mk (¬µ) := mk‚àí1 (¬µ) and œëk (¬µ) := œëk‚àí1 (¬µ). Otherwise, evaluate
              |       primal and sensitivity or adjoint solution of high-dimensional model
              |                                                 ‚àÇu                          ‚àÇr
              |                 uk := u(¬µk )           vÃÇk :=      (¬µ ) or ŒòŒª
              |                                                             k (u(¬µk ), ¬µk )    (u(¬µk ), ¬µk )T Œª(¬µk )
              |                                                 ‚àÇ¬µ k                        ‚àÇu
blank         | 
text          |       and compute reduced-order basis via span-preserving variant of POD (Algorithm 7)
blank         | 
text          |                                               Œ¶k = PODHSP(uk , Uk , vÃÇk , VÃÇk ),
blank         | 
text          |       define model and constraint as
              |                  mk (¬µ) = f (Œ¶k ur (¬µ; Œ¶k , Œ®k ), ¬µ)
              |                  œëk (¬µ) = kr(Œ¶k ur (¬µk ; Œ¶k , Œ®k ), ¬µk )kŒòk + kr(Œ¶k ur (¬µ; Œ¶k , Œ®k ), ¬µ)kŒòk ,
blank         | 
text          |       and update snapshot matrices
blank         |                                                                                       
text          |                                  Uk+1 ‚Üê Uk‚àí1           uk         VÃÇk+1 ‚Üê VÃÇk‚àí1       vÃÇk .
blank         | 
              | 
text          |  3:   Step computation: Solve (exactly) the trust region subproblem
blank         | 
text          |                                        min mk (¬µ)          subject to      œëk (¬µ) ‚â§ ‚àÜk
              |                                     ¬µ‚ààRN¬µ
blank         | 
text          |     for a candidate, ¬µÃÇk , using interior-point method of Section 3.1.2.
              |  4: Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio
              |                      Ô£±
              |                      Ô£¥
              |                      Ô£≤            1            if œëk (¬µÃÇk )œâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }
              |                œÅk =       F (¬µk ) ‚àí F (¬µÃÇk )
              |                      Ô£¥
              |                      Ô£≥                         otherwise
              |                         mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
text          |     where Œ∑ < min{Œ∑1 , 1 ‚àí Œ∑2 }
              |  5: Step acceptance:
blank         | 
text          |                 if        œÅk ‚â• Œ∑1       then         ¬µk+1 = ¬µÃÇk         else    ¬µk+1 = ¬µk      end if
blank         | 
              | 
text          |  6:   Trust region update:
blank         | 
text          |                      if     œÅk ‚â§ Œ∑ 1                then        ‚àÜk+1 ‚àà (0, Œ≥œëk (¬µÃÇk )]        end if
              |                      if     œÅk ‚àà (Œ∑1 , Œ∑2 )         then        ‚àÜk+1 ‚àà [Œ≥œëk (¬µÃÇk ), ‚àÜk ]      end if
              |                      if     œÅk ‚â• Œ∑ 2                then        ‚àÜk+1 ‚àà [‚àÜk , ‚àÜmax ]           end if
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                       128
blank         | 
              | 
              | 
title         | 5.2       Snapshots from Partially Converged Solutions
text          | In many large-scale applications, particularly those arising in turbulent computational fluid dynam-
              | ics, it is difficult and expensive to compute a steady-state solution, and the corresponding sensitivity
              | and adjoint solutions, to tight tolerances. In these cases, the generation of snapshots in Line 2 of
              | Algorithm 11 will dominate the cost of the trust region method. To speed up this step and leverage
              | the flexibility afforded by the trust region method of Section 3.1.1, partially converged solutions are
              | used as snapshots.
              |     Let u(¬µ; œÑu ) denote a partially converged primal solution of tolerance œÑu , defined as any point
              | that satisfies
              |                                                   kr( ¬∑ , ¬µ)kŒò ‚â§ œÑu .                                              (5.32)
blank         | 
text          | While the fully converged solution u(¬µ) is assumed to be unique (Assumption 2.2), there are many
              | points satisfying (5.32) for a given œÑu > 0. A simple method to find a point that satisfies (5.32) is to
              | use the chosen nonlinear solver (Newton-Raphson, Gauss-Newton, pseudo-transient continuation)
              | with (5.32) used as the convergence criteria1 . Similarly, let v(¬µ; œÑu , œÑv ) be a partially converged
              | sensitivity or adjoint solution of tolerance œÑv about a partially converged primal solution of tolerance
              | œÑu , defined as any point satisfying
blank         | 
text          |                                            kr v (u(¬µ; œÑu ), ¬∑ , ¬µ)kŒòv ‚â§ œÑv                                         (5.33)
blank         | 
text          | where r v is the sensitivity or adjoint residual, depending on which method is used to compute
              | reduced-space gradients2 . Furthermore, these definition are extended to define the partially con-
              | verged snapshot vÃÇ(¬µ; œÑu , œÑv ) as
              |                         Ô£±
              |                         Ô£≤v(¬µ; œÑu , œÑv )                                                sensitivity method
              |     vÃÇ(¬µ; œÑu , œÑv ) =                                                                                              (5.34)
              |                         Ô£≥ŒòŒª (u(¬µ; œÑ ), ¬µ) ‚àÇr (u(¬µ; œÑ ), ¬µ)T v(¬µ; œÑ , œÑ )               adjoint method.
              |                                    u                u             u   v
              |                                           ‚àÇu
blank         | 
text          | With these definitions, the snapshot matrices of partially converged solutions are defined as
              |                                     h                                       i
              |                                Uk = u(¬µ0 ; œÑu0 ) ¬∑ ¬∑ ¬∑ u(¬µk‚àí1 , œÑuk‚àí1 )
              |                                     h                                                   i                          (5.35)
              |                                VÃÇk = vÃÇ(¬µ0 ; œÑu0 , œÑv0 ) ¬∑ ¬∑ ¬∑ vÃÇ(¬µk‚àí1 , œÑuk‚àí1 , œÑvk‚àí1 )
blank         | 
text          | and the reduced-order basis is constructed from these snapshots using the heterogeneous span-
              | preserving variant of POD (Algorithm 7)
blank         | 
text          |                               Œ¶k = PODHSP(u(¬µk ; œÑuk ), Uk , vÃÇ(¬µk ; œÑuk , œÑvk ), VÃÇk ),                           (5.36)
              |    1 This is not a restrictive requirement since residual-based convergence criteria are usually, if not always, used for
blank         | 
text          | nonlinear solvers. However, using a norm other than the 2-norm is non-standard.
              |    2 This assumes an iterative solvers is used to solve the linear sensitivity or adjoint system since a direct solver will
blank         | 
text          | always return the solution to machine precision.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    129
blank         | 
              | 
              | 
text          | where œÑuk and œÑvk are iteration-dependent tolerances. This definition of Œ¶k guarantees that the
              | partially converged primal and sensitivity/adjoint solutions are contained in the reduced subspace
              | to the exact accuracy at which they were computed, i.e.,
blank         | 
text          |                             u(¬µk ; œÑuk ) ‚àà col(Œ¶k )     vÃÇ(¬µk ; œÑuk , œÑvk ) ‚àà col(Œ¶k ),          (5.37)
blank         | 
text          | which in turn implies
blank         | 
text          |                      ‚àÇu
              |                         (¬µ ; œÑ k , œÑ k ) ‚àà col(Œ¶k )     or      Œª(¬µk ; œÑuk , œÑvk ) ‚àà col(Œ®k ),   (5.38)
              |                      ‚àÇ¬µ k u v
blank         | 
text          | assuming the conditions in Propositions 4.3 or 4.5 are satisfied. If a minimum-residual primal
              | reduced-order model with optimality metric Œò is used, the optimality property gives
blank         | 
text          |                          kr(Œ¶k ur (¬µk ; Œ¶k , Œ®k )kŒò ‚â§ r(u(¬µk ; œÑuk ), ¬µk )       Œò
              |                                                                                      ‚â§ œÑuk       (5.39)
blank         | 
text          | The first inequality holds from the optimality property (Proposition 4.1) since u(¬µk ; œÑuk ) ‚àà col(Œ¶k )
              | and the second inequality holds from the definition of the partially converged solution in (5.32). For
              | the residual-based error indicators (5.3), (5.6), and (5.7), this implies
blank         | 
text          |                                            œëk (¬µk ) ‚â§ 2œÑuk
              |                                                                                                  (5.40)
              |                                            œïk (¬µk ) ‚â§ Œ±1 œÑuk + Œ±2 œÑvk ,
blank         | 
text          | provided minimum-residual reduced-order models are used. Therefore the objective error condition
              | (3.14) is satisfied if
              |                                                œÑuk ‚â§ (1/2)Œ∫œë ‚àÜk                                  (5.41)
blank         | 
text          | holds and the gradient error condition (3.15) is satisfied if
blank         | 
text          |                                      Œ±1 œÑuk ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }
              |                                                                                                  (5.42)
              |                                      Œ±2 œÑvk ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }
blank         | 
text          | holds. These bounds are combined to yield the following requirement on œÑuk and œÑvk
blank         | 
text          |                                   œÑuk ‚â§ (1/Œ±1 )Œ∫œï min{k‚àámk (¬µk )k , Œ∫‚àÜk }
              |                                                                                                  (5.43)
              |                                   œÑvk ‚â§ (1/Œ±2 )Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk },
blank         | 
text          | where Œ∫ = min{1, Œ±1 Œ∫œë /(2Œ∫œï )}. Since this condition ensures (3.12)-(3.15), global convergence of the
              | resulting trust region method is guaranteed. The relationship in (5.43), and the results that follow,
              | only hold in the Œò-norm (Proposition 4.1) so the I-norm form of the residual-based error indicators
              | in (5.18) cannot be used, unless Œò = I (LSPG).
blank         | 
meta          | Remark. The value of œÑuk and œÑvk depend on k‚àámk (¬µk )k, which in turn depends on the values of
              | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                         130
blank         | 
              | 
              | 
text          | œÑuk and œÑvk used to define snapshots for Œ¶k . Therefore, the values of œÑuk and œÑvk cannot be simply
              | determined from (5.43). Instead, an iterative method is employed that begins initially selects large
              | values œÑuk and œÑvk and systematically reduces them, i.e., via backtracking, until the conditions in
              | (3.14), (3.15) are satisfied. This will lead to an efficient algorithm since the partially converged
              | primal and dual solutions for a given œÑuk and œÑvk can be used to warm-start the nonlinear solvers for
              | any smaller values for these tolerances. An alternate approach replaces the gradient condition in
              | (3.15) with
              |                                œïk (¬µk ) ‚â§ Œ∫œï min{ ‚àámk‚àí1 (¬µk‚àí1 ) , ‚àÜk }.                              (5.44)
blank         | 
text          | It can be verified that this will preserve the convergence result in Theorem A.1 of Appendix A. This
              | replaces the the gradient condition in (5.43) with
blank         | 
text          |                                       1
              |                                œÑuk ‚â§     Œ∫œï min{ ‚àámk‚àí1 (¬µk‚àí1 ) , Œ∫‚àÜk }
              |                                      2Œ±1
              |                                                                                                      (5.45)
              |                                       1
              |                                œÑvk ‚â§     Œ∫œï min{ ‚àámk‚àí1 (¬µk‚àí1 ) , ‚àÜk }.
              |                                      2Œ±2
blank         | 
text          | This alternate gradient condition preserves global convergence and allows for the direct computation
              | of œÑuk and œÑvk since all terms on the right-hand side of the inequality are independent of œÑuk and œÑvk .
blank         | 
text          | Remark. The case with the traditional trust region constraint, œëk (¬µ) = k¬µ ‚àí ¬µk k, satisfies œëk (¬µk ) =
              | 0 trivially and therefore the lighter restrictions on œÑuk and œÑvk in (5.42) can be used in place of those
              | in (5.43).
blank         | 
              | 
title         | 5.3      Efficient Trust Region Assessment with Partially Con-
              |          verged Solutions
text          | Another opportunity for efficiency afforded by the flexible trust region framework introduced in
              | Section 3.1.1, that has not been fully leveraged in the residual-based reduced-order model trust
              | region method of this chapter, is the use of an approximation model to compute the ratio of actual-to-
              | predicted ratio, œÅk . Section 3.1.1 outlined the use of this flexibility to effectively skip the computation
              | of the actual-to-predicted reduction ratio by taking
blank         | 
text          |                               œàk (¬µ) = mk (¬µ)       and      Œ∏k (¬µ) = œëk (¬µ)
blank         | 
text          | whenever œëk (¬µÃÇk )œâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }. In this situation, the approximation to the
              | actual-to-predicted reduction ratio is always unity and the step is accepted and the radius increased.
              | This is implies the trust region assessment step is effectively free since it does not require a query
              | to F (¬µ) and is guaranteed to preserve global convergence since it conforms to (3.21), (3.22). In
              | Section 5.1, the true value of œÅk is computed (3.9) when œëk (¬µÃÇk ) fails to satisfy the above bound.
              | This section seeks to improve on this using the approximate form of œÅk in (3.20) where œàk (¬µ)
              | leverages partially converged solutions.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                        131
blank         | 
              | 
              | 
text          |    In the event that the error condition in (3.22) is not satisfied, the choice œàk (¬µ) = mk (¬µ) is
              | not sufficient to ensure convergence. Instead, partially converged solutions are used as they can
              | substantially less expensive to compute than fully converged ones and can be tailored to exactly
              | meet the error criteria in (3.22). Consider the objective model œàk (¬µ) defined by the quantity of
              | interest evaluated at a partially converged steady state and the corresponding residual-based error
              | indicator
              |                          œàk (¬µ) = f (u(u; œÑÃÇuk ), ¬µ)
              |                                                                                                     (5.46)
              |                          Œ∏k (¬µ) = r(u(¬µk ; œÑÃÇuk ), ¬µk )      Œò
              |                                                                + r(u(¬µ; œÑÃÇuk ), ¬µ)   Œò
              |                                                                                        .
blank         | 
text          | Unlike in the previous section where partially converged solutions are used as snapshots, the Œò-norm
              | above can be freely replaced with the I-norm for simplicity (and computational efficiency), provided
              | partially converged solutions are defined with respect to the I-norm. Either norm can be used in
              | this case since the optimality property of minimum-residual ROMs (Proposition 4.1) is not required
              | as it was in the previous section. However, it is desirable to use the same norm in both cases since
              | the computation of u(¬µÃÇk ; œÑÃÇuk ), required to compute œàk (¬µÃÇk ), will provide a better warm-start for the
              | snapshot computation u(¬µk ; œÑuk+1 ) at iteration k + 1.
              |    With these choices, the bound in (3.21) holds from an identical argument to that in (5.43). From
              | the definition of u(¬µ; œÑÃÇuk ) in Section 5.2, the following relation holds
blank         | 
text          |                                                  Œ∏k (¬µÃÇk ) ‚â§ 2œÑÃÇuk .                                (5.47)
blank         | 
text          | Therefore, the accuracy condition in (3.22) holds provided
blank         | 
text          |                                         1                                   1/œâ
              |                                œÑÃÇuk ‚â§     [Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }]                        (5.48)
              |                                         2
blank         | 
text          | and global convergence is ensured. In addition to being a less expensive option than fully converged
              | evaluations of F (¬µ), this method fits seamlessly with the use of partially converged solutions in the
              | snapshot computations of the previous section. When an iteration is accepted, i.e., ¬µk+1 = ¬µÃÇk , the
              | partially converged solution u(¬µk+1 , œÑÃÇuk ) = u(¬µÃÇk , œÑÃÇuk ) can be used to warm-start the computation
              | of u(¬µk+1 , œÑuk+1 ) that is required to compute snapshots for iteration k + 1. In fact, if œÑÃÇuk ‚â§ œÑuk+1 the
              | computation can be skipped entirely since u(¬µÃÇk , œÑÃÇuk ) already satisfies
blank         | 
text          |                                          r(u(¬µÃÇk , œÑÃÇuk ), ¬µk+1 ) ‚â§ œÑuk+1 .                         (5.49)
blank         | 
text          | The complete algorithm that uses partially converged solutions as snapshots in the model update
              | and in the computation of the actual-to-predicted reduction ratio is provided in Algorithm 12. To
              | establish global convergence of this algorithm based on Theorem A.1, suppose Assumptions (AF1)‚Äì
              | (AF2) and (AM1)‚Äì(AM4) (Appendix A) hold and let {¬µk } denote the sequence of iterates produced
              | by Algorithm 12. Section 5.1.1 already established that the choice of mk (¬µ), œëk (¬µ), œïk (¬µ) satisfy
              | the error bounds in (3.14), (3.15). The construction of the reduced-order basis Œ¶k in (5.27) and the
              | requirements placed on the partially converged solutions in (5.43), combined with the fact that Œ®k
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    132
blank         | 
              | 
              | 
text          | is defined according to (4.14) to ensure the reduced-order model possesses the minimum-residual
              | property guarantees the objective (3.14) and gradient (3.15) error conditions hold. Finally, œàk (¬µ)
              | and Œ∏k (¬µ) in (5.46) must satisfy the error bound (3.21) and condition (3.22) to preserve global
              | convergence when the approximate actual-to-predicted ratio is used to assess the trust region step.
              | The residual-based error bounds established in Lemma B.4, B.7, B.8 ensures the error bound holds.
              | The requirements on the partially converged solution in (5.47)-(5.48) ensure the error condition (3.22)
              | holds. Therefore, by Theorem A.1, the sequences of iterates produced by Algorithm 12 satisfies
blank         | 
text          |                                        lim inf k‚àáF (¬µk )k = 0.                                  (5.50)
              |                                           k‚Üí‚àû
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                 133
blank         | 
              | 
              | 
              | 
text          | Algorithm 12 Residual-based trust region method based on reduced-order models and partially
              | converged solutions
              |  1:   Initialization: Given
              |                      ¬µ0 , U‚àí1 = ‚àÖ, VÃÇ‚àí1 = ‚àÖ, ‚àÜ0 , 0 < Œ≥ < 1, ‚àÜmax > 0, 0 < Œ∑1 < Œ∑2 < 1,
              |                            0 < Œ∫œë < 1, 0 < Œ∫œï , 0 < œâ < 1, {rk }‚àû
              |                                                                 k=0 such that rk ‚Üí 0
blank         | 
text          |  2:   Model and constraint update: If previous model and constraint are sufficient for convergence
blank         | 
text          |                      œëk‚àí1 (¬µk ) ‚â§ Œ∫œë ‚àÜk             œïk‚àí1 (¬µk ) ‚â§ Œ∫œï min{k‚àámk‚àí1 (¬µk )k , ‚àÜk },
blank         | 
text          |       re-use for the current iteration: mk (¬µ) := mk‚àí1 (¬µ) and œëk (¬µ) := œëk‚àí1 (¬µ). Otherwise, evaluate
              |       primal and sensitivity or adjoint solution of high-dimensional model to tolerances œÑuk and œÑvk ,
              |       respectively,
blank         | 
text          |               uk := u(¬µk ; œÑuk )
              |                      ‚àÇu                                            ‚àÇr
              |               vÃÇk :=    (¬µ ; œÑ k , œÑ k ) or ŒòŒª (u(¬µk ; œÑuk ), ¬µk )    (u(¬µk ; œÑuk ), ¬µk )T Œª(¬µk ; œÑuk , œÑvk )
              |                      ‚àÇ¬µ k u v                                      ‚àÇu
              |       with tolerances given by
              |                              1
              |                       œÑuk ‚â§     Œ∫œï min{k‚àámk (¬µk )k , Œ∫‚àÜk }             Œ∫ = min{1, Œ±1 Œ∫œë /(2Œ∫œï )}
              |                             2Œ±1
              |                              1
              |                       œÑvk ‚â§     Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk },
              |                             2Œ±2
              |       and compute reduced-order basis via span-preserving variant of POD (Algorithm 7)
blank         | 
text          |                                           Œ¶k = PODHSP(uk , Uk , vÃÇk , VÃÇk ),
blank         | 
text          |       define model and constraint as
              |                    mk (¬µ) = f (Œ¶k ur (¬µ; Œ¶k , Œ®k ), ¬µ)
              |                     œëk (¬µ) = kr(Œ¶k ur (¬µk ; Œ¶k , Œ®k ), ¬µk )kŒò + kr(Œ¶k ur (¬µ; Œ¶k , Œ®k ), ¬µ)kŒò ,
blank         | 
text          |       and update snapshot matrices
blank         |                                                                                     
text          |                                 Uk+1 ‚Üê Uk‚àí1          uk        VÃÇk+1 ‚Üê VÃÇk‚àí1        vÃÇk .
blank         | 
text          |  3: Step computation: identical to Line 3 in Algorithm 11
              |  4: Computed-to-predicted reduction: Compute computed-to-predicted reduction ratio
              |                               Ô£±
              |                               Ô£¥
              |                               Ô£≤           1           if œëk (¬µÃÇk )œâ ‚â§ œÑÃÇuk
              |                          œÅk =     œà (¬µ ) ‚àí œàk (¬µÃÇk )
              |                               Ô£≥ k k
              |                               Ô£¥                       otherwise
              |                                  mk (¬µk ) ‚àí mk (¬µÃÇk )
              |                                     1                                    1/œâ
              |       œàk (¬µ) := f (u(¬µ; œÑÃÇuk ), ¬µ)     œÑÃÇuk =
              |                                       [Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }]                     Œ∑ < min{Œ∑1 , 1 ‚àí Œ∑2 }
              |                                     2
              |  5: Step acceptance: identical to Line 5 in Algorithm 11
              |  6: Trust region update: identical to Line 6 in Algorithm 11
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                        134
blank         | 
              | 
              | 
title         | 5.4      Extension to Hyperreduced Models
text          | To this point, projection-based reduced-order models have solely been considered as the trust region
              | approximation model. However, as discussed in Chapter 4, this will not be sufficient to realize non-
              | trivial speedups for nonlinear problems. For such problems, the computational complexity associated
              | with the evaluation of the reduced residual and Jacobian scales with the size of the original HDM
              | since they require reconstruction of the full state vector from the reduced coordinates, assembly over
              | the entire mesh, and subsequent projection onto the columnspace of the test basis. For this reason,
              | the developments in this chapter are extended to use collocation-based hyperreduced models as the
              | approximation model.
              |    The approximation model takes the same form as in the previous sections, i.e.,
blank         | 
text          |                                 mk (¬µ) = f (Œ¶k ur (¬µ; Œ¶k , Œ®k , Pk ), ¬µ),                           (5.51)
blank         | 
text          | with the exception that the reduced coordinates ukr (¬µ) are defined as the solution of the collocation-
              | based hyperreduced model
blank         | 
text          |                          (PkT Œ®k )T PkT r(PÃÑk PÃÑkT Œ¶k ur (¬µ; Œ¶k , Œ®k , Pk ), ¬µ) = 0.                (5.52)
blank         | 
text          | The gradient ‚àámk (¬µ) is computed according to the adjoint or sensitivity method presented in
              | Sections 4.2.5‚Äì4.2.6 or approximated using the minimum-residual variants. For the sake of efficiency,
              | the residual-based trust region constraint œëk (¬µ) in (5.3) is replaced with the masked residual
blank         | 
text          |                                   œëk (¬µ) = PkT r(PÃÑk PÃÑkT Œ¶k ukr (¬µ), ¬µ)                            (5.53)
blank         | 
text          | and the gradient error indicator œïk (¬µ) is similarly replaced with its masked counterpart, i.e.,
blank         | 
text          |   œïk (¬µ) = Œ±1 PkT r(PÃÑk PÃÑkT Œ¶k ur (¬µ; Œ¶k , Œ®k , Pk ), ¬µ) +
blank         |                      
text          |                                                                 ‚àÇur
              |                                                                                            (5.54)
              |                 T ‚àÇ          T                              T
              |            Œ±2 Pk r PÃÑk PÃÑk Œ¶k ur (¬µ; Œ¶k , Œ®k , Pk ), PÃÑk PÃÑk Œ¶k     (¬µ; Œ¶k , Œ®k , Pk ), ¬µ .
              |                                                                 ‚àÇ¬µ
blank         | 
text          | Only the sensitivity method is considered since details pertaining to the hyperreduced minimum-
              | residual adjoint method is deferred to future work.
              |    For general nonlinear systems of equations r(u, ¬µ) = 0, these choices of error indicators œëk (¬µ)
              | and œïk (¬µ) do not lead to the required bounds in (3.12) and (3.13) and global convergence cannot
              | be rigorously established. However, due to the concept of a stencil in the discretization of partial
              | differential equations, i.e., the fact that the ith entry of r depends on the jth entry of u for all j ‚àà Si
              | (defined in Section 4.2.2), it is reasonable to expect such bounds to hold (with larger constants Œ∂
              | and Œæ), provided the mask is sufficiently large.
              |    The details pertaining to the construction of Œ¶k from fully (Section 5.1.2) or partially (Sec-
              | tions 5.2) converged solutions carries over to the case of collocation-based hyperreduced models;
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    135
blank         | 
              | 
              | 
text          | however, the bounds on the error indicators can no longer be guaranteed due to the introduction of
              | the mask in the reduced-order model. Once the trial basis is constructed, the mask Pk is constructed
              | according to the algorithm detailed in [198] that relies solely on Œ¶k and possibly problem-specific
              | information. The approximation of the ratio of actual-to-predicted reduction that uses œàk (¬µ) and
              | Œ∏k (¬µ) based on partially converged solutions is not specific to the case of projection-based reduced-
              | order models and therefore trivially carries over to the hyperreduced case. The complete trust region
              | algorithm based on collocation-based hyperreduced models is identical to Algorithms 11 and 12, once
              | the step that constructs the mask Pk from Œ¶k is added, with the above definitions of mk (¬µ), œëk (¬µ),
              | and œïk (¬µ).
blank         | 
              | 
title         | 5.5     Numerical Experiments
text          | In this section, the error-aware trust region method using projection-based reduced-order models as
              | the approximation model is applied to solve a number of problems in computational fluid dynamics,
              | ranging from optimal control of the 1D inviscid Burgers‚Äô equation to shape optimization of a full
              | aircraft configuration.
blank         | 
              | 
title         | 5.5.1    Optimal Control of 1D Inviscid Burgers‚Äô Equation
text          | This section presents a thorough investigation of the trust region methods proposed in this chap-
              | ter based on the various projection-based reduced-order models of Chapter 4. The model PDE-
              | constrained optimization problem considered is optimal control of the steady, inviscid, one-dimensional
              | Burgers‚Äô equation in only a few control parameters. The optimization problem takes the form
blank         | 
text          |                                                  1
              |                                                      1
              |                                             Z
              |                                minimize                (u(¬µ, x) ‚àí uÃÑ(x))2 dx                    (5.55)
              |                                 ¬µ‚ààRn¬µ
              |                                              0       2
blank         | 
text          | where u(¬µ, x) is the solution of the inviscid Burgers‚Äô equation under a specific parametrization of
              | the inflow boundary condition and control
blank         | 
text          |                               u(¬µ, x)‚àÇx u(¬µ, x) = ¬µ2 e¬µ3 x        x ‚àà (0, 100)
              |                                                                                                 (5.56)
              |                                           u(¬µ, 0) = ¬µ1
blank         | 
text          | and uÃÑ(x) is the target state. The PDE is discretized with a first-order, vertex-centered finite volume
              | method with 1000 vertices for a state space of dimension Nu = 999 after application of the inflow
              | boundary condition. The functional form of the control in (5.56) was made to minimize the number
              | of optimization parameters to allow the sensitivity-based approach to be included in the study. The
              | target state corresponds to the solution of the (5.56) at the target parameter configuration ¬µÃÑ =
              | (2.5, 0.02, 0.0425). Therefore, the target state is realizable since it lies within the parametrization
              | of the optimization problem and the optimal value of the objective function is 0. All methods
              | considered will start from an initial guess of ¬µ0 = (1.0, 1.0, 0.0). Figure 5.1 shows the control
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                       136
blank         | 
              | 
              | 
text          |           1.5                                                        15
blank         | 
text          |            1                                                         10
blank         | 
              | 
              | 
              | 
text          |                                                            u(¬µ, x)
              | g(¬µ, x)
blank         | 
              | 
text          |           0.5                                                        5
blank         | 
text          |            0                                                         0
              |                 0   20    40       60     80     100                      0   20   40       60   80         100
              |                                x                                                        x
blank         | 
              | 
text          | Figure 5.1: Control (left) and corresponding solution (right) of the inviscid Burgers‚Äô equation in
              | (5.56) at: the initial condition ¬µ = (1.0, 1.0, 0.0) ( ), the target solution ¬µ = (2.5, 0.02, 0.0425)
              | (    ), and solution of the baseline optimization method (     ).
blank         | 
              | 
text          | g(¬µ, x) and state vector u(¬µ, x) at the initial guess and optimal value of ¬µ. Figure 5.2 shows the
              | contours of the objective function (after discretization) in the ¬µ1 ‚àí¬µ2 plane at a slice of the parameter
              | space at ¬µ3 = 0, with the initial condition ¬µ0 and optimal solution ¬µ‚àó indicated.
blank         | 
title         | Trust region geometry and impact of snapshots
blank         | 
text          | Before studying the entire performance of the proposed optimization solvers on the optimal control
              | problem in (5.55), the geometry of the various trust region constraints presented in this document
              | are considered: the traditional trust region constraint
blank         | 
text          |                                                k¬µ ‚àí ¬µ0 k ‚â§ ‚àÜ
blank         | 
text          | and the residual-based trust region constraint
blank         | 
text          |                          kr(Œ¶ur (¬µ0 ; Œ¶, Œ®), ¬µ0 )k + kr(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)k ‚â§ ‚àÜ.
blank         | 
text          | A trust region constraint based on the true error in the quantity of interest
blank         | 
text          |                                    |f (u(¬µ), ¬µ) ‚àí f (Œ¶ur (¬µ; Œ¶, Œ®))| ‚â§ ‚àÜ
blank         | 
text          | is included in this study for illustration purposes only as it is far too expensive to use in practice.
              | The traditional trust region is purely geometric and therefore does not depend on the reduced-order
              | model, while the residual- and error-based trust regions are heavily dependent on the type of reduced-
              | order model employed and the trial subspace chosen. This section considers reduced-order models
              | based on a Galerkin and LSPG projection. Since the Jacobians of the discrete inviscid Burgers‚Äô
              | equation are not symmetric positive-definite, reduced-order models based on a Galerkin projection
              | do not necessarily possess the minimum-residual property. However, LSPG-based ROMs do possess
              | the minimum-residual property, by definition. The trial basis will be constructed in three different
              | ways following the developments in Section 5.1.2: from snapshots of the primal solution only, from
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                      137
blank         | 
              | 
              | 
text          | snapshots of the primal and sensitivity solutions, and from snapshots of the primal and adjoint
              | solutions. All snapshots will be computed at the control corresponding to the initial condition of
              | the optimization problem, ¬µ0 . That is,
blank         |                                                                                                    
text          |                                                                                          ‚àÇu
              |   col(Œ¶) = span{u(¬µ0 )}        col(Œ¶) = span{u(¬µ0 ), Œª(¬µ0 )}       col(Œ¶) = span u(¬µ0 ),    (¬µ )
              |                                                                                          ‚àÇ¬µ 0
blank         | 
text          | depending on which trial subspace is being considered.
              |    The trust regions for the reduced-order models based on a Galerkin projection are provided in
              | Figure 5.3 and those based on a LSPG projection are in Figure 5.4. These figures show the contours
              | of the reduced objective function f (Œ¶ur (Œ¶, Œ®), ¬µ), which can be compared to the contours of
              | the true objective function f (u(¬µ), ¬µ) in Figure 5.2. It can be seen that the residual-based trust
              | regions do not match the trust regions based on the true error. Even though the use of residuals as
              | a surrogate for the true error partially motivated the introduction of the error-aware trust region
              | theory in Chapter 3, it is not a requirement since the asymptotic bound (3.12) holds due to the
              | derivation in Appendix B. From Figures 5.3 and 5.4, a few more observations are made that agree
              | with the minimum-residual reduced-order model theory in Chapter 4. First, the residual-based trust
              | regions corresponding to a Galerkin ROM (Figure 5.3) are a subset of those corresponding to the
              | LSPG ROM (Figure 5.4). This is expected since LSPG minimizes the residual in the I-norm, which
              | is exactly the quantity defining the trust region. Despite the larger residual-based trust regions of
              | LSPG ROMs, the Galerkin ROMs have larger trust regions based on the true error. While non-
              | intuitive, this does not contradict the theory outlined in Chapter 4 since LSPG is only guaranteed to
              | minimize the residual over the trial subspace, not the error in a quantity of interest. Finally, for both
              | the Galerkin and LSPG ROMs, the trial subspaces built from primal states and sensitivities produce
              | larger residual- and error-based trust regions than only those that only use primal snapshots. There
              | is disagreement between the two types of reduced-order models when it comes to the use of adjoint
              | snapshots. For the Galerkin ROMs, the incorporation of adjoint snapshots improve the prediction
              | capability of the reduced-order model with respect to the quantity of interest, but have little influence
              | on the extent of the residual-based trust region. In contrast, the incorporation of adjoint snapshots
              | increases the extent of the residual-based trust region‚Äîas expected from the monotonicity property
              | of minimum-residual reduced-order models‚Äî however, they actually reduce the extent of the error-
              | based trust region. This provides some evidence that the incorporation of non-physical snapshots
              | can cause the residual minimization to produce worse solutions with respect to prediction of the
              | quantity of interest [198].
blank         | 
title         | Performance of proposed optimization solvers
blank         | 
text          | This section provides a thorough comparison of the variants of the multifidelity trust region method
              | based on reduced-order models and partially converged solutions in Algorithms 11 and 12. The
              | following aspects of the algorithms will be considered in this study:
blank         | 
text          |    ‚Ä¢ the type of reduced-order model underlying the approximation model: Galerkin and LSPG
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                           138
blank         | 
              | 
              | 
              | 
text          |                                    1.4
blank         | 
text          |                                    1.2
blank         | 
text          |                                    1.0
blank         | 
text          |                                    0.8
blank         | 
              | 
              | 
              | 
text          |                               ¬µ2
              |                                    0.6
blank         | 
text          |                                    0.4
blank         | 
text          |                                    0.2
blank         | 
text          |                                    0.0
              |                                          0.0   0.2     0.4        0.6     0.8        1.0
              |                                                              ¬µ1
blank         | 
              | 
text          | Figure 5.2: Contours of the objective function f (u(¬µ), ¬µ) in (5.55) in the ¬µ1 ‚àí¬µ2 plane corresponding
              | to a slice at ¬µ3 = 0.0. The initial condition for the optimization problem and target solution are
              | shown with a red circle and blue square, respectively.
blank         | 
              | 
text          |       projections will be considered,
blank         | 
text          |    ‚Ä¢ the type of snapshots used to define the trial subspace: primal snapshot alone, primal and
              |       sensitivity snapshots, and primal and adjoint snapshots will be considered,
blank         | 
text          |    ‚Ä¢ the trust region constraint used to define the trust region subproblem: the traditional ball
              |       constraint and residual-based constraint (5.3) will be considered, and
blank         | 
text          |    ‚Ä¢ the optimization solver used for the trust region subproblem: the interior point method of
              |       Section 3.1.2 based on a Newton-CG solver3 will be used to exactly solve the subproblem and
              |       Steihaug-Toint CG will be used to approximately solve the subproblem (when the traditional
              |       trust region constraint is used).
blank         | 
text          | Table 5.1 summarizes the variants of Algorithms 11 and 12 considered in this section and pro-
              | vides appropriate names for convenient reference. For the Galerkin reduced-order models, the true
              | sensitivities and adjoints will be computed according to (4.20), (4.47) since this is amenable to imple-
              | mentation due to the constant test basis and will lead to consistency of the reduced functionals and
              | their gradients. For the LSPG reduced-order models, the minimum-residual sensitivity and adjoint
              | approximations in (4.28) and (4.56) will be employed (only guarantees consistency of functionals
              | and gradients at trust region centers). Finally, all numerical experiments use the following trust
              | region parameters:
blank         | 
text          |                           Œ∫œë = 0.5        Œ∫œï = 2.0   Œ≥ = 0.5      Œ∑1 = 0.25 Œ∑2 = 0.75
              |                                                                                                        (5.57)
              |                                          rk = 1/(k + 1)       ‚àÜ0 = 10‚àí1         ‚àÜmax = 105 .
blank         | 
              | 
text          |   3 The interior point method based on a BFGS unconstrained solver of Algorithm 3 is replaced with a Newton-CG
              | unconstrained solver for fair comparison to the second-order Steihaug-Toint CG method.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                                               139
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                       ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           ¬µ2
              |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                ¬µ1                                                  ¬µ1                                                  ¬µ1
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                       ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           ¬µ2
              |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                ¬µ1                                                  ¬µ1                                                  ¬µ1
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                       ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           ¬µ2
blank         | 
              | 
              | 
              | 
text          |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                ¬µ1                                                  ¬µ1                                                  ¬µ1
blank         | 
              | 
              | 
              | 
text          | Figure 5.3: Contour of the reduced objective function f (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) in (5.55) in the ¬µ1 ‚àí ¬µ2
              | plane corresponding to a slice at ¬µ3 = 0.0. The reduced-order model employs a Galerkin projection
              | and the trial basis is constructed from: (top) the primal solution at ¬µ0 , i.e., col(Œ¶) = span{u(¬µ0 )};
              | (middle) the primal and adjoint solution at ¬µ0 , i.e., col(Œ¶)
              |                                                             = span{u(¬µ0      ), Œª(¬µ0 )}; (bottom) the
              |                                                                      ‚àÇu
              | primal and sensitivity solution at ¬µ0 , i.e., col(Œ¶) = span u(¬µ0 ),     (¬µ ) . The green shaded re-
              |                                                                     ‚àÇ¬µ 0
              | gion indicates the areas where: (left) the Euclidean ball is bounded by 0.5, i.e., k¬µ ‚àí ¬µ0 k ‚â§ 0.5,
              | (center) the error between the true and reduced objective function is bounded by 100, i.e.,
              | |f (u(¬µ), ¬µ) ‚àí f (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)| ‚â§ 100, and (right) the residual norm of the reconstructed
              | ROM solution is bounded by 10, i.e., kr(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)k ‚â§ 10. The initial condition for the
              | optimization problem and target solution are shown with a red circle and blue square, respectively.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                                               140
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                       ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           ¬µ2
              |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                ¬µ1                                                  ¬µ1                                                  ¬µ1
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                       ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           ¬µ2
              |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                ¬µ1                                                  ¬µ1                                                  ¬µ1
blank         | 
              | 
              | 
              | 
text          |        1.4                                                 1.4                                                 1.4
blank         | 
text          |        1.2                                                 1.2                                                 1.2
blank         | 
text          |        1.0                                                 1.0                                                 1.0
blank         | 
text          |        0.8                                                 0.8                                                 0.8
              |   ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                       ¬µ2
blank         | 
              | 
              | 
              | 
text          |                                                                                                           ¬µ2
blank         | 
              | 
              | 
              | 
text          |        0.6                                                 0.6                                                 0.6
blank         | 
text          |        0.4                                                 0.4                                                 0.4
blank         | 
text          |        0.2                                                 0.2                                                 0.2
blank         | 
text          |        0.0                                                 0.0                                                 0.0
              |              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0              0.0   0.2   0.4        0.6   0.8   1.0
              |                                ¬µ1                                                  ¬µ1                                                  ¬µ1
blank         | 
              | 
              | 
              | 
text          | Figure 5.4: Contour of the reduced objective function f (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ) in (5.55) in the ¬µ1 ‚àí ¬µ2
              | plane corresponding to a slice at ¬µ3 = 0.0. The reduced-order model employs a LSPG projection
              | and the trial basis is constructed from: (top) the primal solution at ¬µ0 , i.e., col(Œ¶) = span{u(¬µ0 )};
              | (middle) the primal and adjoint solution at ¬µ0 , i.e., col(Œ¶)
              |                                                             = span{u(¬µ0      ), Œª(¬µ0 )}; (bottom) the
              |                                                                      ‚àÇu
              | primal and sensitivity solution at ¬µ0 , i.e., col(Œ¶) = span u(¬µ0 ),     (¬µ ) . The green shaded re-
              |                                                                     ‚àÇ¬µ 0
              | gion indicates the areas where: (left) the Euclidean ball is bounded by 0.5, i.e., k¬µ ‚àí ¬µ0 k ‚â§ 0.5,
              | (center) the error between the true and reduced objective function is bounded by 100, i.e.,
              | |f (u(¬µ), ¬µ) ‚àí f (Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)| ‚â§ 100, and (right) the residual norm of the reconstructed
              | ROM solution is bounded by 10, i.e., kr(Œ¶ur (¬µ; Œ¶, Œ®), ¬µ)k ‚â§ 10. The initial condition for the
              | optimization problem and target solution are shown with a red circle and blue square, respectively.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                        141
blank         | 
              | 
              | 
text          | Table 5.1: Variants of the multifidelity trust region method based on projection-based reduced-
              | order models introduced in Algorithms 11 and 12. The first three methods are not guaranteed to be
              | globally convergent since they do not necessarily satisfy the gradient condition (3.15). The methods
              | that employ the traditional trust region employ two trust region subproblem solvers: an exact
              | solver based on the interior point method in Algorithm 3 and the inexact Steihaug-Toint CG solver.
              | The methods that employ the residual-based trust region rely on the exact interior point solver
              | in Algorithm 3. The interior point solver considered in this section uses Newton-CG to solve the
              | unconstrained subproblem (instead of BFGS) for fair comparison with the second-order Steihaug-
              | Toint CG. The snapshot matrices Uk , Wk , Zk consist of state, sensitivity, and adjoint snapshots,
              | respectively, of the high-dimensional model at all previous trust region centers, i.e., ¬µ0 , . . . , ¬µk‚àí1 .
blank         | 
text          |           Name            Reduced basis (Œ¶k )             TR constraint (œëk (¬µ))          TR solver
              |  prim-etr-intpt           PODSP(u(¬µk ), Uk )                residual-based (5.3)      Intpt Newton-CG
              |  prim-ctr-intpt           PODSP(u(¬µk ), Uk )                     k¬µ ‚àí ¬µk k            Intpt Newton-CG
              |   prim-ctr-stcg           PODSP(u(¬µk ), Uk )                     k¬µ ‚àí ¬µk k            Steihaug-Toint CG
              |                                        ‚àÇu
              |  sens-etr-intpt    PODHSP(u(¬µk ), Uk , ‚àÇ¬µ (¬µk ), Wk )       residual-based (5.3)      Intpt Newton-CG
              |                                        ‚àÇu
              |  sens-ctr-intpt    PODHSP(u(¬µk ), Uk , ‚àÇ¬µ (¬µk ), Wk )            k¬µ ‚àí ¬µk k            Intpt Newton-CG
              |                                        ‚àÇu
              |   sens-ctr-stcg    PODHSP(u(¬µk ), Uk , ‚àÇ¬µ (¬µk ), Wk )            k¬µ ‚àí ¬µk k            Steihaug-Toint CG
              |    adj-etr-intpt    PODHSP(u(¬µk ), Uk , Œªk (¬µk ), Zk )      residual-based (5.3)      Intpt Newton-CG
              |    adj-ctr-intpt    PODHSP(u(¬µk ), Uk , Œªk (¬µk ), Zk )           k¬µ ‚àí ¬µk k            Intpt Newton-CG
              |     adj-ctr-stcg    PODHSP(u(¬µk ), Uk , Œªk (¬µk ), Zk )           k¬µ ‚àí ¬µk k            Steihaug-Toint CG
blank         | 
              | 
text          |    The convergence history of the methods in Table 5.1, in terms of the objective function and gra-
              | dient decrease, is provided in Figures 5.5 for reduced-order models that employ a Galerkin projection
              | and 5.6 for reduced-order models that employ an LSPG projection. The convergence of the baseline
              | solver, an L-BFGS linesearch method (without model reduction), is also included in the figures for
              | comparison. All of methods in Table 5.1 based on Galerkin ROMs converge to a first-order critical
              | point of tolerance at least 10‚àí4 (9 orders of magnitude reduction from the initial control), even
              | though global convergence cannot be rigorously established for the methods that build the reduced
              | basis from only primal snapshots (‚Äòprim-etr-intpt‚Äô, ‚Äòprim-ctr-intpt‚Äô, ‚Äòprim-ctr-stcg‚Äô). In contrast, the
              | methods based on LSPG ROMs that build the reduced basis from primal and adjoint snapshots
              | (‚Äòadj-etr-intpt‚Äô, ‚Äòadj-ctr-intpt‚Äô, ‚Äòadj-ctr-stcg‚Äô) do not converge. These methods are supposed to be
              | globally convergent since the inclusion of adjoint snapshots ensures the error conditions (3.14) and
              | (3.15) holds. The failure of these methods is attributed to failed trust region subproblem solves that
              | results from using inconsistent gradients away from trust region centers. Figures 5.5 and 5.6 lead to
              | two more observations. First, all methods converge faster, in terms of major iterations, when exact
              | trust region subproblem solvers are used. Later in this section, the convergence rate will be assessed
              | in terms of a cost metric that accounts for the cost of each major iteration in the respective methods.
              | Second, the methods that include more information exhibit faster convergence. For example, the
              | methods that incorporate sensitivity information converge faster than those that incorporate adjoint
              | information which converge faster than those that consider solely primal snapshots. In addition to
              | the sensitivities providing more information than adjoints (there are 3 sensitivities and 1 adjoint for
              | this problem), the information is also richer since they equip the basis with first order information
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                  142
blank         | 
              | 
              | 
text          |                      104                                                                    105
blank         | 
              | 
              | 
              | 
text          |                                                                       k‚àáf (u(¬µk ), ¬µk )k
              | f (u(¬µk ), ¬µk )
              |                     10‚àí6                                                                    100
blank         | 
              | 
text          |                    10‚àí16                                                                   10‚àí5
blank         | 
              | 
text          |                    10‚àí26                                                               10‚àí10
              |                            0     10         20         30       40                                0   10         20         30         40
              |                                       Major iterations                                                     Major iterations
blank         | 
text          | Figure 5.5: Convergence history of various optimization solvers for optimal control of the inviscid
              | Burgers‚Äô equation when Galerkin reduced-order model defines the approximation model. Optimiza-
              | tion solvers considered: L-BFGS solver with only HDM evaluations (               ), prim-etr-intpt (     ),
              | prim-ctr-intpt (     ), prim-ctr-stcg (     ), sens-etr-intpt (    ), sens-ctr-intpt (    ), sens-ctr-stcg
              | (    ), adj-etr-intpt (    ), adj-ctr-intpt (    ), adj-ctr-stcg (    ).
blank         | 
              | 
text          | [52, 210].
              |                   The increased convergence rate, in terms of major iterations (and therefore HDM evaluations),
              | of Algorithms 11 and 12 comes at the price of a large number of ROM evaluations. Figure 5.7 shows
              | the cumulative number of primal ROM queries as a function of major iteration and a histogram
              | of the number of primal ROM evaluations at a given reduced basis size (ku ). The methods that
              | use the residual-based trust region constraint constitute more difficult trust region subproblems and
              | require more ROM evaluations than those that use a traditional trust region. The benefit of using
              | the residual-based trust region is fewer major iterations, and thus HDM queries (Figures 5.5 and
              | 5.6). Another observation is that, as expected, the inexact trust region solver (Steihaug-Toint CG)
              | requires far fewer ROM queries than the exact solver (interior point Newton-CG), at the cost of
              | additional major iterations (HDM evaluations).
              |                   To assess the speedups that can be realized by the variants of the proposed ROM-based trust
              | region methods in Table 5.1, the following simplified cost model is introduced
blank         | 
text          |                                                  C = nhp + nhs + œÑ ‚àí1 (nrp + nrs )                                          (5.58)
blank         | 
text          | where C is the total cost associated with a particular method in the units of equivalent number of
              | primal HDM queries, nhp is the number of primal HDM queries, nhs is the number of sensitivity
              | HDM queries, nrp is the number of primal ROM queries, nrs is the number of sensitivity ROM
              | queries, and œÑ is the ratio of the cost of a primal HDM query to a primal ROM query. This cost
              | model assume the cost of computing the primal HDM (ROM) solution is the same as computing all
              | three sensitivities. Under this cost model, Figure 5.8 contains the convergence rates of the various
              | algorithms as a function of cost for three values of œÑ : two moderate values for the expected speedup
              | of the reduced-order model (œÑ = 20, 50) and the asymptotic case of a free reduced-order model
              | (œÑ = ‚àû). All variants of the trust region method outperform the baseline L-BFGS method, with
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                      143
blank         | 
              | 
              | 
text          |                            104                                                                 105
blank         | 
              | 
              | 
              | 
text          |                                                                          k‚àáf (u(¬µk ), ¬µk )k
              |  f (u(¬µk ), ¬µk )
              |                           10‚àí6                                                                 100
blank         | 
              | 
text          |                          10‚àí16                                                                10‚àí5
blank         | 
              | 
text          |                          10‚àí26                                                            10‚àí10
              |                                  0       10         20         30   40                                0   10         20         30         40
              |                                               Major iterations                                                 Major iterations
blank         | 
text          | Figure 5.6: Convergence history of various optimization solvers for optimal control of the inviscid
              | Burgers‚Äô equation when LSPG reduced-order model defines the approximation model. Optimization
              | solvers considered: L-BFGS solver with only HDM evaluations (                ), prim-etr-intpt (    ), prim-
              | ctr-intpt (     ), prim-ctr-stcg (      ), sens-etr-intpt (   ), sens-ctr-intpt (    ), sens-ctr-stcg (    ),
              | adj-etr-intpt (      ), adj-ctr-intpt (     ), adj-ctr-stcg (   ).
blank         | 
text          |                          800
              | number of ROM queries
blank         | 
              | 
              | 
              | 
text          |                          600                                                             400
blank         | 
text          |                          400
              |                                                                                          200
              |                          200
blank         | 
text          |                            0
              |                                                                                               0
              |                                  0   2           4      6      8                                  0            5           10
              |  number of ROM queries
blank         | 
              | 
              | 
              | 
text          |                          103
              |                                                                                           102
              |                          102
blank         | 
text          |                                                                                           101
              |                          101
blank         | 
              | 
text          |                                  0   10       20      30       40                                     0   20       40                60
              |                                           Major iterations                                                ROM size (ku )
blank         | 
              | 
text          | Figure 5.7: Left: Cumulative number of primal ROM queries as a function of major iteration in the
              | trust region algorithm based on reduced-order models (Algorithm 11) as applied to optimal control
              | of the inviscid Burgers‚Äô equation. Right: Histogram of the number of primal ROM queries at a
              | given basis size. Data separated into the top and bottom rows to deal with the disparate x-scales.
              | All reduced-order models use a Galerkin projection. Optimization solvers considered: prim-etr-
              | intpt (     ), prim-ctr-intpt (    ), prim-ctr-stcg (      ), sens-etr-intpt (    ), sens-ctr-intpt ( ),
              | sens-ctr-stcg (    ), adj-etr-intpt (    ), adj-ctr-intpt (     ), adj-ctr-stcg (    ).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     144
blank         | 
              | 
              | 
text          | the variants based on exact trust region solvers (‚Äòsens-etr-intpt‚Äô and ‚Äòsens-ctr-intpt‚Äô) outperforming
              | the inexact solver (‚Äòsens-ctr-intpt‚Äô), even if ROM queries are only 20√ó faster than HDM queries.
              | Depending on the speedup of the ROM, a given value of the objective function or gradient can be
              | achieved by methods ‚Äòsens-etr-intpt‚Äô or ‚Äòsens-ctr-intpt‚Äô at roughly 10 ‚àí 50% the cost required by the
              | baseline method.
              |    The section closes with a study of the convergence behavior of the trust region method that uses
              | a residual-based trust region constraint when Algorithms 11 (fully converged solutions as snapshots
              | and for trust region assessment) and 12 (partially converged solutions as snapshots and for trust
              | region assessment) are used. Figure 5.9 contains the convergence history of the objective function
              | and approximation model at trust region centers and candidate steps. Figures 5.10 and 5.11 contain
              | the same information for the gradient and trust region constraint, respectively. From these figures,
              | the model is first-order consistent at trust region centers for Algorithm 11 (left plots) since the
              | basis is constructed with the span-preserving variant of POD (Algorithm 7) and uses fully converged
              | snapshots. This is not the case for Algorithm 12 (right plots) that uses partially converged snapshots.
              | Despite relatively poor agreement of the model and objective (and the corresponding gradients) at
              | trust region centers and candidate steps, rapid progress is made toward the optimal solution. From
              | Figure 5.11, the trust region constraints are active at early iterations of the trust region algorithm
              | and inactive later. This suggests that, as the optimal solution is approached, the reduced-order
              | model is only queried in regions of the parameter space where it is very accurate, i.e., near training
              | points. Finally, Algorithm 12 requires one additional iteration than Algorithm 11 to converge to a
              | similar tolerance. This is expected since Algorithm 12 utilizes partially converged solutions in the
              | construction of the reduced basis, an additional level of inexactness. These observations are verified
              | in Tables 5.2‚Äì5.5 that contains the convergence history of the relevant trust region quantities for the
              | variants ‚Äòsens-etr-intpt‚Äô and ‚Äòsens-ctr-stcg‚Äô of Algorithms 11 and 12.
blank         | 
              | 
title         | 5.5.2    Optimal Control of 1D Viscous Burgers‚Äô Equation
text          | The investigation into the methods introduced in this chapter continues in this section with an
              | emphasis on problems where the number of parameters is sufficiently large that gradients must be
              | computed with the adjoint method. The model PDE-constrained optimization problem considered is
              | optimal control of the steady, viscous, one-dimensional Burgers‚Äô equation. The optimization problem
              | takes the form
              |                                Z     1                                     1                 
              |                                           1                         Œ±
              |                                                                         Z
              |                    minimize                 (u(¬µ, x) ‚àí uÃÑ(x))2 dx +             z(¬µ, x)2 dx       (5.59)
              |                        n¬µ
              |                      ¬µ‚ààR          0       2                         2   0
blank         | 
              | 
text          | where u(¬µ, x) is the solution of the viscous Burgers‚Äô equation with a general parametrization of the
              | control z(¬µ, x)
blank         | 
text          |                      ‚àíŒΩ‚àÇxx u(¬µ, x) + u(¬µ, x)‚àÇx u(¬µ, x) = z(¬µ, x) x ‚àà (0, 1)
              |                                                                                                   (5.60)
              |                                            u(¬µ, 0) = 1      u(¬µ, 1) = 0
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                               145
blank         | 
              | 
              | 
              | 
text          |                     104                                                                     105
blank         | 
              | 
              | 
              | 
text          |                                                                       k‚àáf (u(¬µk ), ¬µk )k
              | f (u(¬µk ), ¬µk )
blank         | 
              | 
              | 
              | 
text          |                   10‚àí6                                                                      100
blank         | 
              | 
text          |                   10‚àí16                                                                    10‚àí5
blank         | 
              | 
text          |                   10‚àí26                                                                10‚àí10
              |                           0   20    40      60        80        100                               0   20    40      60        80        100
blank         | 
text          |                     104                                                                     105
blank         | 
              | 
              | 
              | 
text          |                                                                       k‚àáf (u(¬µk ), ¬µk )k
              | f (u(¬µk ), ¬µk )
blank         | 
              | 
              | 
              | 
text          |                   10‚àí6                                                                      100
blank         | 
              | 
text          |                   10‚àí16                                                                    10‚àí5
blank         | 
              | 
text          |                   10‚àí26                                                                10‚àí10
              |                           0   20     40          60        80                                     0   20     40          60        80
blank         | 
text          |                     104                                                                     105
              |                                                                       k‚àáf (u(¬µk ), ¬µk )k
              | f (u(¬µk ), ¬µk )
blank         | 
              | 
              | 
              | 
text          |                   10‚àí6                                                                      100
blank         | 
              | 
text          |                   10‚àí16                                                                    10‚àí5
blank         | 
              | 
text          |                   10‚àí26                                                                10‚àí10
              |                           0    20     40         60         80                                    0    20     40         60         80
              |                                      Cost                                                                    Cost
blank         | 
text          | Figure 5.8: Convergence of the objective function (left) and gradient (right) as a function of the cost
              | metric in (5.58) for several values of the speedup factor of the reduced-order model: œÑ = 20 (top row),
              | œÑ = 50 (middle row), œÑ = ‚àû (bottom row) for optimal control of the inviscid Burgers‚Äô equation. All
              | reduced-order models use a Galerkin projection. Optimization solvers considered: L-BFGS solver
              | with only HDM evaluations (         ), sens-etr-intpt (    ), sens-ctr-intpt (  ), sens-ctr-stcg (  ).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                     146
blank         | 
              | 
              | 
              | 
text          |    105                                                      105
blank         | 
              | 
              | 
text          | 10‚àí10                                                    10‚àí10
blank         | 
              | 
              | 
text          | 10‚àí25                                                    10‚àí25
              |          0       1         2         3         4                  0    1      2        3        4          5
              |                      Major iteration                                        Major iteration
blank         | 
              | 
text          | Figure 5.9: Convergence history of the objective quantities for optimal control of the inviscid Burgers‚Äô
              | equation using Algorithm 11 (left ‚Äì fully converged solutions as snapshots and in the evaluation of
              | trust region steps) and Algorithm 12 (right ‚Äì partially converged solutions as snapshots and in the
              | evaluation of trust region steps): F (¬µk ) (   ), F (¬µÃÇk ) (   ), mk (¬µk ) (    ), mk (¬µÃÇk ) (   ). The
              | variant ‚Äòsens-etr-intpt‚Äô (Table 5.1) of the multifidelity trust region algorithm with Galerkin-based
              | reduced-order models is used. Since the approximation model in the left plot is first-order consistent
              | at trust region centers, mk (¬µk ) is omitted.
blank         | 
              | 
              | 
              | 
text          |    106                                                      106
blank         | 
              | 
              | 
text          |  10‚àí3                                                     10‚àí3
blank         | 
              | 
              | 
text          | 10‚àí12                                                    10‚àí12
              |          0       1         2         3         4                  0    1      2        3        4          5
              |                      Major iteration                                        Major iteration
blank         | 
              | 
text          | Figure 5.10: Convergence history of the gradient quantities for optimal control of the inviscid Burg-
              | ers‚Äô equation using Algorithm 11 (left ‚Äì fully converged solutions as snapshots and in the evaluation
              | of trust region steps) and Algorithm 12 (right ‚Äì partially converged solutions as snapshots and
              | in the evaluation of trust region steps): k‚àáF (¬µk )k (       ), k‚àáF (¬µÃÇk )k (   ), k‚àámk (¬µk )k (     ),
              | k‚àámk (¬µÃÇk )k (      ). The variant ‚Äòsens-etr-intpt‚Äô (Table 5.1) of the multifidelity trust region algo-
              | rithm with Galerkin-based reduced-order models is used. Since the approximation model in the left
              | plot is first-order consistent at trust region centers, k‚àámk (¬µk )k is omitted.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                   147
blank         | 
              | 
              | 
text          |           101                                                             101
blank         | 
              | 
              | 
text          |   10‚àí6                                                                   10‚àí6
blank         | 
              | 
              | 
text          | 10‚àí13                                                               10‚àí13
              |                 0         1         2         3       4                         0    1      2        3       4            5
              |                               Major iteration                                             Major iteration
blank         | 
              | 
text          | Figure 5.11: Convergence history of the constraint quantities for optimal control of the inviscid
              | Burgers‚Äô equation using Algorithm 11 (left ‚Äì fully converged solutions as snapshots and in the
              | evaluation of trust region steps) and Algorithm 12 (right ‚Äì partially converged solutions as snapshots
              | and in the evaluation of trust region steps): œëk (¬µk ) (      ), œëk (¬µÃÇk ) ( ), ‚àÜk (   ). The variant
              | ‚Äòsens-etr-intpt‚Äô (Table 5.1) of the multifidelity trust region algorithm with Galerkin-based reduced-
              | order models is used.
blank         | 
text          |            8
              |                                                                           1.5
              |            6
              |                                                                u(¬µ, x)
              | z(¬µ, x)
blank         | 
              | 
              | 
              | 
text          |                                                                            1
              |            4
              |            2                                                              0.5
              |            0                                                               0
              |                0    0.2        0.4       0.6   0.8   1                          0   0.2    0.4       0.6    0.8       1
              |                                      x                                                           x
blank         | 
              | 
text          | Figure 5.12: Control (left) and corresponding solution (right) of the viscous Burgers‚Äô equation in
              | (5.60) at: the initial guess for the optimization problem (   ) and the optimal solution of (5.59)
              | (    ).
blank         | 
              | 
text          | and uÃÑ(x) is the target state. The viscosity is fixed at ŒΩ = 10‚àí2 and the PDE is discretized with
              | 1000 linear finite elements for a state space of dimension Nu = 999, after application of the essential
              | boundary conditions. The target state is chosen as the constant solution uÃÑ(x) ‚â° 1, which is not
              | reachable due to the boundary conditions on the PDE. The control is parametrized with 50 cubic
              | splines with clamped boundary conditions for a total of 53 optimization variables4 , i.e., N¬µ = 53.
              | The control is parametrized in this way, instead of the standard approach [78, 96, 108, 109] of
              | interpolating the control using the underlying finite element shape functions to avoid a parameter
              | space whose dimension is comparable to that of the state space, i.e., N¬µ = O(Nu ), as this case
              | requires special consideration (Appendix C).
              |           Even though the number of parameters does not scale with the dimension of the state space,
              | the large number of parameter (N¬µ = 53) calls for the adjoint approach to compute gradients of
              |    4 The optimization variables are the value of each spline knot (the location of each knot is fixed) and the slope of
blank         | 
meta          | the curve its boundaries.
              | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                      148
blank         | 
              | 
              | 
              | 
text          | |f (u(¬µk ), ¬µk ) ‚àí f (u(¬µ‚àó ), ¬µ‚àó )|
              |                                   10‚àí1                                                             10‚àí1
blank         | 
              | 
              | 
              | 
text          |                                                                                   k‚àáf (u(¬µk ), ¬µk )k
              |                                   10‚àí3                                                             10‚àí3
blank         | 
              | 
text          |                                   10‚àí5                                                             10‚àí5
blank         | 
              | 
text          |                                   10‚àí7                                                             10‚àí7
blank         | 
text          |                                          0   10     20      30     40     50                              0   10     20      30     40      50
              |                                                   Major iterations                                                 Major iterations
blank         | 
text          | Figure 5.13: Convergence history of various optimization solvers for optimal control of the viscous
              | Burgers‚Äô equation when Galerkin reduced-order model defines the approximation model. Optimiza-
              | tion solvers considered: L-BFGS solver with only HDM evaluations (          ), adj-etr-intpt (    ),
              | adj-ctr-intpt (   ), adj-ctr-stcg (  ).
blank         | 
              | 
text          | quantities of interest; therefore, this section only studies ‚Äòadj-etr-intpt‚Äô, ‚Äòadj-ctr-intpt‚Äô, and ‚Äòadj-ctr-
              | stcg‚Äô from Table 5.1. Furthermore, this section only considers reduced-order models based on a
              | Galerkin projection to ensure consistent gradients, which is a particularly important consideration
              | when the number of parameters is large. The convergence of these methods, as a function of
              | major iteration, is provided in Figure 5.13, along with the convergence of the baseline method that
              | uses an L-BFGS method (without model reduction). The trust region method with a residual-based
              | constraint converges most rapidly and, similar to the previous section, the methods that employ exact
              | trust region solvers outperform the inexact Steihaug-Toint CG solver. In fact, the method based on
              | the Steihaug-Toint CG solver is converging; however, after the maximum number of iterations (50)
              | the iterates are not close enough to the solution for quadratic convergence to be realized and does
              | not converge to the same tolerance as the other methods.
              |                         The increased convergence rate, in terms of major iterations (and therefore HDM evaluations),
              | of Algorithm 11 comes at the price of a large number of ROM evaluations. Figure 5.14 shows the
              | cumulative number of primal ROM queries as a function of major iteration and a histogram of the
              | number of primal ROM evaluations at a given reduced basis size (ku ). Similar to the previous
              | section, the inexact solver requires far fewer ROM queries than the exact solvers. However, unlike
              | the previous section, the number of ROM queries required by the residual-based trust region and
              | traditional trust region are not significantly different.
              |                         To assess the speedups that can be realized by the variants of the proposed ROM-based trust
              | region methods in Table 5.1, the following simplified cost model is introduced
blank         | 
text          |                                                           C = nhp + nha /2 + œÑ ‚àí1 (nrp + nra /2)                                   (5.61)
blank         | 
text          | where C is the total cost associated with a particular method in the units of equivalent number of
              | primal HDM queries, nhp is the number of primal HDM queries, nha is the number of adjoint HDM
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                           149
blank         | 
              | 
              | 
              | 
text          | number of ROM queries
blank         | 
              | 
              | 
              | 
text          |                                                                                   150
              |                                  1,000
blank         | 
              | 
              | 
text          |                                         500                                       100
blank         | 
              | 
              | 
text          |                                           0
              |                                                                                   50
              |                                               0   2        4     6     8     10         0       5        10          15          20
              |                 number of ROM queries
blank         | 
              | 
              | 
              | 
text          |                                         150
              |                                                                                    8
              |                                         100
              |                                                                                    6
              |                                          50
              |                                                                                    4
              |                                           0
              |                                               0   10      20     30     40   50             0       20        40            60
              |                                                        Major iterations                             ROM size (ku )
blank         | 
              | 
text          | Figure 5.14: Left: Cumulative number of primal ROM queries as a function of major iteration in the
              | trust region algorithm based on reduced-order models (Algorithm 11) as applied to optimal control
              | of the viscous Burgers‚Äô equation. Right: Histogram of the number of primal ROM queries at a
              | given basis size. Data separated into the top and bottom rows to deal with the disparate x-scales.
              | All reduced-order models use a Galerkin projection. Optimization solvers considered: adj-etr-intpt
              | (    ), adj-ctr-intpt (  ), adj-ctr-stcg (   ).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    150
blank         | 
              | 
              | 
text          | queries, nrp is the number of primal ROM queries, nra is the number of adjoint ROM queries, and œÑ
              | is the ratio of the cost of a primal HDM query to a primal ROM query. This cost model assume the
              | cost of computing the primal HDM (ROM) solution is twice that of computing an adjoint solution.
              | Under this cost model, Figure 5.15 contains the convergence rates of the various algorithms as a
              | function of cost for three values of œÑ : two moderate values for the expected speedup of the reduced-
              | order model (œÑ = 50, 100) and the asymptotic case of a free reduced-order model (œÑ = ‚àû). The
              | variants of the trust region method based on the exact trust region solver (‚Äòadj-etr-intpt‚Äô and ‚Äòadj-
              | ctr-intpt‚Äô) outperform the baseline L-BFGS method, even if ROM queries are only 50√ó faster than
              | HDM queries. Depending on the speedup of the ROM, a given value of the objective function or
              | gradient can be achieved by methods ‚Äòadj-etr-intpt‚Äô at less than 50% the cost required by the baseline
              | method.
              |    This section closes with a study of the convergence behavior of the trust region method that
              | uses a residual-based trust region constraint. Figure 5.16 contains the convergence history of the
              | objective function and approximation model (left) and their gradients (right) at trust region centers
              | and candidate steps. The approximation model is first-order consistent at trust region centers
              | since the basis is constructed with the span-preserving variant of POD (Algorithm 7) and uses
              | fully converged snapshots. Despite relatively poor agreement of the model and objective (and the
              | corresponding gradients) at the candidate steps, rapid progress is made toward the optimal solution.
              | These observations are verified in Tables 5.6‚Äì5.7 that contains the convergence history of the relevant
              | trust region quantities for methods ‚Äòadj-etr-intpt‚Äô and ‚Äòadj-ctr-intpt‚Äô.
blank         | 
              | 
title         | 5.5.3     Shape Optimization of Airfoil in Inviscid, Subsonic Flow
text          | In this section, we consider the inverse shape design of an airfoil in inviscid, subsonic flow: given
              | only the pressure distribution of a target shape‚Äîthe RAE2822 airfoil, in this case‚Äîthe goal is to use
              | shape optimization to recover the underlying shape. The initial guess for the optimization problem
              | is the symmetric NACA0012 airfoil.
blank         | 
title         | Shape parametrization and problem setup
blank         | 
text          | A plethora of shape parametrization techniques exist [177, 9], each with strengths and weaknesses.
              | They typically trade-off between efficiency and flexibility. A subset of these techniques have been
              | studied in the context of model order reduction [174]. In this work, the SDESIGN software [129, 127,
              | 128], based on the design element approach [99, 61], is used for shape parametrization (Section 2.1.2).
              | Here, a single ‚Äúcubic‚Äù design element is used to parametrize the deformation of the NACA0012 airfoil.
              | Such a design element has 8 control nodes. They are used to define cubic Lagrangian polynomials
              | to describe the displacement field along the horizontal edges of the element, and linear functions
              | to define the displacement field along its vertical edges. For this application, the set of admissible
              | shapes is further restricted by constraining the control nodes to move in the vertical direction only.
              | This results in a parametrization with 8 variables where each of them represents the displacement
              | of a control node in the vertical direction. The case where all parameters are equal, ¬µ = c1 for
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                    151
blank         | 
              | 
              | 
              | 
text          | |f (u(¬µk ), ¬µk ) ‚àí f (u(¬µ‚àó ), ¬µ‚àó )|
              |                                   10‚àí1                                                10‚àí1
blank         | 
              | 
              | 
              | 
text          |                                                                      k‚àáf (u(¬µk ), ¬µk )k
              |                                   10‚àí3                                                10‚àí3
blank         | 
              | 
text          |                                   10‚àí5                                                10‚àí5
blank         | 
              | 
text          |                                   10‚àí7                                                10‚àí7
blank         | 
text          |                                          0   50          100   150                           0   50          100    150
              | |f (u(¬µk ), ¬µk ) ‚àí f (u(¬µ‚àó ), ¬µ‚àó )|
blank         | 
              | 
              | 
              | 
text          |                                   10‚àí1                                                10‚àí1
blank         | 
              | 
              | 
              | 
text          |                                                                      k‚àáf (u(¬µk ), ¬µk )k
              |                                   10‚àí3                                                10‚àí3
blank         | 
              | 
text          |                                   10‚àí5                                                10‚àí5
blank         | 
              | 
text          |                                   10‚àí7                                                10‚àí7
blank         | 
text          |                                          0   50          100   150                           0   50          100    150
              | |f (u(¬µk ), ¬µk ) ‚àí f (u(¬µ‚àó ), ¬µ‚àó )|
blank         | 
              | 
              | 
              | 
text          |                                   10‚àí1                                                10‚àí1
              |                                                                      k‚àáf (u(¬µk ), ¬µk )k
blank         | 
              | 
              | 
              | 
text          |                                   10‚àí3                                                10‚àí3
blank         | 
              | 
text          |                                   10‚àí5                                                10‚àí5
blank         | 
              | 
text          |                                   10‚àí7                                                10‚àí7
blank         | 
text          |                                          0   50          100   150                           0   50          100    150
              |                                                   Cost                                                Cost
blank         | 
text          | Figure 5.15: Convergence of the objective function (left) and gradient (right) as a function of the
              | cost metric in (5.61) for several values of the speedup factor of the reduced-order model: œÑ = 50
              | (top row), œÑ = 100 (middle row), œÑ = ‚àû (bottom row) for optimal control of the viscous Burgers‚Äô
              | equation. All reduced-order models use a Galerkin projection. Optimization solvers considered: L-
              | BFGS solver with only HDM evaluations (        ), adj-etr-intpt ( ), adj-ctr-intpt (  ), adj-ctr-stcg
              | (    ).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                              152
blank         | 
              | 
              | 
              | 
text          | 10‚àí2                                                          10‚àí2
blank         | 
              | 
text          | 10‚àí5                                                          10‚àí5
blank         | 
              | 
text          | 10‚àí8                                                          10‚àí8
              |        0      2      4      6       8        10                      0      2      4      6       8        10
              |                     Major iteration                                               Major iteration
blank         | 
text          | Figure 5.16: Convergence history of the objective (left) and gradient (right) quantities for optimal
              | control of the viscous Burgers‚Äô equation using Algorithm 11 (fully converged solutions as snapshots
              | and in the evaluation of trust region steps). Left: |F (¬µk ) ‚àí F (¬µ‚àó )| (   ), |F (¬µÃÇk ) ‚àí F (¬µ‚àó )| (   ),
              | |mk (¬µÃÇk ) ‚àí F (¬µ‚àó )| (   ). Right: k‚àáF (¬µk )k (      ), k‚àáF (¬µÃÇk )k (    ), k‚àámk (¬µÃÇk )k (        ). The
              | variant ‚Äòadj-etr-intpt‚Äô (Table 5.1) of the multifidelity trust region algorithm with Galerkin-based
              | reduced-order models is used. Since the approximation model is first-order consistent at trust region
              | centers mk (¬µk ) and k‚àámk (¬µk )k are omitted.
blank         | 
              | 
text          | c ‚àà R, corresponds to a rigid translation in the vertical direction. Because such a translation does
              | not affect the definition of a shape, it is eliminated by constraining one of the displacement variables
              | to zero. Furthermore, because the control nodes are allowed to move only in the vertical direction,
              | rigid rotations are automatically eliminated. A visualization of the vertices of the design element
              | and the deformation induced by perturbing each design variable is given in Figure 5.17. While
              | SDESIGN is used to deform the surface nodes of the airfoil, a robust mesh motion algorithm based
              | on a structural analogy is used to deform the surrounding body-fitted CFD mesh accordingly.
              |    The flow over the airfoil is modeled using the compressible Euler equations, and these are solved
              | numerically using AERO-F [68]. Because this flow solver is three-dimensional, the two-dimensional
              | fluid domain around the airfoil is represented as a slice of a three-dimensional domain. This slice is
              | discretized using a body-fitted CFD mesh with 54 816 tetrahedra and 19 296 nodes (Figure 5.18a).
              | Specifically, the flow equations are semi-discretized by AERO-F on this CFD mesh using a second-
              | order finite volume method based on Roe‚Äôs flux [169].
              |    For each airfoil configuration generated during the iterative optimization procedure, the steady
              | state solution of the flow problem is computed iteratively using pseudo-transient continuation. For
              | this purpose, each sought-after steady state solution is initialized using the best previously computed
              | steady state solution available in the database5 . The best steady state solution is defined here as that
              | steady state solution available in the database which, for the given airfoil configuration, minimizes
              | the residual of the discretized steady state Euler equations. Because the database of steady state
              | flow solutions is initially empty, the iterative computation of the steady state flow over the initial
              | shape‚Äîin this case, that of the NACA0012 airfoil‚Äîis initialized with the uniform flow solution.
              |    The trust region method described in this chapter that employs ROMs as the approximation
              | model is used to solve the aerodynamic shape optimization problem. At each HDM sample, the
              |    5 In this context, the database refers to the flow solutions computed for all shapes previously visited by the
blank         | 
text          | optimization trajectory.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                   153
blank         | 
              | 
              | 
              | 
text          |                        (a) ¬µ1 = 0.1                                 (b) ¬µ2 = 0.1
blank         | 
              | 
              | 
              | 
text          |                        (c) ¬µ3 = 0.1                                 (d) ¬µ4 = 0.1
blank         | 
              | 
              | 
              | 
text          |                        (e) ¬µ5 = 0.1                                 (f) ¬µ6 = 0.1
blank         | 
              | 
              | 
              | 
text          |                        (g) ¬µ7 = 0.1                                 (h) ¬µ8 = 0.1
blank         | 
text          | Figure 5.17: Shape parametrization of a NACA0012 airfoil using a cubic design element (the notation
              | ¬µi designates the i-th component of the vector ¬µ which refers to the i-th displacement degree of
              | freedom of the shape parametrization)
blank         | 
              | 
text          | steady state solution and sensitivities with respect to shape parameters are computed and used as
              | snapshots. As the chosen shape parametrization has 8 parameters, 9 snapshots are generated per
              | HDM sample: one snapshot corresponding to the steady state solution and 8 solution sensitivities. A
              | ROB is extracted from these snapshots using the heterogeneous span-preserving variant of the POD
              | method in Algorithm 7. Because very few snapshots are generated for this problem, the truncation
              | step in the POD algorithm is skipped. Consequently, the size of the constructed ROB is ku = 9s,
              | where s is the number of sampled HDMs. The nonlinear least-squares problem describing the ROM
              | is solved using the Gauss-Newton method equipped with a backtracking linesearch algorithm. The
              | python interface to the SNOPT [70] software, pyOpt [151], is used to solve the optimization problem
              | itself.
              |     At this point, it is noted that since the exact profile of the RAE2822 airfoil does not lie in
              | the space of admissible airfoil profiles defined by the cubic design element parametrization, it is
              | approximated by the closest admissible profile. This approximation is referred to in the remainder
              | of this section as the Cub-RAE2822 airfoil. It is graphically depicted in Figure 5.19 which also shows
              | the pressure isolines computed for this airfoil at the free-stream Mach number M‚àû = 0.5 and angle
              | of attack Œ± = 0.0‚ó¶ .
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    154
blank         | 
              | 
              | 
              | 
text          |  (a) CFD mesh for the NACA0012 airfoil               (b) Pressure field (M‚àû = 0.5, Œ± = 0.0‚ó¶ )
              |  (undeformed, 19 296 nodes)
blank         | 
text          |   Figure 5.18: NACA0012 mesh and pressure distribution at Mach 0.5 and zero angle of attack.
blank         | 
              | 
title         | Subsonic inverse design
blank         | 
text          | The free-stream conditions of interest are set to the subsonic Mach number M‚àû = 0.5 and zero
              | angle of attack (Œ± = 0‚ó¶ ), and the following optimization problem is considered
blank         | 
text          |                                             1                              2
              |                             minimize          p(u(¬µ)) ‚àí p(u(¬µRAE2822 ))    2
              |                               ¬µ‚ààRN¬µ         2
              |                                                                                                 (5.62)
              |                             subject to ¬µ3 = 0
              |                                             ¬µl ‚â§ ¬µ ‚â§ ¬µu
blank         | 
text          | where p(u) is the vector of nodal pressures, and ¬µRAE2822 designates the parameter solution vector
              | morphing the NACA0012 airfoil into the Cub-RAE2822 airfoil. The first constraint is introduced
              | to eliminate the rigid body translation in the vertical direction as discussed in the previous section.
              | The box constraints prohibit the optimization trajectory from going through highly distorted shapes
              | that would cause the flow solver to fail.
              |    To obtain a reference solution that can be used for assessing the performance of the proposed
              | ROM-based optimization method, problem (5.62) is first solved using the HDM as the constraining
              | PDE. In this case, the optimizer is found to reduce the initial value of the objective function by
              | 9 orders of magnitude, before numerical difficulties cause it to terminate (Figure 5.20). Relevant
              | statistics associated with this HDM-based reference solution of the optimization problem are gath-
              | ered in Table 5.8. Essentially, 24 optimization iterations are required to obtain a solution with a
              | relative error well below 0.1%. These iterations incur a total of 29 HDM queries (including those
              | associated with the linesearch iterations). Figure 5.21 shows the pressure distribution associated
              | with this reference solution matches the target pressure distribution very well.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                       155
blank         | 
              | 
              | 
              | 
text          |      (a) CFD mesh for the Cub-RAE2822 airfoil            (b) Pressure field (M‚àû = 0.5, Œ± = 0.0‚ó¶ )
blank         | 
text          | Figure 5.19: Cub-RAE2822 mesh and pressure isolines computed at Mach 0.5 and zero angle of
              | attack.
blank         | 
              | 
text          |    Next, the ROM-based trust region method developed in this chapter and summarized in Algo-
              | rithm 11 is applied to solve problem (5.62), which solves a sequence of trust region subproblems of
              | the form
              |                                        1                                    2
              |                           minimize       p(Œ¶k ur (¬µ)) ‚àí p(u(¬µRAE2822 ))     2
              |                            ¬µ‚ààRN¬µ       2
              |                           subject to ¬µ3 = 0
              |                                                                                                     (5.63)
              |                                        ¬µl ‚â§ ¬µ ‚â§ ¬µu
              |                                        1                  2
              |                                          kr(Œ¶k ur (¬µ), ¬µ)k2 ‚â§ ‚àÜk .
              |                                        2
              |    The HDM is sampled at the initial configuration and the resulting 9 snapshots are used to build
              | a ROB using Algorithm 7, without truncation. The resulting ROB is used to construct a reduced-
              | order model based on a LSPG projection and the corresponding minimum-residual sensitivity model
              | to solve (5.63). Indeed, as the minimum-residual sensitivity computation described in Section 4.1.2
              | is not consistent with the true reduced sensitivities for large residuals, convergence of the optimiza-
              | tion problem is not guaranteed. To address this issue, an upper bound is set on the number of
              | optimization iterations (25 in this case) and the goal of the reduced optimization problem is set to
              | finding an improvement to the current solution before updating the ROB. The HDM is sampled at
              | the termination point of each reduced optimization problem yielding 9 additional snapshots which
              | are appended to the ROB using Algorithm 9. Linear independence of the basis is maintained by
              | truncating vectors corresponding to singular values below some tolerance. For the present applica-
              | tion, such truncation was not necessary as the snapshots added to the ROB at a given iteration were
              | not contained in the span of the snapshots from previous iterations.
              |    Using only 7 HDM samples, the progressive ROM optimization framework reduces the initial
              | pressure discrepancy by 18 orders of magnitude, to essentially machine zero. Interestingly, this is 4
              | times fewer HDM queries than required by the HDM-based optimization. Figure 5.21 shows that
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                156
blank         | 
              | 
              | 
              | 
text          |                                      10‚àí1
blank         | 
text          |                                  2
              |                                  2
              |                      RAE2822 ))k2
              |                                  2
              |       k               RAE2822
              |                               ))
              |                                      10‚àí3
              |      2 p(u(¬µ))‚àíp(u(¬µ
blank         | 
title         |      2 kp(u(0))‚àíp(u(¬µ
blank         | 
              | 
              | 
              | 
text          |                                      10‚àí5
blank         | 
              | 
text          |                                      10‚àí7
              |       k
              |      1
              |      1
blank         | 
              | 
              | 
              | 
text          |                                      10‚àí9
blank         | 
text          |                                             0   2   4   6   8       10    12    14    16   18   20   22   24
              |                                                                 Optimization iterations
blank         | 
text          | Figure 5.20: Progression of the objective function during the HDM-based optimization. The initial
              | guess is defined as the 0th optimization iteration.
blank         | 
              | 
text          | both the shape of the airfoil and the associated pressure distribution discovered by the ROM-based
              | optimization method match the target shape and pressure distribution very well.
              |    Surprisingly, the ROM-based optimization process achieves a lower value of the objective function
              | than the HDM-based one. This can be traced to convergence tolerance on the HDM sensitivity
              | analysis. The HDM-based sensitivities are obtained by solving the multiple right-hand side linear
              | system of equations in (2.87) using GMRES. The convergence tolerance is kAx ‚àí bk2 ‚â§ Œ≥ kbk2
              | for solving the linear system of equations Ax = b, with Œ≥ = 10‚àí10 in this case. If kbk is large
              | (b = ‚àÇr/‚àÇ¬µ in this case), the convergence requirement may be rather flexible. Conversely, the
              | minimum-residual ROM sensitivities in (4.28) are solved to machine precision using a direct QR
              | factorization.
              |    Recall from Chapter 4 that the minimum-residual reduced sensitivities approach the true sen-
              | sitivities for LSPG projection as the HDM residual approaches zero. Figure 5.24 verifies that the
              | HDM residual is small after 6 HDM samples are taken, which implies the minimum-residual ROM
              | sensitivities are (nearly) consistent with the true ROM sensitivities. This consistency will guaran-
              | tee convergence of the reduced optimization problem when using a globally convergent optimization
              | solver. Additionally, the small HDM residual implies that the ROM is highly accurate in this region,
              | making it likely that the reduced optimization problem will converge to a point close to the true
              | optimum.
              |    Figure 5.22 reports on the evolution of the objective function with the number of optimization
              | iterations, and marks each new HDM query along the optimization trajectory. The reader can
              | observe that the proposed ROM-based optimization method performs a total of 160 trust region
              | subproblem iterations (Figure 5.23) requiring 346 ROM evaluations (see Table 5.8) and 7 HDM
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                  157
blank         | 
              | 
              | 
text          |           0.6
              |                                                                     Initial
              |                                                                     Target                0.6
              |           0.4
              |                                                              HDM-based optimization
              |                                                              ROM-based optimization
              |           0.2                                                                             0.5
blank         | 
              | 
              | 
              | 
text          |                                                                                                  Distance Transverse to Centerline
              |             0                                                                             0.4
blank         | 
              | 
text          |          ‚àí0.2                                                                             0.3
              |    -Cp
blank         | 
              | 
              | 
              | 
text          |          ‚àí0.4                                                                             0.2
blank         | 
              | 
text          |          ‚àí0.6                                                                             0.1
blank         | 
text          |          ‚àí0.8                                                                             0
blank         | 
text          |           ‚àí1                                                                              ‚àí0.1
blank         | 
text          |          ‚àí1.2
              |                 0   0.1   0.2    0.3    0.4     0.5    0.6      0.7    0.8    0.9     1
              |                                        Distance along airfoil
blank         | 
text          | Figure 5.21: Subsonic inverse design of the airfoil Cub-RAE2822: initial shape (NACA0012) and
              | associated Cp function, and final shape (Cub-RAE2822) and associated Cp functions delivered by
              | the HDM- and ROM-based optimizations, respectively.
blank         | 
              | 
text          | queries. From a computational complexity viewpoint, this compares favorably with the 24 HDM-
              | based optimization iterations requiring 29 HDM queries (see Table 5.8). Figure 5.23 graphically
              | depicts the progression of the reduced objective function across all reduced optimization problems
              | using a dashed line to indicate a new HDM sample and a subsequent update of the ROB. For each
              | optimization problem, it also reports the size of the ROM.
              |    Finally, Figure 5.24 shows the evolution of the HDM residual evaluated at the solution of the
              | ROM‚Äîwhich is an indicator of the ROM error‚Äîacross all reduced optimization problems, along
              | with the trust region radius ‚àÜk . It is common practice in nonlinear programming software to allow
              | violation of nonlinear constraints during an optimization procedure, which explains the residual
              | bound violation seen in this figure. Figure 5.24 also shows that the ROM solution coincides with
              | the HDM solution at the initial condition of each optimization problem, as expected from the
              | interpolation property of minimum-residual reduced-order models. In the first few major iterations
              | that are far the optimal solution, the residual grows rapidly as the iterates move into areas of the
              | parameter space away from HDM samples. However, near the optimal solution, the residual remains
              | small as the optimization iterates remain in a small neighborhood of the most recent HDM sample.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                     158
blank         | 
              | 
              | 
text          | Remark. There are two mechanisms that prevent the reduced optimization problem from venturing
              | into regions of the parameter space where it lacks accuracy: (1) the objective function and (2) the
              | nonlinear trust region. In the present inverse design example, the objective function is mostly suffi-
              | cient to keep the ROM in regions of accuracy, as can be seen from Figure 5.24 where the trust region
              | bound is only reached once and the upper bound always increases. For other objective functions such
              | as drag, the nonlinear trust region will be necessary as it is likely that an inaccurate ROM can predict
              | a lower value in such objective functions than is actually present. In practice, inaccurate ROMs have
              | been observed to predict the nonphysical situation of negative drag (i.e. thrust), which motivates the
              | need for the residual-based trust region.
blank         | 
              | 
              | 
              | 
text          |                                        10‚àí2
              |                                    2
              |                                    2
              |                                    2
              |                                ))k2
              |        k               RA E2822
              |                                 ))
              |                        RAE2822
blank         | 
              | 
              | 
              | 
text          |                                        10‚àí6
              |       2 p(u(¬µ))‚àíp(u(¬µ
blank         | 
title         |       2 kp(u(0))‚àíp(u(¬µ
blank         | 
              | 
              | 
              | 
text          |                                        10‚àí10
blank         | 
              | 
text          |                                        10‚àí14
              |        k
              |       1
              |       1
blank         | 
              | 
              | 
              | 
text          |                                                                                     HDM-based optimization
              |                                                                                     ROM-based optimization
              |                                        10‚àí18
              |                                                1   3   5   7   9   11 13 15 17 19          21   23   25   27   29
              |                                                                    Number of HDM queries
blank         | 
text          | Figure 5.22: Objective function versus number of queries to the HDM: ROM-based optimization
              | (red) and HDM-based optimization (black).
blank         | 
              | 
              | 
text          | 5.5.4                          Shape Optimization of the Common Research Model in Viscous,
              |                                Turbulent Flow
              | This section applies the proposed trust region method based on reduced-order approximation models
              | to shape design of a full aircraft configuration‚Äîthe Common Research Model (CRM)‚Äîin viscous,
              | turbulent flow. The goal of the optimization problem is to maximize the lift-to-drag ratio of the
              | aircraft while maintaining a constant lift. The flow is modeled using the Reynolds‚Äô Averaged Navier-
              | Stokes (RANS) equations with a Spalart-Allmaras turbulence model. The freestream Mach number
              | and angle of attack are taken as M = 0.85 and Œ± = 2.32‚ó¶ , which are standard operating conditions
              | for a commercial aircraft of this size. The Reynolds‚Äô number is Re = 5 √ó 106 , which is based on wind
              | tunnel model conditions and the reference chord length in the undeformed configuration. The chosen
              |    6 The last HDM sample in Figure 5.22 was not included in this count as the residual-based error indicator is small
blank         | 
text          | at this configuration (Figure 5.24). A similar argument could also be made for the 7th HDM sample.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                                                           159
blank         | 
              | 
              | 
              | 
text          |   2
              |                                                                                                                                60
              |   kp(Œ¶ur (¬µ))‚àíp(Œ¶ur (¬µRAE2822 ))k2
              |                                                       10‚àí2
              |                                        RAE2822 ))k2
              |                                                   2
blank         | 
              | 
text          |                                                       10‚àí6
blank         | 
              | 
              | 
              | 
text          |                                                                                                                                     ROM size
              |                                                                                                                                40
              |                        2 kp(u(0))‚àíp(u(¬µ
blank         | 
              | 
              | 
              | 
text          |                                                       10‚àí10
blank         | 
text          |                                                                                                                                20
              |                                                         ‚àí14
              |                                                       10
              |                        1
              |    1
              |    2
blank         | 
              | 
              | 
              | 
text          |                                                                   HDM sample
              |                                                       10‚àí18                                                                    0
              |                                                               0   20    40         60      80      100       120   140   160
              |                                                                                Reduced optimization iterations
blank         | 
text          | Figure 5.23: Progression of reduced objective function: dashed line indicates an HDM sample and a
              | subsequent update of the ROB.
blank         | 
              | 
text          | freestream Mach number place the flow in the transonic regime, which implies shocks will develop
              | on the wing of the aircraft. The governing equations are discretized with a second-order, vertex-
              | centered finite volume scheme using the AERO-F software [68] and the resulting system of nonlinear
              | equations are solved using pseudo-transient continuation. The mesh employed was validated for
              | these freestream conditions [198] and consists of 11 454 702 nodes for a total of 68 728 212 degrees of
              | freedom.
              |        The design problem (5.64) looks to maximize the lift-to-drag ratio at a constant lift subject to
              | box constraints over a four-dimensional shape design space
blank         | 
text          |                                                                                maximize
              |                                                                                     4
              |                                                                                           Lz (¬µ)/Lx (¬µ)
              |                                                                                  ¬µ‚ààR
blank         | 
text          |                                                                                subject to Lz (¬µ) = Lz (0)                           (5.64)
blank         | 
text          |                                                                                           ¬µl ‚â§ ¬µ ‚â§ ¬µu .
blank         | 
text          | The four shape parameters considered in this problem are: wingspan (¬µ1 ), localized sweep (¬µ2 ),
              | twist (¬µ3 ), and localized dihedral (¬µ4 ); see Figure 5.25 for an illustration of each parameter. The
              | lift constraint is included to ensure the optimized aircraft can carry the same payload as the original
              | aircraft. The box constraints are included to ensure the shape changes are reasonable and the com-
              | putational mesh does not tangle. The optimization problem in (5.64) is initialized from a perturbed
              | CRM configuration that shortens the wing and adds negative twist. The optimized configuration
              | achieves a drag count reduction of 2.2 by lengthening the wing and adding positive sweep, dihedral,
              | and twist. This solution was obtained using by embedding a L-BFGS-B [215] bound-constrained op-
              | timization solver in an augmented Lagrangian framework to handle the nonlinear equality constraint
              | and solve (5.64) directly. This method, which solely relies on HDM solves for objective and gradient
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    160
blank         | 
              | 
              | 
text          |                            1011
blank         | 
              | 
              | 
text          |       2                     105
              |        kr(Œ¶ur (¬µ), ¬µ)k2
blank         | 
              | 
text          |                           10‚àí1
              |        1
              |        2
blank         | 
              | 
              | 
              | 
text          |                           10‚àí7              Residual Norm
              |                                       Residual Norm Bound (‚àÜk )
              |                                             HDM Sample
              |                           10‚àí13
              |                                   0   20      40       60      80      100       120   140   160
              |                                                    Reduced optimization iterations
blank         | 
text          | Figure 5.24: Progression of HDM residual: dashed line indicates an HDM sample and a subsequent
              | update of the ROB.
blank         | 
              | 
text          | queries, will serve as a baseline for comparison with the proposed hyperreduced trust region method
              | in the remainder. Figure 5.26 provides two different views of the initial and optimized shapes to
              | illustrate the changes that occur during the maximization process. Figure 5.27 shows the initial
              | and optimized shapes colored by the pressure coefficient distribution on the surface. The optimized
              | shape weakens the shock near the wing tip, which explains the 2.2 drag count reduction.
              |    The proposed trust region method based on masked minimum-residual hyperreduced approxi-
              | mation models (with an underlying LSPG projection) is applied to solve the optimization problem
              | in (5.64). Gradients are computed according to the masked minimum-residual sensitivity method
              | and primal/sensitivity snapshots are used in the heterogeneous, span-preserving variant of POD
              | without truncation. Therefore, the size of the reduced-order model increases by 5 at each iteration
              | since a single primal snapshot and four sensitivity snapshots are added to the reduced-order basis.
              | Due to the presence of the nonlinear equality constraint, the unconstrained trust region method is
              | wrapped in the augmented Lagrangian framework described in Section 3.2.1. Figure 5.28 shows the
              | convergence history of the drag count reduction as a function of the number of HDM queries for the
              | baseline method and the hyperreduced trust region method. The hyperreduced trust region method
              | requires half as many queries to the HDM to converge to a prescribed tolerance. However, this
              | does not account for all sources of cost in the hyperreduced trust region method since there is cost
              | associated with solving the trust region subproblem (hyperreduced model queries) and construction
              | of the hyperreduced model at each iteration. Figure 5.29 includes these additional sources of cost
              | by showing the convergence of the drag count reduction as a function of wall time normalized by
              | the wall time of a single HDM solve. When properly accounting for all sources of cost, the speedup
              | of the hyperreduced trust region method decreases marginally from 2√ó to 1.6√ó (80% efficiency).
              | Finally, Figure 5.30 shows the sample mesh used at intermediate of the hyperreduced trust region
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                          161
blank         | 
              | 
              | 
              | 
text          | Figure 5.25: Parametrization of CRM. Left: Undeformed CRM configuration. Right: Deformed
              | CRM configuration with positive perturbation to the wingspan ¬µ1 (top row), localized sweep ¬µ2
              | (second row), twist ¬µ3 (third row), and localized dihedral ¬µ4 (bottom row).
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    162
blank         | 
              | 
              | 
              | 
text          | Figure 5.26: Two different views of the initial guess (gray) and solution (red) of the optimization
              | problem in (5.64). The displacement from the undeformed configuration to the optimal solution
              | (red) is magnified by 2√ó. There is a 2.2 drag count reduction from the initial to optimized shape.
blank         | 
              | 
              | 
              | 
text          | Figure 5.27: Left: Initial guess for optimization problem in (5.64). Right: Solution of optimization
              | problem in (5.64). Both plots are colored by the coefficient of pressure Cp . There is a 2.2 drag count
              | reduction from the initial to optimized shape.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                    163
blank         | 
              | 
              | 
              | 
text          |                              15
blank         | 
text          |                              10
              |      Drag Count Reduction
              |                               5
blank         | 
text          |                               0
blank         | 
text          |                             ‚àí5
blank         | 
text          |                             ‚àí10
blank         | 
text          |                             ‚àí15
blank         | 
text          |                             ‚àí20
blank         | 
text          |                                   ‚àí2   0   2   4   6   8  10 12 14 16 18       20   22   24   26
              |                                                        Number of HDM queries
blank         | 
text          | Figure 5.28: Convergence history of the baseline PDE-constrained optimization solver without model
              | reduction (     ) and proposed trust region method based on hyperreduced approximation models
              | (    ). A yellow square ( ) indicates an augmented Lagrangian update. The reduction in drag
              | count is taken as the performance metric and the number of primal HDM queries is the cost model.
              | With respect to this cost metric, the ROM-based optimization solver converges 2√ó faster than the
              | HDM-based solver.
blank         | 
              | 
text          | method, which contains only 72 110 nodes‚Äî0.6% of the original mesh.
meta          | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                                          164
blank         | 
              | 
              | 
              | 
text          |                              15
blank         | 
              | 
text          |      Drag Count Reduction    10
blank         | 
text          |                               5
blank         | 
text          |                               0
blank         | 
text          |                             ‚àí5
blank         | 
text          |                             ‚àí10
blank         | 
text          |                             ‚àí15
blank         | 
text          |                             ‚àí20
blank         | 
text          |                                   0   2   4   6     8    10 12 14 16 18 20           22   24   26   28
              |                                                   Equivalent number of HDM queries
blank         | 
text          | Figure 5.29: Convergence history of the baseline PDE-constrained optimization solver without model
              | reduction (      ) and proposed trust region method based on hyperreduced approximation models
              | (     ). A yellow square ( ) indicates an augmented Lagrangian update. The reduction in drag count
              | is taken as the performance metric and the total wall time of the optimization procedure (normalized
              | by the wall time of a single primal HDM solve) is the cost model. With respect to this cost metric,
              | the ROM-based optimization solver converges 1.6√ó faster than the HDM-based solver.
blank         | 
              | 
              | 
              | 
text          | Figure 5.30: The sample mesh (72 √ó 103 nodes) used at an intermediate iteration of the trust region
              | method based on hyperreduced (collocation) approximation models.
              | Table 5.2: Convergence history of Algorithm 11 applied to optimal control of the inviscid Burgers‚Äô equation using method ‚Äòsens-etr-intpt‚Äô
              | using reduced-order models based on a Galerkin projection.
blank         | 
text          |                  F (¬µk )      mk (¬µk )      F (¬µÃÇk )     mk (¬µÃÇk )    k‚àáF (¬µk )k        œÅk           ‚àÜk        Success?
              |               1.5922e+04    1.5922e+04    2.4783e+02    2.4528e+02   5.3000e+04    9.9984e-01    1.0000e-01   1.0000e+00
              |               2.4783e+02    2.4783e+02    1.4391e+02    1.4451e+02   5.2213e+02    1.0057e+00    2.0000e-01   1.0000e+00
              |               1.4391e+02    1.4391e+02    1.4317e-05    2.2758e-03   6.1988e+02    1.0000e+00    4.0000e-01   1.0000e+00
              |               1.4317e-05    1.4317e-05    6.3338e-20    8.7340e-16   1.4083e+00    1.0000e+00    8.0000e-01   1.0000e+00
              |               6.3338e-20    8.7340e-16         -             -       3.6015e-07         -             -            -
blank         | 
              | 
              | 
              | 
text          | Table 5.3: Convergence history of Algorithm 12 applied to optimal control of the inviscid Burgers‚Äô equation using method ‚Äòsens-etr-intpt‚Äô
              | using reduced-order models based on a Galerkin projection.
blank         | 
text          |                  F (¬µk )      mk (¬µk )      F (¬µÃÇk )     mk (¬µÃÇk )    k‚àáF (¬µk )k        œÅk           ‚àÜk         Success?
              |               1.6431e+04    1.6033e+04    1.8873e+02    1.9717e+02   5.3540e+04    1.0257e+00    1.0000e-01    1.0000e+00
              |               1.8873e+02    1.9784e+02    1.0874e+02    1.0973e+02   7.9026e+02    9.0777e-01    2.0000e-01    1.0000e+00
              |               1.0874e+02    1.0884e+02    1.2551e-03    5.9436e-03   6.7856e+02    9.9911e-01    4.0000e-01    1.0000e+00
              |               1.2551e-03    1.3252e-04    5.3325e-08    4.4413e-05   7.8316e+01    1.4245e+01    8.0000e-01    1.0000e+00
              |               5.3325e-08    5.3325e-08    4.4166e-23    3.1841e-20   1.2847e-01    1.0000e+00    1.6000e+00    1.0000e+00
              |               4.4166e-23    3.1841e-20         -             -       3.4743e-09         -             -             -
meta          |                                                                                                                                             CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS
              |                                                                                                                                             165
text          | Table 5.4: Convergence history of Algorithm 11 applied to optimal control of the inviscid Burgers‚Äô equation using method ‚Äòsens-ctr-stcg‚Äô using
              | reduced-order models based on a Galerkin projection.
blank         | 
text          |                  F (¬µk )       mk (¬µk )       F (¬µÃÇk )     mk (¬µÃÇk )    k‚àáF (¬µk )k         œÅk            ‚àÜk         Success?
              |                1.5922e+04    1.5922e+04    1.1372e+04     1.1346e+04    5.3000e+04    9.9439e-01     1.0000e-01    1.0000e+00
              |                1.1372e+04    1.1372e+04    5.5895e+03     5.5895e+03    3.8805e+04    1.0000e+00     2.0000e-01    1.0000e+00
              |                5.5895e+03    5.5895e+03    8.9676e+02     8.9676e+02    2.0968e+04    1.0000e+00     4.0000e-01    1.0000e+00
              |                8.9676e+02    8.9676e+02    8.0315e+05     3.4795e+04    4.4001e+03    2.3666e+01     8.0000e-01    0.0000e+00
              |                8.9676e+02    8.9676e+02    6.0483e+02     6.0483e+02    4.4001e+03    1.0000e+00     1.6000e-01    1.0000e+00
              |                6.0483e+02    6.0483e+02    4.0276e+02     4.0276e+02    1.3683e+03    1.0000e+00     3.2000e-01    1.0000e+00
              |                4.0276e+02    4.0276e+02    5.1981e+05     6.9325e+03    1.7695e+03    7.9545e+01     6.4000e-01    0.0000e+00
              |                4.0276e+02    4.0276e+02    3.3109e+02     3.3109e+02    1.7695e+03    1.0000e+00     1.0240e-01    1.0000e+00
              |                3.3109e+02    3.3109e+02    2.1174e+02     2.1174e+02    1.2858e+03    1.0000e+00     2.0480e-01    1.0000e+00
              |                2.1174e+02    2.1174e+02    4.9377e+02     4.9378e+02    6.7866e+02    9.9998e-01     4.0960e-01    0.0000e+00
              |                2.1174e+02    2.1174e+02    1.9116e+02     1.9116e+02    6.7866e+02    1.0000e+00     4.1943e-02    1.0000e+00
              |                1.9116e+02    1.9116e+02    1.4938e+02     1.4938e+02    9.5647e+02    1.0000e+00     8.3886e-02    1.0000e+00
              |                1.4938e+02    1.4938e+02    9.0046e+01     9.0046e+01    5.5266e+02    1.0000e+00     1.6777e-01    1.0000e+00
              |                9.0046e+01    9.0046e+01    7.9533e+01     7.9533e+01    3.8955e+03    1.0000e+00     3.3554e-01    1.0000e+00
              |                7.9533e+01    7.9533e+01    1.8002e+02     1.8002e+02    3.5646e+02    1.0000e+00     6.7109e-01    0.0000e+00
              |                7.9533e+01    7.9533e+01    7.1672e+01     7.1672e+01    3.5646e+02    1.0000e+00     2.3488e-02    1.0000e+00
              |                7.1672e+01    7.1672e+01    5.6588e+01     5.6588e+01    7.3693e+02    1.0000e+00     4.6976e-02    1.0000e+00
              |                5.6588e+01    5.6588e+01    3.2573e+01     3.2573e+01    6.6885e+02    1.0000e+00     9.3951e-02    1.0000e+00
              |                3.2573e+01    3.2573e+01    2.6527e+01     2.6527e+01    1.5016e+03    1.0000e+00     1.8790e-01    1.0000e+00
              |                2.6527e+01    2.6527e+01    5.4958e+00     5.4958e+00    1.3171e+04    1.0000e+00     3.7581e-01    1.0000e+00
              |                5.4958e+00    5.4958e+00    4.6810e+00     4.6810e+00    2.3990e+03    1.0000e+00     7.5161e-01    1.0000e+00
              |                4.6810e+00    4.6810e+00    6.7425e+00     6.7425e+00    1.1796e+02    1.0000e+00     1.5032e+00    0.0000e+00
              |                4.6810e+00    4.6810e+00    4.4251e+00     4.4251e+00    1.1796e+02    1.0000e+00     3.1837e-03    1.0000e+00
              |                4.4251e+00    4.4251e+00    3.9356e+00     3.9356e+00    1.1226e+02    1.0000e+00     6.3673e-03    1.0000e+00
              |                3.9356e+00    3.9356e+00    3.0492e+00     3.0492e+00    9.4010e+01    1.0000e+00     1.2735e-02    1.0000e+00
              |                3.0492e+00    3.0492e+00    1.6303e+00     1.6303e+00    1.7259e+02    1.0000e+00     2.5469e-02    1.0000e+00
              |                1.6303e+00    1.6303e+00    3.8792e-01     3.8792e-01    3.3882e+02    1.0000e+00     5.0939e-02    1.0000e+00
              |                3.8792e-01    3.8792e-01    1.1734e-01     1.1734e-01    1.8628e+03    1.0000e+00     1.0188e-01    1.0000e+00
              |                1.1734e-01    1.1734e-01    4.6898e-03     4.6898e-03    8.2406e+01    1.0000e+00     2.0375e-01    1.0000e+00
              |                4.6898e-03    4.6898e-03    2.7171e-05     2.7171e-05    2.5736e+02    1.0000e+00     4.0751e-01    1.0000e+00
              |                2.7171e-05    2.7171e-05    9.5916e-06     9.5916e-06    1.8011e+00    1.0000e+00     8.1502e-01    1.0000e+00
              |                                                                                                                                                  CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS
blank         | 
              | 
              | 
              | 
text          |                9.5916e-06    9.5916e-06    2.5087e-11     2.5087e-11    1.1313e-01    1.0000e+00     1.6300e+00    1.0000e+00
              |                2.5087e-11    2.5087e-11    3.7967e-12     3.7967e-12    1.7368e-02    1.0000e+00     3.2601e+00    1.0000e+00
              |                                                                                                                                                  166
blank         | 
              | 
              | 
              | 
text          |                3.7967e-12    3.7967e-12    2.5080e-18     2.5047e-18    1.4714e-04    1.0000e+00     6.5202e+00    1.0000e+00
              |                2.5080e-18    2.5080e-18         -              -        4.2318e-06         -              -             -
              | Table 5.5: Convergence history of Algorithm 12 applied to optimal control of the inviscid Burgers‚Äô equation using method ‚Äòsens-ctr-stcg‚Äô using
              | reduced-order models based on a Galerkin projection.
blank         | 
text          |                  F (¬µk )       mk (¬µk )       F (¬µÃÇk )     mk (¬µÃÇk )    k‚àáF (¬µk )k         œÅk            ‚àÜk         Success?
              |                1.6431e+04    1.6033e+04    1.1573e+04     1.1464e+04    5.3540e+04    1.0634e+00     1.0000e-01    1.0000e+00
              |                1.1573e+04    1.1372e+04    5.6146e+03     5.5885e+03    3.9065e+04    1.0303e+00     2.0000e-01    1.0000e+00
              |                5.6146e+03    5.5885e+03    9.0886e+02     8.9613e+02    2.1019e+04    1.0028e+00     4.0000e-01    1.0000e+00
              |                9.0886e+02    8.9613e+02    8.8161e+07     7.2819e+04    4.6131e+03    1.2258e+03     8.0000e-01    0.0000e+00
              |                9.0886e+02    8.9613e+02    5.9847e+02     6.0460e+02    4.6131e+03    1.0647e+00     1.6000e-01    1.0000e+00
              |                5.9847e+02    6.0459e+02    3.9555e+02     4.0246e+02    1.2978e+03    1.0039e+00     3.2000e-01    1.0000e+00
              |                3.9555e+02    4.0246e+02    2.4110e+05     2.6063e+04    2.0151e+03    9.3805e+00     6.4000e-01    0.0000e+00
              |                3.9555e+02    4.0246e+02    3.3074e+02     3.3080e+02    2.0151e+03    9.0433e-01     1.0240e-01    1.0000e+00
              |                3.3074e+02    3.3080e+02    2.1140e+02     2.1150e+02    1.2856e+03    1.0004e+00     2.0480e-01    1.0000e+00
              |                2.1140e+02    2.1150e+02    4.9353e+02     4.9353e+02    6.7904e+02    1.0003e+00     4.0960e-01    0.0000e+00
              |                2.1140e+02    2.1150e+02    1.9084e+02     1.9093e+02    6.7904e+02    1.0000e+00     4.1943e-02    1.0000e+00
              |                1.9084e+02    1.9093e+02    1.4906e+02     1.4917e+02    9.5568e+02    1.0003e+00     8.3886e-02    1.0000e+00
              |                1.4906e+02    1.4917e+02    8.9792e+01     8.9903e+01    5.5440e+02    1.0001e+00     1.6777e-01    1.0000e+00
              |                8.9792e+01    8.9903e+01    7.9260e+01     7.9375e+01    3.8951e+03    1.0003e+00     3.3554e-01    1.0000e+00
              |                7.9260e+01    7.9375e+01    1.7709e+02     1.7709e+02    3.5553e+02    1.0012e+00     6.7109e-01    0.0000e+00
              |                7.9260e+01    7.9375e+01    7.1456e+01     7.1569e+01    3.5553e+02    9.9979e-01     2.3349e-02    1.0000e+00
              |                7.1456e+01    7.1569e+01    5.6462e+01     5.6577e+01    7.4645e+02    1.0002e+00     4.6697e-02    1.0000e+00
              |                5.6462e+01    5.6577e+01    3.2554e+01     3.2667e+01    6.5370e+02    9.9991e-01     9.3395e-02    1.0000e+00
              |                3.2554e+01    3.2667e+01    2.7533e+01     2.7533e+01    1.4597e+03    9.7789e-01     1.8679e-01    1.0000e+00
              |                2.7533e+01    2.7533e+01    5.5226e+00     5.5226e+00    1.3537e+04    1.0000e+00     3.7358e-01    1.0000e+00
              |                5.5226e+00    5.5226e+00    4.6138e+00     4.6138e+00    2.5382e+03    1.0000e+00     7.4716e-01    1.0000e+00
              |                4.6138e+00    4.6138e+00    6.6933e+00     6.6933e+00    1.2584e+02    1.0000e+00     1.4943e+00    0.0000e+00
              |                4.6138e+00    4.6138e+00    4.3612e+00     4.3612e+00    1.2584e+02    1.0000e+00     3.1598e-03    1.0000e+00
              |                4.3612e+00    4.3612e+00    3.8793e+00     3.8793e+00    1.0487e+02    1.0000e+00     6.3196e-03    1.0000e+00
              |                3.8793e+00    3.8793e+00    3.0053e+00     3.0053e+00    1.0029e+02    9.9998e-01     1.2639e-02    1.0000e+00
              |                3.0053e+00    3.0053e+00    1.6074e+00     1.6075e+00    1.5696e+02    1.0000e+00     2.5279e-02    1.0000e+00
              |                1.6074e+00    1.6075e+00    3.7861e-01     3.7862e-01    3.2953e+02    1.0000e+00     5.0557e-02    1.0000e+00
              |                3.7861e-01    3.7862e-01    1.1784e-01     1.1784e-01    1.8295e+03    9.9998e-01     1.0111e-01    1.0000e+00
              |                1.1784e-01    1.1784e-01    4.5753e-03     4.5761e-03    7.9865e+01    9.9998e-01     2.0223e-01    1.0000e+00
              |                4.5753e-03    4.5761e-03    3.4610e-05     3.4584e-05    2.5399e+02    9.9982e-01     4.0446e-01    1.0000e+00
              |                3.4610e-05    3.4584e-05    1.3090e-05     1.3090e-05    1.8009e+00    1.0012e+00     8.0891e-01    1.0000e+00
              |                                                                                                                                                  CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS
blank         | 
              | 
              | 
              | 
text          |                1.3090e-05    1.3090e-05    4.7238e-11     4.7238e-11    1.3249e-01    1.0000e+00     1.6178e+00    1.0000e+00
              |                4.7238e-11    4.7238e-11    6.3287e-12     6.3287e-12    2.4075e-02    1.0000e+00     3.2357e+00    1.0000e+00
              |                6.3287e-12    6.3287e-12    5.0572e-21     5.0530e-21    1.9695e-04    1.0000e+00     6.4713e+00    1.0000e+00
              |                                                                                                                                                  167
blank         | 
              | 
              | 
              | 
text          |                5.0572e-21    5.0572e-21    4.4579e-24     5.0572e-21    2.6757e-07        inf        1.2943e+01    1.0000e+00
              |                4.4579e-24    5.0572e-21         -              -        8.1378e-11         -              -             -
              | Table 5.6: Convergence history of Algorithm 11 applied to optimal control of the viscous Burgers‚Äô equation using method ‚Äòdual-etr-intpt‚Äô
              | using reduced-order models based on a Galerkin projection.
blank         | 
text          |                   F (¬µk )     mk (¬µk )     F (¬µÃÇk )     mk (¬µÃÇk )   k‚àáF (¬µk )k      œÅk            ‚àÜk         Success?
              |                 3.8620e-02   3.8620e-02   3.2002e-02   3.4377e-02   1.2191e-02   1.5596e+00   1.0000e-01   1.0000e+00
              |                 3.2002e-02   3.2002e-02   2.2060e-02   1.6674e-02   7.0854e-03   6.4861e-01   2.0000e-01   1.0000e+00
              |                 2.2060e-02   2.2060e-02   1.5559e-02   1.5606e-02   2.0611e-03   1.0072e+00   1.8000e-01   1.0000e+00
              |                 1.5559e-02   1.5559e-02   1.5527e-02   1.5521e-02   6.9906e-05   8.4404e-01   3.6000e-01   1.0000e+00
              |                 1.5527e-02   1.5527e-02   1.5524e-02   1.5524e-02   1.6373e-05   9.3144e-01   7.2000e-01   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   1.5633e-06   9.7164e-01   1.4400e+00   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   9.1808e-08   9.9643e-01   2.8800e+00   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   5.5276e-07   9.9994e-01   5.7600e+00   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   3.2305e-08   1.0020e+00   1.1520e+01   1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   6.9441e-08   1.0042e+00   2.3040e+01   1.0000e+00
              |                 1.5524e-02   1.5524e-02        -            -       6.1859e-08        -            -            -
blank         | 
              | 
              | 
              | 
text          | Table 5.7: Convergence history of Algorithm 11 applied to optimal control of the viscous Burgers‚Äô equation using method ‚Äòdual-ctr-intpt‚Äô
              | using reduced-order models based on a Galerkin projection.
blank         | 
text          |                   F (¬µk )     mk (¬µk )      F (¬µÃÇk )    mk (¬µÃÇk )   k‚àáF (¬µk )k       œÅk           ‚àÜk         Success?
              |                 3.8620e-02   3.8620e-02   3.3959e-02   3.4919e-02   1.2191e-02   1.2596e+00   1.0000e-01    1.0000e+00
              |                 3.3959e-02   3.3959e-02   2.9644e-02   2.9645e-02   8.7651e-03   1.0003e+00   2.0000e-01    1.0000e+00
              |                 2.9644e-02   2.9644e-02   2.6406e-02   2.6406e-02   5.1460e-03   1.0000e+00   4.0000e-01    1.0000e+00
              |                 2.6406e-02   2.6406e-02   2.3727e-02   2.3727e-02   2.6859e-03   1.0000e+00   8.0000e-01    1.0000e+00
              |                 2.3727e-02   2.3727e-02   2.1113e-02   2.1113e-02   1.7850e-03   1.0000e+00   1.6000e+00    1.0000e+00
              |                 2.1113e-02   2.1113e-02   1.8771e-02   1.8771e-02   1.1968e-03   1.0000e+00   3.2000e+00    1.0000e+00
              |                 1.8771e-02   1.8771e-02   1.6877e-02   1.6877e-02   7.2259e-04   1.0000e+00   6.4000e+00    1.0000e+00
              |                 1.6877e-02   1.6877e-02   1.5700e-02   1.5700e-02   3.8119e-04   1.0000e+00   1.2800e+01    1.0000e+00
              |                                                                                                                                            CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS
blank         | 
              | 
              | 
              | 
text          |                 1.5700e-02   1.5700e-02   1.5524e-02   1.5524e-02   1.1431e-04   1.0000e+00   2.5600e+01    1.0000e+00
              |                 1.5524e-02   1.5524e-02   1.5524e-02   1.5524e-02   3.7482e-08   1.0042e+00   5.1200e+01    1.0000e+00
              |                 1.5524e-02   1.5524e-02        -            -       6.3774e-08        -            -             -
meta          |                                                                                                                                            168
              | CHAPTER 5. DETERMINISTIC PDE-CONSTRAINED OPTIMIZATION WITH ROMS                 169
blank         | 
              | 
              | 
              | 
text          |        Table 5.8: Performance of the HDM- and ROM-based optimization methods.
blank         | 
text          |                                       HDM-based            ROM-based
              |                                       optimization         optimization
              |           # of HDM Evaluations            29                    76
              |           # of ROM Evaluations             -                   346
              |               ¬µ‚àó ‚àí ¬µRAE2822
              |                                      2.28 √ó 10‚àí3 %        4.17 √ó 10‚àí6 %
              |                k¬µRAE2822 k
title         | Chapter 6
blank         | 
title         | Model Reduction and Sparse Grids
              | for Efficient Stochastic
              | Optimization
blank         | 
text          | To this point, all partial differential equations, and the corresponding optimization problems, have
              | been posed in a deterministic setting, that is, the PDE itself and all its data are assumed known. This
              | is not a realistic assumption since all PDEs are merely mathematical models of physical phenomena
              | and even if the PDE is an accurate approximation of reality, its data‚Äîcoefficients, boundary con-
              | ditions, source terms, etc‚Äîwill rarely be known with certainty. This is particularly true in physical
              | systems characterized by a high degree of volatility or those where physical measurements are diffi-
              | cult to take. In such settings, the uncertainty must be incorporated into the optimization problem
              | if a robust, risk-averse design or control is to be attained. In this work, parametrized uncertainties
              | are considered and risk-averse measures (Section 2.2.1) of quantities of interest will be used as the
              | objective and constraint functions for the stochastic PDE-constrained optimization problem. The
              | mathematical construction and discretized of parametrized stochastic partial differential equations
              | is provided in Section 2.2, including the introduction of a complete probability space, the finite
              | noise assumption, spatio-temporal discretization of a realization of the stochastic partial differential
              | equation, and collocation-based discretization of the stochastic space. Since risk-averse measures
              | usually require the computation of an integral over the stochastic space, a single query to an op-
              | timization function requires the evaluation of an integral whose integrand depends on the solution
              | of a realization of the stochastic partial differential equation. In general, this requires a (possibly
              | large) ensemble of deterministic PDE solves and makes stochastic PDE-constrained optimization
              | problems potentially many orders of magnitude more expensive than the deterministic counterparts.
              | In fact, if there are large number of stochastic parameters, it is difficult to evaluate an integral
              | over a high-dimensional space even if the integrand is inexpensive to evaluate due to the curse of
              | dimensionality. A straightforward or brute force approach to solve such optimization problems is
blank         | 
              | 
meta          |                                                   170
text          | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 171
blank         | 
              | 
              | 
text          | infeasible for all but the simplest problems.
              |    To address the large cost of PDE-constrained optimization under uncertainty, a multifidelity trust
              | region method based on the theory introduced in Chapter 3 is developed. The approximation model
              | incorporates two levels of inexactness: dimension-adaptive sparse grids for efficient collocation-
              | based integration in moderate-to-high dimensional spaces and reduced-order models to reduce the
              | cost of queries to a realization of the stochastic partial differential equation. Both levels of the
              | approximation will be incorporated into the required error indicators. A two-level greedy method is
              | proposed to construct the sparse grid and reduced-order basis such that, at a given iteration of the
              | trust region method, the required error conditions are satisfied, thus ensuring global convergence.
              | The proposed method is demonstrated on a one-dimensional optimal flow control problem. For
              | simplicity, the remainder of this document will consider only the risk-neutral measure, or expectation,
              | of a quantity of interest. Extension to other risk-averse measures will be deferred to later work.
blank         | 
              | 
title         | 6.1     Background
text          | This chapter begins with an overview of ingredients that will be necessary to develop the proposed
              | trust region method based on the two-level approximation of risk measures of quantities of interest
              | of stochastic PDEs: stochastic reduced-order models and anisotropic sparse grids.
blank         | 
              | 
title         | 6.1.1    Stochastic High-Dimensional Model
text          | Consider the discrete collocation-based stochastic PDE introduced in Section 2.2
blank         | 
text          |                                         r(u, ¬µ, y) = 0         ‚àÄy ‚àà Œû                             (6.1)
blank         | 
text          | where u ‚àà RNu is the state vector, ¬µ ‚àà RN¬µ is the parameter vector, y ‚àà Œû are the stochastic
              | variables, and Œû ‚äÇ RNy is the stochastic space. The existence of a continuously differentiable
              | function u(¬µ; y), defined as the solution of r( ¬∑ , ¬µ, y) = 0, is guaranteed by Theorem 2.1, under
              | suitable assumptions. Depending on the nature of the stochastic variables, the quantity of interest
              | may be stochastic as well and a realization will take the form
blank         | 
text          |                                                  f (u, ¬µ, y)                                      (6.2)
blank         | 
text          | for y ‚àà Œû, which can be considered only a function of ¬µ and y using the implicit definition u(¬µ; y)
blank         | 
text          |                                        F (¬µ; y) = f (u(¬µ; y), ¬µ, y).                              (6.3)
blank         | 
text          | The risk-neutral measure of the QoI, which will be used as the objective for the stochastic optimiza-
              | tion problem in this work, is
blank         | 
text          |                                 F (¬µ) = E[f (u(¬µ; ¬∑ ), ¬µ, ¬∑ )] = E[F (¬µ, ¬∑ )].                    (6.4)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 172
blank         | 
              | 
              | 
text          | Generalization to other risk-averse measures proceeds by replacing E[¬∑] with R[¬∑] defined in (2.51)-
              | (2.55); however, special care will be required in the construction of quadrature rules for non-smooth
              | risk-averse measures.
              |    The gradient of the risk-neutral measure of the QoI, ‚àáF (¬µ), is computed via the sensitivity or
              | adjoint method, depending on the number of QoIs versus the number of parameters (N¬µ ). Since
              | the differentiation operation (with respect to ¬µ) can be pulled inside the expectation operation
              | (integration with respect to y), the gradient takes the form
blank         | 
text          |                           ‚àáF (¬µ) = E [‚àáf (u(¬µ; ¬∑ ), ¬µ, ¬∑ )] = E [‚àáF (¬µ, ¬∑ )] .                    (6.5)
blank         | 
text          | This illustrates that the computation of the gradient of the risk-neutral measure of the QoI reduces
              | to an integral over realizations of the QoI gradients, i.e., for a fixed y ‚àà Œû and ¬µ ‚àà RN¬µ . The
              | gradient of a particular realization proceeds exactly according to the adjoint or sensitivity method
              | outlined in Sections 2.3.3 and 2.3.4. Following the procedures outlined in that section, the stochastic
              | variant of the sensitivity and adjoint residuals are
blank         | 
text          |                                               ‚àÇf             ‚àÇf
              |                          r ‚àÇ (u, w, ¬µ, y) :=     (u, ¬µ, y) +     (u, ¬µ, y)w
              |                                               ‚àÇ¬µ             ‚àÇu
              |                                                                                                   (6.6)
              |                                               ‚àÇf                 ‚àÇr
              |                           r Œª (u, z, ¬µ, y) :=    (u, ¬µ, y) ‚àí z T    (u, ¬µ, y).
              |                                               ‚àÇ¬µ                 ‚àÇ¬µ
blank         | 
text          | Then, a realization of the sensitivity problem for a fixed y ‚àà Œû and ¬µ ‚ààRN¬µ is: given the  primal
              |                                                          ‚àÇu                         ‚àÇu
              | solution u(¬µ; y) that satisfies r( ¬∑ , ¬µ, y) = 0, find       such that r ‚àÇ u(¬µ; y),    , ¬µ, y = 0.
              |                                                          ‚àÇ¬µ                         ‚àÇ¬µ
              | Similarly, a realization of the adjoint problem for a fixed y ‚àà Œû and ¬µ ‚àà RN¬µ is: given the primal
              | solution u(¬µ; y), find Œª such that r Œª (u(¬µ; y), Œª, ¬µ, y) = 0. The sensitivity and adjoint solution,
              |                                                           ‚àÇu
              | for a particular y ‚àà Œû and ¬µ ‚àà RN¬µ , will be denoted         (¬µ; y) and Œª(¬µ; y), respectively. The
              |                                                           ‚àÇ¬µ
              | reconstruction of the gradient of a QoI from a sensitivity or adjoint solution are generalized from
              | the deterministic case in (2.90), (2.102) to the stochastic case as
blank         | 
text          |                                               ‚àÇf             ‚àÇf
              |                          g ‚àÇ (u, w, ¬µ, y) :=     (u, ¬µ, y) +     (u, ¬µ, y)w
              |                                               ‚àÇ¬µ             ‚àÇu
              |                                                                                                   (6.7)
              |                                               ‚àÇf                 ‚àÇr
              |                           g Œª (u, z, ¬µ, y) :=    (u, ¬µ, y) + z T    (u, ¬µ, y).
              |                                               ‚àÇ¬µ                 ‚àÇ¬µ
blank         | 
text          | With these definitions, a realization of the gradient of a QoI corresponding to y ‚àà Œû and ¬µ ‚àà RN¬µ
              | takes the form
blank         |                                                  
text          |                            ‚àÇ       ‚àÇu
              |             ‚àáF (¬µ, y) = g u(¬µ; y),    (¬µ; y), ¬µ, y = g Œª (u(¬µ; y), Œª(¬µ; y), ¬µ, y)                 (6.8)
              |                                    ‚àÇ¬µ
blank         | 
text          | and the gradient of the risk-neutral measure in (6.4) is
blank         |                                                 
text          |                       ‚àÇ          ‚àÇu
              |                                                      = E g Œª (u(¬µ; ¬∑ ), Œª(¬µ; ¬∑ ), ¬µ, ¬∑ ) .
blank         |                                                                                        
text          |           ‚àáF (¬µ) = E g u(¬µ; ¬∑ ),    (¬µ; ¬∑ ), ¬µ, ¬∑                                                 (6.9)
              |                                  ‚àÇ¬µ
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 173
blank         | 
              | 
              | 
title         | 6.1.2    Stochastic Reduced-Order Model
text          | The dimension of the discretized stochastic PDE in (6.1) is reduced through the introduction of the
              | model reduction ansatz u = Œ¶ur from (4.2) into (6.1), where Œ¶ ‚àà RNu √óku is the trial basis that
              | defines a subspace that (approximately) contains the solution of any realization of the stochastic
              | PDE, i.e., u(¬µ, y) for ¬µ ‚àà RN¬µ and y ‚àà Œû. The result is an overdetermined nonlinear system
              | of equations r(Œ¶ur , ¬µ, y) = 0 for any realization y ‚àà Œû. Projection of these equations onto the
              | columnspace of the test basis Œ® ‚àà RNu √óku leads to the projection-based reduced-order model with
              | ku equations and unknowns
blank         | 
text          |                                     rr (ur , ¬µ, y) := Œ®T r(Œ¶ur , ¬µ, y) = 0.                    (6.10)
blank         | 
text          | This work will primarily be consider minimum-residual reduced-order models (Definition 4.1), which
              | completely prescribes the test basis Œ® based on the trial basis Œ¶ and optimality metric Œò. For a
              | given ¬µ ‚àà RN¬µ and realization y ‚àà Œû, the solution of (6.10) will be denoted ur (¬µ; y, Œ¶, Œ®), which
              | will be shortened to ur (¬µ; y) when there is no risk of confusion regarding the choice of test and
              | trial basis. From Theorem 2.1, ur (¬µ; y, Œ¶, Œ®) is a continuously differentiable function of ¬µ. A
              | realization of the reduced quantity of interest takes the form f (Œ¶ur , ¬µ, y), which can be considered
              | solely a function of ¬µ and y through the implicit solution of (6.10)
blank         | 
text          |                               Fr (¬µ; y, Œ¶, Œ®) = f (Œ¶ur (¬µ; y, Œ¶, Œ®), ¬µ, y).                    (6.11)
blank         | 
text          | Finally, the risk-neutral measure of the reduced quantity of interest, which serves as an approxima-
              | tion for the risk-neutral measure of the true quantity of interest in (6.4), is
blank         | 
text          |                  Fr (¬µ; Œ¶, Œ®) = E[f (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )] = E[Fr (¬µ; ¬∑ , Œ¶, Œ®)].         (6.12)
blank         | 
text          | Following the exposition in Sections 4.1.2 and 4.1.3, the gradient of the risk-neutral measure is
              | computed according to the sensitivity or adjoint method as
blank         |                                                                                   
text          |                                    ‚àÇ                       ‚àÇur
              |                ‚àáFr (¬µ; Œ¶, Œ®) = E g Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), Œ¶        (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑
              |                                                            ‚àÇ¬µ                                  (6.13)
              |                              = E g Œª (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), Œ®Œªr (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ ) .
blank         |                                                                                  
              | 
              | 
text          |                                                                                    ‚àÇur
              | where Œ¶ur (¬µ; y, Œ¶, Œ®) is the reconstructed primal solution for realization y ‚àà Œû,      (¬µ; y, Œ¶, Œ®)
              |                                                                                    ‚àÇ¬µ
              | is the reduced sensitivity, and Œªr (¬µ; y, Œ¶, Œ®) is the reduced adjoint. The minimum-residual variants
              | of the reduced gradient computation in (6.13) can be used in place of ‚àáFr (¬µ)
              |                                           "                                             !#
              |                           ‚àÇ     ‚àÇ             ‚àÇ    ‚àÇu
              |                                                    dr
              |                                                         ‚àÇ         ‚àÇ    ‚àÇ
              |         ‚àáF
              |         dr (¬µ; Œ¶, Œ®, Œ¶ , Œò ) = E g     u( ¬∑ ), Œ¶        (¬µ; ¬∑ , Œ¶ , Œò , u( ¬∑ )), ¬µ, ¬∑
              |                                                     ‚àÇ¬µ                                         (6.14)
              |                                   h                                                  i
              |         dr (¬µ; Œ¶, Œ®, Œ¶Œª , ŒòŒª ) = E g Œª u( ¬∑ ), Œ¶Œª ŒªÃÇr (¬µ; ¬∑ , Œ¶Œª , ŒòŒª , u( ¬∑ )), ¬µ, ¬∑
              |         ‚àáF
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 174
blank         | 
              | 
              | 
text          | where u(y) = Œ¶ur (¬µ; y, Œ¶, Œ®) is the reconstructed primal solution for realization y ‚àà Œû,
              | ‚àÇu
              |  dr
              |     (¬µ; y, Œ¶‚àÇ , Œò‚àÇ , u(y)) is the solution of the minimum-residual sensitivity equations in (4.28)
              |  ‚àÇ¬µ
              | and ŒªÃÇr (¬µ; y, Œ¶Œª , ŒòŒª , u(y)) is the solution of the minimum-residual adjoint equations in (4.56)
              | for the realization corresponding to y ‚àà Œû. Using Propositions 4.3 and 4.5 as motivation, the
              | sensitivity/adjoint bases and optimality metrics are chosen according to (4.35) and (4.63). This
              | implies that the selection of Œ¶ and Œ® completely specify Œ¶‚àÇ and Œ¶Œª . From these propositions,
              | the exact and minimum-residual sensitivities will only agree if the test basis Œ® is constant (such
              | as a Galerkin projection) or the primal reduced-order model solution is exact for each realization
              | y ‚àà Œû. Since training a reduced-order model to be exact for all y ‚àà Œû is impractical, the relation
              | ‚àáFr (¬µ) = ‚àáF
              |           dr (¬µ) will only hold if the test basis is constant.
              |    The residual-based error bounds derived in Appendix B hold for a particular realization y ‚àà Œû
              | of the stochastic PDE, provided Assumptions (AR1)‚Äì(AR8), (AQ1)‚Äì(AQ4) hold for this realization.
              | The primal, sensitivity, and adjoint residual error bounds for a realization y ‚àà Œû are
blank         | 
text          |                            |f (u(¬µ, y), ¬µ, y) ‚àí f (u, ¬µ, y)| ‚â§ Œ∂ kr(u, ¬µ, y)k
blank         |                                      
text          |                        ‚àÇu
              |      g‚àÇ       u(¬µ, y),    (¬µ, y), ¬µ, y ‚àí g ‚àÇ (u, w, ¬µ, y) ‚â§ Œ∫ kr(u, ¬µ, y)k + œÑ r ‚àÇ (u, w, ¬µ, y)
              |                        ‚àÇ¬µ
              |           g Œª (u(¬µ, y), Œª(¬µ, y), ¬µ, y) ‚àí g Œª (u, z, ¬µ, y) ‚â§ Œ∫ kr(u, ¬µ, y)k + œÑ r Œª (u, z, ¬µ, y)
              |                                                                                                (6.15)
              | where u = Œ¶ur (¬µ; y, Œ¶, Œ®) is the reconstructed primal solution, w is the reconstructed reduced sen-
              |                                                   ‚àÇur                         ‚àÇu
              |                                                                               dr
              | sitivity (exact or minimum-residual), i.e., w = Œ¶     (¬µ; y, Œ¶, Œ®) or w = Œ¶‚àÇ      (¬µ; y, Œ¶‚àÇ , Œò‚àÇ , u),
              |                                                   ‚àÇ¬µ                          ‚àÇ¬µ
              | and z is the reconstructed reduced adjoint (exact or minimum-residual), i.e., z = Œ®Œªr (¬µ; y, Œ¶, Œ®)
              | or z = Œ¶Œª Œªr (¬µ; y, Œ¶Œª , ŒòŒª , u).
              |    Finally, the stochastic generalization of the collocation-based hyperreduced models of Section 4.2
              | follows immediately from the construction in that section and takes the form
blank         | 
text          |                                 (P T Œ®)T P T r(Œ¶ur , ¬µ, y) = 0   ‚àÄy ‚àà Œû                         (6.16)
blank         | 
text          | The case of stochastic hyperreduction will not be considered further as only problems amenable
              | to precomputations (polynomial nonlinearities) will be considered in the numerical experiments
              | (Section 6.4).
              |    While the introduction of the stochastic reduced-order and hyperreduced models in this section
              | reduces the cost of evaluating risk-averse measures of PDE quantities of interest, e.g., for stochastic
              | optimization, they may still be prohibitively expensive due to the curse of dimensionality. The
              | next section introduces anisotropic sparse grids to mitigate or delay the impact of the curse of
              | dimensionality when evaluating risk-averse measures in moderate-to-large dimensional stochastic
              | spaces.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 175
blank         | 
              | 
              | 
title         | 6.1.3     Anisotropic Sparse Grids
text          | Consider the difficult problem of evaluating the expectation of a smooth function g : RNy ‚Üí R
              |                                               Z
              |                                        E[g] =    œÅ(y)g(y) dy                                (6.17)
              |                                                     Œû
blank         | 
              | 
text          | where Œû ‚äÇ [‚àí1, 1]Ny is the stochastic space and œÅ : Œû ‚Üí R+ is the joint probability density
              | function with marginal probability density functions œÅk : Œûk ‚Üí R+ for k = 1, . . . , Ny such that
              | œÅ = œÅ1 ‚äó ¬∑ ¬∑ ¬∑ ‚äó œÅNy ). When Ny is moderate-to-large, the evaluation of the integral in (6.17) is difficult
              | since multidimensional quadrature rules derived from optimal 1D quadrature rules suffer from the
              | curse of dimensionality. Isotropic sparse grids, originally introduced in [184] and extensively studied
              | since [144, 66, 145, 156, 18, 157], generate efficient quadrature rules that delay the influence of the
              | curse of dimensionality and allows for larger stochastic spaces to be considered. Anisotropic sparse
              | grids [67] further optimize the quadrature rules by leveraging anisotropy of the integrand.
              |      The sparse grid construction begins with the definition of a one-dimensional quadrature rule of
              | level i that will be used in the kth dimension, Eik . The level is an integer used to indicate refinement
              | of the one-dimensional quadrature rule such that
              |                                             Z
              |                          Eik [h] ‚Üí Ek [h] =   œÅk (y)h(y) dy              as i ‚Üí ‚àû.                  (6.18)
              |                                                Œûk
blank         | 
              | 
text          | for h : Œûk ‚Üí R. Let Œûik ‚äÇ [‚àí1, 1] be the quadrature nodes associated with the quadrature rule Eik .
              | While the sparse grid construction to follow holds for any valid and refinable quadrature rule that
              | satisfies (6.18), only nested quadrature rules will be considered. That is, the nodes at level i are a
              | subset of the nodes at level i + 1, Œûik ‚äÇ Œûi+1
              |                                            k . The nested property will not be used in the sparse
              | grid construction, but leads to an efficient implementation since, at level i + 1, only h(y) must be
              | evaluated for y ‚àà Œûi+1
              |                    k   \ Œûik .
              |      From the one-dimensional quadrature rules, the corresponding difference operators are defined
              | as
              |                           ‚àÜ1k := E1k    and      ‚àÜik := Eik ‚àí Ei‚àí1
              |                                                                k           for i ‚â• 2.               (6.19)
blank         | 
text          | The requirement in (6.18) on the quadrature rules implies ‚àÜik [g] ‚Üí 0 as i ‚Üí 0. The one-dimensional
              | quadrature rule Eik is recovered by summing over all difference operators in dimension k up through
              | level i
              |                                                         i
              |                                                         X
              |                                               Eik =           ‚àÜjk                                   (6.20)
              |                                                         j=1
blank         | 
text          | since the sum telescopes due to the definition of ‚àÜik in (6.19). A multi-dimensional difference
              | operator is constructed from a tensor product of one-dimensional difference operators, each possibly
              | at a different level of refinement
              |                                                                     iN
              |                                         ‚àÜi := ‚àÜi11 ‚äó ¬∑ ¬∑ ¬∑ ‚äó ‚àÜNyy .                                 (6.21)
              |                       N
              | A multi-index i ‚àà N+y with components i = (i1 , . . . , iNy ) is used to track the refinement level of
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 176
blank         | 
              | 
              | 
text          | each one-dimensional difference operator, i.e., ik is the refinement level of the difference operator in
              | dimension k. From the multi-dimensional difference operator, a quadrature rule EI is defined by
              |                                                                          N
              | summing over all multi-indices in a multi-index set I ‚äÇ N+y
              |                                            X               X                     iN
              |                                     EI =         ‚àÜi :=           ‚àÜi11 ‚äó ¬∑ ¬∑ ¬∑ ‚äó ‚àÜNyy .            (6.22)
              |                                            i‚ààI             i‚ààI
blank         | 
              | 
text          | Let ŒûI ‚äÇ [‚àí1, 1]Ny denote the quadrature nodes associated with the multi-dimensional quadrature
              | rule EI . The use of nested, one-dimensional quadrature rules implies ŒûI ‚äÇ ŒûJ for I, J multi-index
              | sets such that I ‚äÇ J . In the multi-dimensional case, this leads to substantial savings as evaluations
              | of g can be recycled as the sparse grid is refined.
              |                                                                                           N
              |    For EI to be a convergent quadrature rule, i.e., EI ‚Üí E as I ‚Üí N+y , a telescoping property
              | similar to that in (6.20) must hold. This requirement is satisfied if the multi-index I is admissible
              | in the sense of Definition 6.1.
              |                                            N
              | Definition 6.1. An index set I ‚äÇ N+y is admissible if for all k ‚àà I,
blank         | 
text          |                                    k ‚àí ej ‚àà I        for       1 ‚â§ j ‚â§ Ny , k j > 1               (6.23)
blank         | 
text          |    This completes the construction of general, anisotropic sparse grids. From this general construc-
              | tion, some well-known special cases can be recovered. The tensor product quadrature rule of level
              |                                                                         N
              | i, Ei1 ‚äó ¬∑ ¬∑ ¬∑ ‚äó EiNy , can written as EI‚àû
              |                                          i
              |                                                  i
              |                                            with I‚àû = {i ‚àà N+y | |i|‚àû ‚â§ i}, i.e.,
              |                                                       X
              |                           Ei1 ‚äó ¬∑ ¬∑ ¬∑ ‚äó EiNy [g] =             (‚àÜi1 ‚äó ¬∑ ¬∑ ¬∑ ‚äó ‚àÜiNy )[g] = EI‚àû
              |                                                                                             i .   (6.24)
              |                                                      |i|‚àû ‚â§i
blank         | 
              | 
text          | Figure 6.1 provides an example of a tensor product quadrature rule, and the corresponding index
              | set, based on Clenshaw-Curtis quadrature rules; the index set is dense and leads to a quadrature
              | rule with the maximum number of nodes. The isotropic Smolyak sparse grid of level i is
              |                                        X
              |                                                  (‚àÜi1 ‚äó ¬∑ ¬∑ ¬∑ ‚äó ‚àÜiNy )[g] = EIiso
              |                                                                               i ,                 (6.25)
              |                                   |i|1 ‚â§i+Ny +1
blank         | 
              | 
text          |        i              N
              | where Iiso = {i ‚àà N+y | |i|1 ‚â§ i + Ny ‚àí 1}. See Figure 6.2 for an illustration of the quadrature
              | nodes and index set; the index set is only refined along the diagonal, which leads to much sparser
              | quadrature rules than direct tensor products. Finally, Figure 6.3 illustrates an anisotropic sparse
              | grid, including quadrature nodes and index set, which further reduces the number of quadrature
              | compared to the other options and (potentially) takes advantage of anisotropy in the integrand g(y)
              | and probability density function œÅ(y).
              |    The neighbor of a sparse grid is the final concept introduced in this section and will be used
              | extensively in assessing the truncation error that arises from approximating E[g] by EI [g]. The set
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 177
blank         | 
              | 
              | 
text          |                                                           N                            N
              | of neighbors corresponding to a sparse grid I ‚äÇ N+y , denoted N (I) ‚äÇ N+y is defined as
blank         | 
text          |                                N (I) := {i ‚àà I c | I ‚à™ {i} is admissible}                        (6.26)
blank         | 
text          |                                                                   N                        N
              | where I c is the complement of the multi-index set I in N+y , i.e., I c = {i ‚àà N+y | i 6‚àà I}. Figure 6.4
              | shows the quadrature nodes and index set corresponding to the anisotropic sparse grid, including
              | neighbors, in Figure 6.3. Following the work in [67, 108, 109], the truncation error, which can be
              | written as the infinite sum
              |                                                   X                       iN
              |                                E[g] ‚àí EI [g] =           (‚àÜi11 ‚äó ¬∑ ¬∑ ¬∑ ‚äó ‚àÜNyy )[g]               (6.27)
              |                                                  i‚ààI c
blank         | 
              | 
text          | can be approximated as
              |                                                  X                         iN
              |                               E[g] ‚àí EI [g] ‚âà             (‚àÜi11 ‚äó ¬∑ ¬∑ ¬∑ ‚äó ‚àÜNyy )[g].             (6.28)
              |                                                 i‚ààN (I)
blank         | 
              | 
text          | The concept and notation used to represent neighbors of a sparse grid is easily extended to handle
              | j layers of neighbors, that is, N (I) is the 1st layer of neighbors, N (N (I)) is the 2nd layer, and
              | N j (I) := N
              |            | ‚ó¶ ¬∑{z
              |                  ¬∑ ¬∑ ‚ó¶ N} ‚ó¶I is the jth layer. A more accurate approximation of the truncation error
              |              j terms
              | is attainable by including more distant neighbors, but expense of the corresponding computation
              | rapidly increases. For this reason, usually only the first layer of neighbors is used to approximate
              | the truncation error [67, 108, 109], which is the approach taken in this work.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 178
blank         | 
              | 
              | 
              | 
text          |           6                                     6                                    6
              |           5                                     5                                    5
              |      i2   4                                     4                                    4
blank         | 
              | 
              | 
              | 
text          |                                            i2
blank         | 
              | 
              | 
              | 
text          |                                                                                 i2
              |           3                                     3                                    3
              |           2                                     2                                    2
              |           1                                     1                                    1
blank         | 
text          |                 1 2 3 4 5 6                           1 2 3 4 5 6                          1 2 3 4 5 6
              |                      i1                                    i1                                   i1
              |           1                                     1                                    1
blank         | 
text          |       0.5                                   0.5                                  0.5
              | Œæ2
blank         | 
              | 
              | 
              | 
text          |                                       Œæ2
blank         | 
              | 
              | 
              | 
text          |                                                                            Œæ2
              |           0                                     0                                    0
blank         | 
text          |      ‚àí0.5                                  ‚àí0.5                                 ‚àí0.5
blank         | 
text          |       ‚àí1                                    ‚àí1                                   ‚àí1
              |               ‚àí1 ‚àí0.5 0     0.5   1                 ‚àí1 ‚àí0.5 0    0.5   1                 ‚àí1 ‚àí0.5 0    0.5   1
              |                       Œæ1                                    Œæ1                                   Œæ1
blank         | 
              | 
text          |                 Figure 6.1: Full tensor product based on Clenshaw-Curtis (levels 1, 3, 5)
blank         | 
              | 
text          |           6                                     6                                    6
              |           5                                     5                                    5
              |           4                                     4                                    4
              |      i2
blank         | 
              | 
              | 
              | 
text          |                                            i2
blank         | 
              | 
              | 
              | 
text          |                                                                                 i2
blank         | 
text          |           3                                     3                                    3
              |           2                                     2                                    2
              |           1                                     1                                    1
blank         | 
text          |                 1 2 3 4 5 6                           1 2 3 4 5 6                          1 2 3 4 5 6
              |                      i1                                    i1                                   i1
              |           1                                     1                                    1
blank         | 
text          |       0.5                                   0.5                                  0.5
              | Œæ2
blank         | 
              | 
              | 
              | 
text          |                                       Œæ2
blank         | 
              | 
              | 
              | 
text          |                                                                            Œæ2
blank         | 
              | 
              | 
              | 
text          |           0                                     0                                    0
blank         | 
text          |      ‚àí0.5                                  ‚àí0.5                                 ‚àí0.5
blank         | 
text          |       ‚àí1                                    ‚àí1                                   ‚àí1
              |               ‚àí1 ‚àí0.5 0     0.5   1                 ‚àí1 ‚àí0.5 0    0.5   1                 ‚àí1 ‚àí0.5 0    0.5   1
              |                       Œæ1                                    Œæ1                                   Œæ1
blank         | 
              | 
text          |                 Figure 6.2: Isotropic sparse grid based on Clenshaw-Curtis (levels 1, 3, 5)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 179
blank         | 
              | 
              | 
              | 
text          |           6                                     6                                    6
              |           5                                     5                                    5
              |      i2   4                                     4                                    4
blank         | 
              | 
              | 
              | 
text          |                                            i2
blank         | 
              | 
              | 
              | 
text          |                                                                                 i2
              |           3                                     3                                    3
              |           2                                     2                                    2
              |           1                                     1                                    1
blank         | 
text          |                  1 2 3 4 5 6                          1 2 3 4 5 6                          1 2 3 4 5 6
              |                       i1                                   i1                                   i1
              |           1                                     1                                    1
blank         | 
text          |       0.5                                   0.5                                  0.5
              | Œæ2
blank         | 
              | 
              | 
              | 
text          |                                       Œæ2
blank         | 
              | 
              | 
              | 
text          |                                                                            Œæ2
              |           0                                     0                                    0
blank         | 
text          |      ‚àí0.5                                  ‚àí0.5                                 ‚àí0.5
blank         | 
text          |       ‚àí1                                    ‚àí1                                   ‚àí1
              |               ‚àí1 ‚àí0.5 0     0.5   1                 ‚àí1 ‚àí0.5 0    0.5   1                 ‚àí1 ‚àí0.5 0    0.5   1
              |                       Œæ1                                    Œæ1                                   Œæ1
blank         | 
              | 
text          |                Figure 6.3: Anisotropic sparse grid based on Clenshaw-Curtis (levels 1, 3, 5)
blank         | 
              | 
text          |           6                                     6                                    6
              |           5                                     5                                    5
              |           4                                     4                                    4
              |      i2
blank         | 
              | 
              | 
              | 
text          |                                            i2
blank         | 
              | 
              | 
              | 
text          |                                                                                 i2
blank         | 
text          |           3                                     3                                    3
              |           2                                     2                                    2
              |           1                                     1                                    1
blank         | 
text          |                  1 2 3 4 5 6                          1 2 3 4 5 6                          1 2 3 4 5 6
              |                       i1                                   i1                                   i1
              |           1                                     1                                    1
blank         | 
text          |       0.5                                   0.5                                  0.5
              | Œæ2
blank         | 
              | 
              | 
              | 
text          |                                       Œæ2
blank         | 
              | 
              | 
              | 
text          |                                                                            Œæ2
blank         | 
              | 
              | 
              | 
text          |           0                                     0                                    0
blank         | 
text          |      ‚àí0.5                                  ‚àí0.5                                 ‚àí0.5
blank         | 
text          |       ‚àí1                                    ‚àí1                                   ‚àí1
              |               ‚àí1 ‚àí0.5 0     0.5   1                 ‚àí1 ‚àí0.5 0    0.5   1                 ‚àí1 ‚àí0.5 0    0.5   1
              |                       Œæ1                                    Œæ1                                   Œæ1
blank         | 
              | 
text          | Figure 6.4: Anisotropic sparse grid based on Clenshaw-Curtis with all (including non-admissible)
              | forward neighbors (levels 1, 3, 5)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 180
blank         | 
              | 
              | 
title         | 6.2     Two Levels of Approximation of Risk-Averse Measures
text          | The two approximation technologies introduced‚Äîanisotropic sparse grids for the efficient approxima-
              | tion of high-dimensional integrals and stochastic reduced-order models to reduce the cost associated
              | with solving a realization of the SPDE‚Äîare combined to yield an inexpensive approximation of
              | risk-averse measures of quantities of interest of high-fidelity partial differential equations. For the
              | remainder of this section, suppose a sparse grid I and reduced-order model (Œ¶, Œ®) are given ‚Äì the
              | construction of each will be considered in detail in Section 6.3.2. The two-level approximation of
              | the risk-averse measure of the quantity of interest in (6.4) is
blank         | 
text          |                             Fr (¬µ; Œ¶, Œ®, I) := EI [f (Œ¶ur (¬µ; ¬∑, Œ¶, Œ®), ¬µ, ¬∑ )].                 (6.29)
blank         | 
text          | The introduction of the sparse grid introduces a truncation error into the evaluation of the integral
              | and the reduced-order model introduces an error in the evaluation of the quantity of interest at
              | each collocation node. The benefit of such an approximation is that the many high-dimensional
              | model solutions required to evaluate F (¬µ) are replace by few reduced-order model solutions to
              | evaluate Fr (¬µ). The introduction of the sparse grid further benefits the reduced-order model since
              | it only needs to be trained on the collocation nodes instead of everywhere in Œû. The gradient of the
              | approximation in (6.29) is computed according to the sensitivity or adjoint method as
blank         |                                                                                  
text          |                                                            ‚àÇur
              |              ‚àáFr (¬µ; Œ¶, Œ®, I) = EI g ‚àÇ Œ¶ur (¬µ; ¬∑, Œ¶, Œ®), Œ¶     (¬µ; ¬∑, Œ¶, Œ®), ¬µ, ¬∑
              |                                                            ‚àÇ¬µ                                    (6.30)
              |                                    Œª                                            
              |                               = EI g (Œ¶ur (¬µ; ¬∑, Œ¶, Œ®), Œ®Œªr (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ ) .
blank         | 
text          | If the true sensitivity and adjoint of the stochastic reduced-order model are too cumbersome to
              | compute, i.e., if second derivatives of r are required (see Chapter 4), the minimum-residual variants
              | can be used to compute an approximation to ‚àáFr (¬µ) as
              |                                           "                                              !#
              |                         ‚àÇ     ‚àÇ               ‚àÇ      ‚àÇu
              |                                                      dr
              |                                                           ‚àÇ         ‚àÇ    ‚àÇ
              |       ‚àáF
              |       dr (¬µ; Œ¶, Œ®, Œ¶ , Œò , I) = EI g     u( ¬∑ ), Œ¶        (¬µ; ¬∑, Œ¶ , Œò , u( ¬∑ )), ¬µ, ¬∑
              |                                                       ‚àÇ¬µ                                         (6.31)
              |                                     h                                                  i
              |       dr (¬µ; Œ¶, Œ®, Œ¶Œª , ŒòŒª , I) = EI g Œª u( ¬∑ ), Œ¶Œª ŒªÃÇr (¬µ; ¬∑ , Œ¶Œª , ŒòŒª , u( ¬∑ )), ¬µ, ¬∑
              |       ‚àáF
blank         | 
text          | where u(y) = Œ¶ur (¬µ; y, Œ¶, Œ®) is the reconstructed primal reduced-order model solution of the
              | realization corresponding to y ‚àà Œû.
              |    The error incurred by approximating F (¬µ) with Fr (¬µ; Œ¶, Œ®, I) must account for both the
              | truncation error introduced by the sparse grid and the pointwise error in the reduced-order model.
              | These terms arise naturally from a simple application of the triangle inequality to the error
blank         | 
text          |        |F (¬µ) ‚àí Fr (¬µ; Œ¶, Œ®, I)| = |E[f (u(¬µ; ¬∑ ), ¬µ, ¬∑ ) ‚àí EI [f (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )]|
              |                                    ‚â§ E[|f (u(¬µ; ¬∑ ), ¬µ, ¬∑ ) ‚àí f (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )|]     (6.32)
              |                                           + EI c [|f (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )|]
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 181
blank         | 
              | 
              | 
text          | where EI c := E ‚àí EI was used (Section 6.1.3). The first term in the error bound is the integrated
              | reduced-order model error and the second is the truncation error that results from using the sparse
              | grid I to integrate the reduced quantity of interest. While the error bound is instructive in un-
              | derstanding the sources of error, it can not be efficiently computed due to the presence of the true
              | error in the first integrand and the infinite sum required to compute the expectations in both terms
              | (quadrature over an infinite set of collocation nodes to compute the integral exactly). The residual-
              | based error bounds from Appendix B are used to circumvent the first issue by bounding the true
              | error by an arbitrary constant (Œ∂ > 0) times the residual norm
blank         | 
text          |   |F (¬µ) ‚àí Fr (¬µ; Œ¶, Œ®, I)| ‚â§ Œ∂E[kr(Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )k] + EI c [|f (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )|]
              |                              = Œ∂E[kr(Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )k] + EI c [|Fr (¬µ; ¬∑ , Œ¶, Œ®)|],
              |                                                                                                    (6.33)
              | where the definition of Fr , introduced in Section 6.1.2, was used in the second line. The infinite sums
              | required to compute both expectations are reduced to finite sums by approximating the complement
              | of the sparse grid I c (infinite set of collocation points) with the forward neighbors of the sparse grid
              | N (I) (finite set of collocation points). With this approximation, the expectation operator E and
              | truncation operator EI c become
blank         | 
text          |                           E := EI‚à™I c ‚âà EI‚à™N (I)       and      EI c ‚âà EN (I) .                    (6.34)
blank         | 
text          | The introduction of this approximation into the error bound in (6.33) reduces the uncomputable
              | right-hand side (due to the infinite sums required for evaluation of the expectation and truncation
              | operators) to
blank         | 
text          |    |F (¬µ) ‚àí Fr (¬µ; Œ¶, Œ®, I)| . Œ∂EI‚à™N (I) [kr(Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )k] + EN (I) [|Fr (¬µ; ¬∑ , Œ¶, Œ®)|],
              |                                                                                                    (6.35)
              | which is amenable to computation as only finite summations are required. In general, the right-
              | hand side of (6.35) does not bound the left-hand side due to the introduction of the approximation
              | I c ‚âà N (I). While this approximation does not necessarily preserve the error bound in (6.33), it
              | leads to an inexpensive error indicators: the right-hand side of (6.35) only requires reduced-order
              | models solves and residual evaluations on the sparse grid I and its neighbors N (I).
              |    An identical procedure is carried out to convert the pointwise error bounds in (6.13) for a given
              | realization of the stochastic PDE to an inexpensive error indicator. The error indicator for gradients
              | computed via the sensitivity method takes the form
blank         | 
text          |    |‚àáF (¬µ) ‚àí ‚àáFr (¬µ; Œ¶, Œ®)| . Œ∫EI‚à™N (I) [kr(Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )k] +
blank         |                                                                                              
text          |                                                                     ‚àÇur
              |                               œÑ EI‚à™N (I) r ‚àÇ (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), Œ¶       (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ ) +
              |                                                                      ‚àÇ¬µ
              |                                    EN (I) [k‚àáFr (¬µ; Œ¶, Œ®)k]
              |                                                                                                    (6.36)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 182
blank         | 
              | 
              | 
text          | and for gradients computed via the adjoint method, it takes the form
blank         | 
text          |     |‚àáF (¬µ) ‚àí ‚àáFr (¬µ; Œ¶, Œ®)| . Œ∫EI‚à™N (I) [kr(Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )k] +
              |                                œÑ EI‚à™N (I) r Œª (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), Œ®Œªr (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ ) +
blank         |                                                                                           
              | 
text          |                                     EN (I) [k‚àáFr (¬µ; Œ¶, Œ®)k] .
              |                                                                                                 (6.37)
              | Similar to the error indicator in (6.35) for the value of quantity of interest, the gradient error
              | indicators in (6.36) and (6.37) have terms that separately account for the reduced-order model error
              | and integral truncation error. The three terms in these error indicators account for the error in the
              | primal reduced-order model solution, the error in the reduced-order model sensitivity/adjoint, and
              | truncation error from approximating the expectation operators with the sparse grid I, respectively.
              | The gradient error indicators must include the terms that accounts for the error in the primal solution
              | since, in general, the sensitivity/adjoint equations are defined about an approximate linearization
              | point. The gradient error indicators for the minimum-residual sensitivity and adjoint reduced-order
              | model follow in a similar manner
blank         | 
text          |           dr (¬µ; Œ¶, Œ®, Œ¶‚àÇ , Œò‚àÇ )| . Œ∫EI‚à™N (I) [kr(u( ¬∑ ), ¬µ, ¬∑ )k] +
              | |‚àáF (¬µ) ‚àí ‚àáF
              |                                               "                                                     #
              |                                                                ‚àÇ ‚àÇur
              |                                                                  d
              |                                                   ‚àÇ                           ‚àÇ   ‚àÇ
              |                                     œÑ EI‚à™N (I) r (u( ¬∑ ), Œ¶          (¬µ; ¬∑ , Œ¶ , Œò ; u( ¬∑ )), ¬µ, ¬∑ ) +
              |                                                                  ‚àÇ¬µ
              |                                             h                               i
              |                                       EN (I) ‚àáFdr (¬µ; Œ¶, Œ®, Œ¶‚àÇ , Œò‚àÇ )
blank         | 
text          |           dr (¬µ; Œ¶, Œ®, Œ¶Œª , ŒòŒª )| . Œ∫EI‚à™N (I) [kr(Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )k] +
              | |‚àáF (¬µ) ‚àí ‚àáF
              |                                               h                                                     i
              |                                     œÑ EI‚à™N (I) r Œª (u( ¬∑ ), Œ¶Œª ŒªÃÇr (¬µ; ¬∑ , Œ¶Œª , ŒòŒª , u( ¬∑ )), ¬µ, ¬∑ ) +
              |                                             h                               i
              |                                       EN (I) ‚àáFdr (¬µ; Œ¶, Œ®, Œ¶Œª , ŒòŒª ) .
              |                                                                                                 (6.38)
              | where u(y) = Œ¶ur (¬µ; y, Œ¶, Œ®) is the reconstructed primal solution for realization y ‚àà Œû.
              |    At this point, the proposed two-level approximation of risk-averse measures of quantities of inter-
              | est based on anisotropic sparse grids and model reduction has been introduced and relevant details
              | pertaining to gradients and computable error indicators have been discussed. The next section uses
              | this technology as the approximation model in the multifidelity trust region method of Chapter 3 to
              | yield an efficient algorithm to solve stochastic PDE-constrained optimization problems. To simplify
              | the exposition in the next section, details pertaining to the use of the minimum-residual sensitiv-
              | ity/adjoint reduced-order models to approximate ‚àáFr (¬µ) with ‚àáF    dr (¬µ) will be dropped. These
              | details follow in a straightforward manner from those corresponding to the exact sensitivity/adjoint
              | method to compute ‚àáFr (¬µ). Furthermore, the numerical experiments in Section 6.4 will solely
              | consider a reduced-order models based on a Galerkin projection Œ® = Œ¶, which implies the test
              | basis is constant and ‚àáF
              |                       dr (¬µ) = ‚àáFr (¬µ), provided the sensitivity and adjoint bases are chosen
              | according to (4.35), (4.63). Therefore, the distinction between the exact and minimum-residual
              | sensitivity/adjoint methods is irrelevant since they are identical in this case.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 183
blank         | 
              | 
              | 
title         | 6.3       Multifidelity Trust Region Method Based on Two-Level
              |           Approximation
text          | This section presents the primary contribution of this chapter: the use of sparse grids and model
              | reduction in the multifidelity trust region framework of Chapter 3 to yield an efficient algorithm for
              | stochastic PDE-constrained optimization. The approximation model, mk (¬µ), that is central to the
              | trust region theory will be taken as the two-level approximation of risk-averse measures of quantities
              | of interest introduced in the previous section. The error indicators required for the trust region
              | theory are inspired from the error indicators in (6.35), (6.36)-(6.37). A two-level greedy algorithm
              | will be introduced in Section 6.3.2 that combines dimension-adaptive sparse grid construction [67]
              | with a classical reduced basis greedy method [149, 173]. The purpose of the greedy algorithm is to
              | simultaneously construct a sparse grid Ik and reduced-order model Œ¶k , Œ®k such that the two-level
              | approximation is sufficiently accurate to guarantee convergence based on the requirements (3.14),
              | (3.15), (3.22) detailed in Chapter 3.
              |    Before proceeding to the exposition of the multifidelity trust region method, additional notation
              | will be introduced for convenience. In particular, each component of the error indicators in (6.35)
              | and (6.36)-(6.37) are separated into individual terms. Define the following primal error terms from
              | (6.35)
              |                        E1 (Œ¶, Œ®, I, ¬µ) := EI‚à™N (I) [kr(Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )k]
              |                                                                                                (6.39)
              |                        E2 (Œ¶, Œ®, I, ¬µ) := EN (I) [|f (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )|] ,
blank         | 
text          | where E1 is the (integrated) reduced-order model error indicator and E2 is the truncation error
              | indicator associated with using the sparse grid I in place of the true expectation. The gradient error
              | terms depend on whether the sensitivity or adjoint method are used in the gradient computation.
              | If the sensitivity method is employed, define the error terms
blank         |                                                                                         
text          |                                          ‚àÇ                   ‚àÇur
              |          E3 (Œ¶, Œ®, I, ¬µ) := EI‚à™N (I) r (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), Œ¶     (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )
              |                                                              ‚àÇ¬µ
              |                                                                                              (6.40)
              |                                      ‚àÇ                     ‚àÇur
              |          E4 (Œ¶, Œ®, I, ¬µ) := EN (I) g (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), Œ¶     (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ ) .
              |                                                            ‚àÇ¬µ
blank         | 
text          | Otherwise, the adjoint method is used and the error terms are defined as
blank         | 
text          |           E3 (Œ¶, Œ®, I, ¬µ) := EI‚à™N (I) r Œª (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), Œ®Œªr (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ )
blank         |                                                                                        
text          |                                                                                                (6.41)
              |           E4 (Œ¶, Œ®, I, ¬µ) := EN (I) g Œª (Œ¶ur (¬µ; ¬∑ , Œ¶, Œ®), Œ®Œªr (¬µ; ¬∑ , Œ¶, Œ®), ¬µ, ¬∑ ) .
blank         |                                                                                      
              | 
              | 
text          | Regardless of whether the sensitivity or adjoint method is used, E3 is the (integrated) sensitiv-
              | ity/adjoint reduced-order model error indicator and E4 is the truncation error indicator associated
              | with using the sparse grid I in place of the true expectation in the gradient computation. With the
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 184
blank         | 
              | 
              | 
text          | above definitions of the individual error terms, the error indicators in (6.35), (6.36)-(6.37) become
blank         | 
text          |           |F (¬µ) ‚àí Fr (¬µ; Œ¶, Œ®)| ‚â§ Œ∂E1 (Œ¶, Œ®, I, ¬µ) + E2 (Œ¶, Œ®, I, ¬µ)
              |                                                                                                       (6.42)
              |      k‚àáF (¬µ) ‚àí ‚àáFr (¬µ; Œ¶, Œ®)k ‚â§ Œ∫E1 (Œ¶, Œ®, I, ¬µ) + œÑ E3 (Œ¶, Œ®, I, ¬µ) + E4 (Œ¶, Œ®, I, ¬µ),
blank         | 
text          | where Œ∂, Œ∫, œÑ > 0 are arbitrary constants.
blank         | 
              | 
title         | 6.3.1    Trust Region Ingredients
text          | This section will detail the various ingredients required to leverage the two-level approximation of
              | risk-averse measures of quantities of interest in the multifidelity trust region framework of Chapter 3.
              | In particular, the approximation model mk (¬µ), objective decrease error indicator œëk (¬µ), gradient
              | error indicator œëk (¬µ), and inexact objective model œàk (¬µ) and associated error indicator Œ∏k (¬µ) will
              | be specified using the developments of Section 6.2. For the remainder of this section, it is assumed
              | that, at iteration k, the sparse grid Ik and reduced-order model Œ¶k , Œ®k have been constructed.
              | Details pertaining to their construction will be provided in the next section.
              |    At the kth iteration, the approximation model is taken as the two-level approximation of the
              | risk-averse measure of the PDE quantity of interest, i.e.,
blank         | 
text          |                 mk (¬µ) := Fr (¬µ; Œ¶k , Œ®k , Ik ) = EIk [f (Œ¶k ur (¬µ, ¬∑ , Œ¶k , Œ®k ), ¬µ, ¬∑ )] .          (6.43)
blank         | 
text          | Similar to the trust region method detailed in Chapter 5, there are two options for the objective
              | decrease error indicators: (1) the two-level residual-based indicator introduced in Section 6.2 and
              | (2) the classical trust region constraint. The residual-based error indicator requires the pointwise
              | form of the objective condition (3.14) to leverage the error terms E1 and E2
blank         | 
text          |      |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ |F (¬µk ) ‚àí mk (¬µk )| + |F (¬µ) ‚àí mk (¬µ)|
              |                                               . Œ∂(E1 (Œ¶k , Œ®k , Ik , ¬µk ) + E1 (Œ¶k , Œ®k , Ik , ¬µ))+   (6.44)
              |                                                    E2 (Œ¶k , Œ®k , Ik , ¬µk ) + E2 (Œ¶k , Œ®k , Ik , ¬µ).
blank         | 
text          | for an arbitrary constant Œ∂ > 0. Inspired from the above error indicator, the residual-based trust
              | region constraint is defined as
blank         | 
text          |                       œëk (¬µ) =Œ±1 (E1 (Œ¶k , Œ®k , Ik , ¬µk ) + E1 (Œ¶k , Œ®k , Ik , ¬µ)) +
              |                                                                                                       (6.45)
              |                                   Œ±2 (E2 (Œ¶k , Œ®k , Ik , ¬µk ) + E2 (Œ¶k , Œ®k , Ik , ¬µ))
blank         | 
text          | for user-defined parameters Œ±1 , Œ±2 > 0 that balance the contribution of the reduced-order model
              | error and truncation error. However, unlike the approach taken in Chapter 5, the classical trust
              | region is primarily used in this section
blank         | 
text          |                                              œëk (¬µ) = k¬µ ‚àí ¬µk k .                                     (6.46)
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 185
blank         | 
              | 
              | 
text          | This choice is primarily due to the fact that the objective error bound required for global convergence
              | of the trust region method (3.14)
blank         | 
text          |                              |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ Œ∂œëk (¬µ)
blank         | 
text          | for some constant Œ∂ > 0, cannot be guaranteed with the residual-based choice of œëk (¬µ) in (6.35)
              | due to approximate bound that results from approximating the truncation error using only the
              | collocation nodes corresponding to the neighbors of the sparse grid. From the discussion in Chapter 3,
              | the classical choice of œëk (¬µ) in (6.46) guarantees the above error bound if the gradient conditions
              | (3.13),(3.15) are satisfied. Another reason for the choice of the classical trust region is that (6.45)
              | can significantly increase the cost of an iteration of the trust region subproblem since œëk (¬µ) requires
              | an expectation computation over I ‚à™ N (I) at two points ¬µk and ¬µ, which may have substantially
              | more nodes that I alone (used for the evaluation of mk (¬µ)). Finally, the definition of œëk (¬µ) is
              | not differentiable for all ¬µ ‚àà RN¬µ due to the presence of the norm in E1 and absolute value in E2 ,
              | which may cause convergence issues in the interior-point trust region subproblem solver discussed
              | in Section 3.1.2.
              |    From the choice of mk (¬µ), the gradient is ‚àámk (¬µ) = ‚àáFr (¬µ; Œ¶k , Œ®k , Ik ), which suggests the
              | following gradient error bound based on the approximate bound in (6.42)
blank         | 
text          |          œïk (¬µ) = Œ≤1 E1 (Œ¶k , Œ®k , Ik , ¬µ) + Œ≤2 E3 (Œ¶k , Œ®k , Ik , ¬µ) + Œ≤3 E4 (Œ¶k , Œ®k , Ik , ¬µ)   (6.47)
blank         | 
text          | This choice of œïk (¬µ) does not guarantee the bound required by the global convergence theory in
              | Chapter 3, i.e.,
              |                                    k‚àáF (¬µk ) ‚àí ‚àámk (¬µk )k ‚â§ Œæœïk (¬µk ),                             (6.48)
blank         | 
text          | for a constant Œæ > 0, due to the approximation of the truncation error on the neighbors of the
              | sparse grid. Therefore global convergence is not strictly guaranteed; however, the numerical results
              | in Sections 6.4 suggest this choice does lead to global convergence for these problems.
              |    With these choices of œëk (¬µ) and œïk (¬µ), the sparse grid Ik and reduced-order model Œ¶k , Œ®k
              | must be constructed to satisfy the error conditions in (3.14), (3.15), i.e.,
blank         | 
text          |                                     œëk (¬µk ) ‚â§ Œ∫œë ‚àÜk
              |                                     œïk (¬µk ) ‚â§ Œ∫œï min{‚àámk (¬µk ), ‚àÜk }.
blank         | 
text          | The construction of these quantities such that the above error bounds are satisfied is somewhat
              | delicate since the error terms E1 and E3 behave differently than E2 and E4 when Œ¶k , Œ®k and Ik
              | are refined. For a fixed basis Œ¶k , refinement of the sparse grid Ik decreases the truncation error
              | terms E2 and E4 . However, refinement of Ik may cause the model reduction error terms E1 and
              | E3 to increase since the pointwise error is integrated over an expanded set of collocation nodes.
              | A dimension-adaptive greedy algorithm that accounts for this interplay between the various error
              | terms in response to refinement of the sparse grid and reduced-order basis will be introduced in the
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 186
blank         | 
              | 
              | 
text          | next section.
              |    At this point, the basic version of the multifidelity trust region method in Algorithm 1 is fully
              | defined using the proposed two-level approximation and corresponding error indicators. An immedi-
              | ate issue with using these definitions in Algorithm 1 of Chapter 3 pertains to the evaluation of F (¬µ)
              | in the computation of œÅk in (3.9). From its definition, evaluation of F (¬µ) requires an infinite sum
              | to evaluate the true expectation. The true expectation can be approximated on a ‚Äúfine‚Äù quadra-
              | ture rule (possibly based on a refined sparse grid) to evaluate F (¬µ) to high precision. While this
              | option is simple and effective, it requires a large number of collocation nodes and the computation
              | will constitute a bottleneck in the trust region algorithm since it must be performed at each major
              | iteration. Instead, we opt to use the flexibility afforded by the trust region method in Chapter 3 for
              | inexact objective evaluations in the computation of the actual-to-predicted ratio. This follows the
              | work in [109] that uses dimension-adaptive sparse grids (without reduced-order models) for inexact
              | objective evaluations. For this purpose, a separate sparse grid Ik0 and reduced-order model Œ¶0k , Œ®0k
              | are introduced and, following the notation in Chapter 3, the inexact objective function, œàk (¬µ), is
              | employed with corresponding error indicator Œ∏k (¬µ) defined as
blank         | 
text          |                        œàk (¬µ) = Fr (¬µ; Œ¶0k , Œ®0k , Ik0 )
              |                        Œ∏k (¬µ) = Œ±1 (E1 (Œ¶0k , Œ®0k , Ik0 , ¬µk ) + E1 (Œ¶0k , Œ®0k , Ik0 , ¬µ))+     (6.49)
              |                                  Œ±2 (E2 (Œ¶0k , Œ®0k , Ik0 , ¬µk ) + E2 (Œ¶0k , Œ®0k , Ik0 , ¬µ)).
blank         | 
text          | These choices are identical to mk (¬µ) and the residual-based definition of œëk (¬µ), based on a (possibly
              | refined) sparse grid Ik0 and reduced-order model Œ¶0k , Œ®0k . They do not necessarily guarantee the
              | bound required for global convergence (3.21), again due to the approximation of the truncation
              | errors on the neighbors of the sparse grid in (6.35). With these definitions, the actual-to-predicted
              | ratio is computed as
              |                                                 œàk (¬µk ) ‚àí œàk (¬µÃÇk )
              |                                          œÅk =                                                   (6.50)
              |                                                 mk (¬µk ) ‚àí mk (¬µÃÇk )
              | where ¬µÃÇk is the solution of the trust region subproblem, i.e.,
blank         | 
text          |                           ¬µÃÇk = arg min mk (¬µ)             subject to œëk (¬µ) ‚â§ ‚àÜk .             (6.51)
              |                                    ¬µ‚ààRN¬µ
blank         | 
              | 
text          | The sparse grid Ik0 and reduced-order model Œ¶0k , Œ®0k are constructed to guarantee
blank         | 
text          |                                   Œ∏kœâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk },                       (6.52)
blank         | 
text          | where œâ ‚àà (0, 1), Œ∑ < min{Œ∑1 , 1 ‚àí Œ∑2 }, and {rk }‚àû
              |                                                   k=1 is a sequence such that rk ‚Üí 0, using the two-
              | level dimension-adaptive greedy algorithm to be introduced in the next section. Once this training
              | algorithm is completely specified, the complete trust region algorithm will be fully prescribed and
              | is summarized in Section 6.3.3 and Algorithm 15.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 187
blank         | 
              | 
              | 
title         | 6.3.2     Greedy Construction of Sparse Grid and Reduced Basis
text          | The quality of the two-level approximation of risk-averse measures of PDE quantities of interest
              | introduced in the previous section depends critically on the sparse grid I and reduced-order basis
              | Œ¶ used in (6.43). This section develops dimension-adaptive greedy methods for the simultaneous
              | construction of the sparse grid Ik (Ik0 ) and reduced-order basis Œ¶k (Œ¶0k ) that targets each term in the
              | error indicators œïk (¬µ) and Œ∏k (¬µ) such that the error conditions (required for global convergence) in
              | (3.14), (3.15), (3.22) are satisfied. Since the classical trust region constraint is used to define œëk (¬µ),
              | the objective decrease condition (3.14) will be automatically satisfied if the gradient bound (3.13)
              | and condition (3.15) are satisfied (Chapter 3). Thus, the task reduces to construction of Ik , Œ¶k such
              | that the gradient condition (3.15) is satisfied and Ik0 , Œ¶0k such that the inexact objective condition
              | (3.22) is satisfied. We begin with the gradient condition.
              |     Recall from (6.47), the gradient error indicator is a weighted sum of three terms: the primal
              | error E1 , the sensitivity/adjoint error E3 , and the gradient truncation error E4
blank         | 
text          |            œïk (¬µ) = Œ≤1 E1 (Œ¶k , Œ®k , Ik , ¬µ) + Œ≤2 E3 (Œ¶k , Œ®k , Ik , ¬µ) + Œ≤3 E4 (Œ¶k , Œ®k , Ik , ¬µ).
blank         | 
text          | From Chapter 3, global convergence of the multifidelity trust region method is predicated on the
              | satisfaction of the gradient condition: œïk (¬µk ) ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk }. A sufficient condition
              | for this gradient condition to hold is that each term satisfies an appropriate fraction of the condition,
              | i.e.,
              |                                                     Œ∫œï
              |                           E1 (Œ¶k , Œ®k , Ik , ¬µk ) ‚â§     min{k‚àámk (¬µk )k , ‚àÜk }
              |                                                     3Œ≤1
              |                                                     Œ∫œï
              |                           E3 (Œ¶k , Œ®k , Ik , ¬µk ) ‚â§     min{k‚àámk (¬µk )k , ‚àÜk }                        (6.53)
              |                                                     3Œ≤2
              |                                                     Œ∫œï
              |                           E4 (Œ¶k , Œ®k , Ik , ¬µk ) ‚â§     min{k‚àámk (¬µk )k , ‚àÜk }.
              |                                                     3Œ≤3
              | The purpose of the positive weights Œ≤1 , Œ≤2 , Œ≤3 , introduced in the previous section, is to balance
              | or scale the individual contributions of the error terms such the uniform split above is justified.
              | This decomposition has reduced the monolithic task of constructing a sparse grid and reduced-order
              | model such that the gradient condition in (3.15) holds to the individual tasks in (6.53). While
              | the interplay between the three error terms in (6.53) and refinement of the reduced-order model
              | and sparse grid is highly coupled and fairly complex, the following observations suggest an effective
              | training strategy: (1) for a fixed sparse grid, E1 and E3 decrease (possibly non-monotonically) as the
              | reduced-order model is hierarchically refined and (2) for a fixed reduced-order model, E4 decreases
              | (possibly non-monotonically) as the sparse grid is refined. Therefore, the construction of the reduced-
              | order model, for a fixed sparse grid, will proceed according to a variant of the classical greedy method
              | [149, 173], to target the error terms E1 and E3 . For a fixed reduced-order model, the sparse grid
              | will be adapted using the anisotropic dimension-adaptive approach [67] to target E4 . These steps
              | will be performed iteratively until the conditions in (6.53) are met. Before discussing the combined
              | algorithm in detail, the individual components, namely dimension-adaptive construction of a sparse
              | grid and greedy construction of a reduced-order model, are introduced.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 188
blank         | 
              | 
              | 
text          |    The construction of I will mimic the dimension-adaptive algorithm introduced in the seminal
              | work by Gerstner and Griebel [67] for constructing a goal-oriented, anisotropic sparse grid. In
              | this approach, the truncation error associated with the sparse grid I is approximated solely on the
              | neighbors N (I), exactly as discussed in Section 6.1.3. If this truncation error approximation is
              | larger than a specified tolerance, the multi-index in the set of neighbors that contributes most to
              | the error is added to the index set, that is, I ‚Üê I ‚à™ {i‚àó } where
blank         | 
text          |                                         i‚àó = arg min   ‚àÜi [g] .                                 (6.54)
              |                                              i‚ààN (I)
blank         | 
              | 
text          | for the integrand g : RNy ‚Üí R. In the context of the proposed two-level approximation, the
              | dimension-adaptive algorithm is applied to the integrand
blank         | 
text          |                                          k‚àáFr (¬µ; ¬∑ , Œ¶, Œ®)k
blank         | 
text          | for a fixed reduced-order model Œ¶, Œ® and given ¬µ. With this integrand, the dimension-adaptive
              | algorithm decreases the error terms E4 (Œ¶, Œ®, ¬∑ , ¬µ). While the convergence is not necessarily mono-
              |                                                                   N
              | tonic, this term approaches zero in the limiting case as I ‚Üí N+y . In fact, since E4 (Œ¶, Œ®, I, ¬µ) is
              | exactly the truncation error approximation, it is used for the convergence criteria in the algorithm.
              |    The construction of the reduced-order basis follows the well-studied greedy algorithm [149, 173].
              | The original greedy algorithm improves the parametric robustness (usually over ¬µ-space) of a
              | reduced-order basis Œ¶ by adding snapshots of the high-dimensional model at the point where the
              | reduced-order model error is largest. Regions of high error are found by evaluating the reduced-
              | order model and an inexpensive error indicator at a (possibly large) set of candidate points (in the
              | space where the ROM is being trained) and performs a direct search for maximum value of the error
              | indicator over the candidate set. A weighted variant of the greedy algorithm was developed [42] for
              | stochastic problems with non-uniform probability distributions to train a reduced-order model over
              | the stochastic space Œû. This weighted greedy method uses the probability density œÅ(y) to weight
              | the error indicator at a particular realization y ‚àà Œû since regions with significant mass will amplify
              | errors during the expectation computation. In the same work, the weighted greedy algorithm was
              | coupled with sparse grids by using the sparse grid nodes as the candidate set; since the reduced-
              | order model is only queried on the nodes of the sparse grid, it only needs to be trained at these
              | points. In the present work, a similar weighted greedy algorithm is applied to train the reduced-order
              | model over the stochastic collocation nodes (and neighbors) ŒûI‚à™N (I) for a fixed ¬µ and sparse grid
              | I. Since the gradient condition is only required to hold at the trust region center, the training is
              | performed solely in stochastic space (with ŒûI‚à™N (I) as the candidate set) for ¬µ = ¬µk fixed. Unlike
              | the traditional greedy methods, the proposed method builds a reduced-order model that accurately
              | represents primal and sensitivity or adjoint states over the training space. This is required since the
              | greedy algorithm will be responsible for reducing the primal E1 and sensitivity/adjoint E3 error terms
              | as both terms arise in the gradient error indicator œïk (¬µ). This is achieved by adding sensitivity or
              | adjoint snapshots to the reduced-order basis, in addition to the standard primal snapshots. From
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 189
blank         | 
              | 
              | 
text          | the form of the gradient error indicator œïk (¬µ) in (6.47), the primal error indicator is taken as the
              | weighted primal residual norm, i.e., the integrand in E1 ,
blank         | 
text          |                                   œÅ(y) kr(Œ¶ur (¬µ; y, Œ¶, Œ®), ¬µ, y)k                               (6.55)
blank         | 
text          | and the dual error indicator is taken as the weighted dual solution, i.e., the integrand in E3
blank         | 
text          |                                                       ‚àÇur
              |                       œÅ(y) r ‚àÇ (Œ¶ur (¬µ; y, Œ¶, Œ®), Œ¶       (¬µ; y, Œ¶, Œ®), ¬µ, y)
              |                                                       ‚àÇ¬µ                                         (6.56)
              |                       œÅ(y) r Œª (Œ¶ur (¬µ; y, Œ¶, Œ®), Œ®Œªr (¬µ; y, Œ¶, Œ®), ¬µ, y) .
blank         | 
text          | Since the error terms E1 and E3 are integrated over I ‚à™ N (I), ŒûI‚à™N (I) is used as the candidate
              | set. For a fixed point in parameter space ¬µ and sparse grid I, the greedy algorithm builds up the
              | reduced-order basis using primal and sensitivity/adjoint snapshots in the described manner, un-
              | til E1 (Œ¶, Œ®, I, ¬µ) and E3 (Œ¶, Œ®, I, ¬µ) drop below user-defined tolerances. If a minimum-residual
              | reduced-order model is employed, the algorithm is guaranteed to terminate due to the monotonic-
              | ity property (Proposition 4.1). In the limiting case where snapshots have been added for each
              | y ‚àà ŒûI‚à™N (I) , the primal reduced-order model will be exact for each y ‚àà ŒûI‚à™N (I) and thus E1 is
              | identically zero. If the reduced-order model is exact at these sparse grid nodes, the reduced sensi-
              | tivity and adjoint method possess the minimum-residual property, which (Propositions 4.2 and 4.4)
              | guarantees the reduced sensitivity/adjoint exactly reconstruct the corresponding high-dimensional
              | quantity. Therefore E3 is identically zero. If a minimum-residual reduced-order model is employed,
              | E1 (in the appropriate norm) will actually decrease monotonically since adding snapshots to the
              | reduced-order basis can only improve the approximation quality (in terms of the residual norm in
              | a particular metric). A similar argument cannot be made for E3 , even if minimum-residual sensi-
              | tivity/adjoint reduced-order models are used, since modification of Œ¶ alters the linearization point
              | defining the sensitivity/adjoint residual and the objective function in successive minimum-residual
              | optimization problems cannot be compared.
              |    The final training algorithm combines the dimension-adaptive sparse grid construction with
              | greedy sampling to build a reduced-order basis. For a fixed sparse grid I, the primal-sensitivity/adjoint
              | weighted greedy algorithm is used to build a reduced-order basis Œ¶ such that E1 (Œ¶, Œ®, I, ¬µ) and
              | E3 (Œ¶, Œ®, I, ¬µ) satisfy (6.53). This reduced-order basis is fixed and a single step of the dimension-
              | adaptive sparse grid is applied to updated I according to I ‚Üê I ‚à™ {i‚àó } where i‚àó is defined in (6.54).
              | Then the weighted greedy algorithm is applied with the new sparse grid. The algorithm proceeds
              | in this manner until E4 (Œ¶, Œ®, I, ¬µ) satisfies (6.53). Therefore the combined algorithm consists of
              | an outer loop that refines the sparse grid (to reduced truncation error, E4 ) and an inner loop that
              | builds an accurate reduced-order basis for a given sparse grid (to decrease the reduced-order model
              | error, E1 and E3 ).
              |    Algorithm 13 summarizes the combined dimension-adaptive greedy algorithm that proceeds ac-
              | cording to the above two-level iteration to improve a given sparse grid and reduced-order basis such
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 190
blank         | 
              | 
              | 
text          | that the gradient condition in (6.53) is satisfied. As stated, the algorithm implicitly requires ini-
              | tialization of each quantity. At iteration 0, the sparse grid is initialized as the uniform level-one
              | sparse grid, I = {(1, . . . , 1)}, which consists of a single node ŒûI = {0}. The reduced-order basis is
              | constructed from the primal and sensitivity/adjoint snapshot at this single sparse grid node at the
              |                                                ‚àÇu
              | first trust region center, i.e., u(¬µ0 , 0) and    (¬µ , 0) or Œª(¬µ0 , 0). That is, the sparse grid I0 and
              |                                                ‚àÇ¬µ 0
              | reduced-order model Œ¶0 , Œ®0 are constructed as
blank         | 
text          |                        Œ¶0 , Œ®0 , I0 = two-level-refine-grad(Œ¶‚àí1 , Œ®‚àí1 , I‚àí1 , ¬µ0 )                           (6.57)
blank         | 
text          | where I‚àí1 = {(1, . . . , 1)}, Œ¶‚àí1 , Œ®‚àí1 is the reduced-order model constructed with the aforemen-
              | tioned snapshots, and two-level-refine-grad is defined in Algorithm 13. For all subsequent
              | iterations, the sparse grid and reduced-basis basis are initialized from the previous iteration, i.e.,
              | the construction of Ik , Œ¶k is initialized with Ik‚àí1 , Œ¶k‚àí1
blank         | 
text          |                      Œ¶k , Œ®k , Ik = two-level-refine-grad(Œ¶k‚àí1 , Œ®k‚àí1 , Ik‚àí1 , ¬µk ).                         (6.58)
blank         | 
text          | Apart from being a natural way to initialize the dimension-adaptive greedy algorithm, it has the
              | added benefit of only refining Ik‚àí1 and Œ¶k‚àí1 if the choice Ik = Ik‚àí1 , Œ¶k = Œ¶k‚àí1 are not sufficient
              | to guarantee convergence, i.e., the gradient condition in (6.53) does not hold.
              |    This completes the discussion of the training algorithm to build Ik and Œ¶k , Œ®k to ensure the
              | gradient condition holds and attention is shifted to construction of Ik0 , Œ¶0k , Œ®0k such that the inexact
              | objective condition (3.22) holds in order to properly assess the trust region step without requiring
              | queries to F (¬µ). The error indicator for the objective decrease is a weighted sum of two terms: the
              | primal error E1 and QoI truncation error E2
blank         | 
text          |                            Œ∏k (¬µ) =Œ±1 (E1 (Œ¶0k , Œ®0k , Ik0 , ¬µk ) + E1 (Œ¶0k , Œ®0k , Ik0 , ¬µ))+
              |                                                                                                              (6.59)
              |                                       Œ±2 (E2 (Œ¶0k , Œ®0k , Ik0 , ¬µk ) + E2 (Œ¶0k , Œ®0k , Ik0 , ¬µ)),
blank         | 
text          | which involves error terms evaluated at ¬µk and ¬µÃÇk since the pointwise version of the objective
              | decrease bound is used. From Chapter 3, the error condition (3.22), i.e., Œ∏k (¬µÃÇk )œâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí
              | mk (¬µÃÇk ), rk }, is required to preserve global convergence of the trust region method when œàk (¬µ) is
              | used in place of F (¬µ) in the computation of œÅk . A sufficient condition for the objective condition to
              | hold is that each term satisfies an appropriate fraction of the condition, i.e.,
blank         | 
text          |                                                                  1                                     1/œâ
              |      E1 (Œ¶0k , Œ®0k , Ik0 , ¬µk ) + E1 (Œ¶0k , Œ®0k , Ik0 , ¬µÃÇk ) ‚â§     (Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk })
              |                                                                 2Œ±1
              |                                                                                                              (6.60)
              |                                                                  1                                     1/œâ
              |      E2 (Œ¶0k , Œ®0k , Ik0 , ¬µk ) + E2 (Œ¶0k , Œ®0k , Ik0 , ¬µÃÇk ) ‚â§     (Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk })
              |                                                                 2Œ±2
blank         | 
text          | where the positive weights Œ±1 , Œ±2 balance the contributions of E1 and E2 to justify this uniform split.
              | Therefore the monolithic task of satisfying the objective condition has been broken into the modular
              | tasks in (6.60). Similar to the approach taken to construct Ik and Œ¶k , a weighted greedy algorithm
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 191
blank         | 
              | 
              | 
              | 
text          | Algorithm 13 Refine reduced-order basis and sparse grid for gradient condition
blank         | 
text          |                             Œ¶, Œ®, I = two-level-refine-grad(Œ¶, Œ®, I, ¬µ, Œ¥)
              |  1:   Initialization: Given
              |                                 Œ¶, Œ®, I, ¬µ, Œ¥, Œ≤1 > 0, Œ≤2 > 0, Œ≤3 > 0, Œ∫œï > 0
              |                             Œ∫œï
              |  2:   while E4 (Œ¶, Œ®, I, ¬µ) >   min {kEI [‚àáFr (¬µ; ¬∑ , Œ¶, Œ®)]k , Œ¥} do
              |                             3Œ≤3
              |  3:     Refine index set: Add index set with largest contribution to truncation error
blank         | 
text          |                     I ‚Üê I ‚à™ {i‚àó }      where     i‚àó = arg max ‚àÜi [k‚àáFr (¬µ; ¬∑ , Œ¶, Œ®)k]
              |                                                        i‚ààN (I)
blank         | 
text          |                               Œ∫œï
              |  4:     while E1 (Œ¶, Œ®, I, ¬µ) >  min {kEI [‚àáFr (¬µ; ¬∑ , Œ¶, Œ®)]k , Œ¥} do
              |                              3Œ≤1
              |  5:       Evaluate primal error indicator: Greedily select y ‚àà Œûi‚àó with largest error
blank         | 
text          |                               y ‚àó = arg max œÅ(y) kr(Œ¶ur (¬µ; y, Œ¶, Œ®); ¬µ, y)k
              |                                      y‚ààŒûi‚àó
blank         | 
text          |  6:       Reduced-order model construction: Update reduced basis with new snapshots
blank         |                                             
text          |                                  ‚àÇu
              |                Œ¶ = Œ¶ u(¬µ; y ‚àó )     (¬µ; y ‚àó )          Œ¶ u(¬µ; y ‚àó ) Œª(¬µ; y ‚àó )
blank         |                                                                               
text          |                                                or
              |                                  ‚àÇ¬µ
blank         | 
text          |  7:     end while
              |                               Œ∫œï
              |  8:     while E2 (Œ¶, Œ®, I, ¬µ) >   min {kEI [‚àáFr (¬µ; ¬∑ , Œ¶, Œ®)]k , Œ¥} do
              |                               3Œ≤2
              |  9:       Evaluate dual error indicator: Greedily select y ‚àà Œûi‚àó with largest error
blank         |                                                                                
text          |               ‚àó                 ‚àÇ                        ‚àÇur
              |              y = arg max œÅ(y) r Œ¶ur (¬µ; y, Œ¶, Œ®); Œ¶          (¬µ; y, Œ¶, Œ®), ¬µ, y          or
              |                    y‚ààŒûi‚àó                                 ‚àÇ¬µ
blank         | 
text          |                  = arg max œÅ(y) r Œª (Œ¶ur (¬µ; y, Œ¶, Œ®); Œ®Œªr (¬µ; y, Œ¶, Œ®), ¬µ, y)
              |                     y‚ààŒûi‚àó
blank         | 
text          | 10:       Reduced-order model construction: Update reduced basis with new snapshots
blank         |                                             
text          |                                  ‚àÇu
              |                Œ¶ = Œ¶ u(¬µ; y ‚àó )     (¬µ; y ‚àó )          Œ¶ u(¬µ; y ‚àó ) Œª(¬µ; y ‚àó )
blank         |                                                                               
text          |                                                or
              |                                  ‚àÇ¬µ
blank         | 
text          | 11:     end while
              | 12:   end while
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 192
blank         | 
              | 
              | 
text          | will be used to enforce the conditions on E1 and the dimension-adaptive sparse grid construction to
              | satisfy the conditions on E2 .
              |    While the dimension-adaptive greedy algorithm to construct Ik0 , Œ¶0k , Œ®0k for the objective decrease
              | condition will be very similar that used to construct Ik , Œ¶k , Œ®k , there will be two critical differences.
              | First, the error terms in (6.60) involve two points in parameters space: the trust region center ¬µk and
              | the candidate step ¬µÃÇk . In contrast, the gradient condition only imposed requirements at the trust
              | region center. This has implications for both the dimension-adaptive sparse grid construction and
              | greedy method. Second, the conditions in (6.60) only impose requirements on the primal reduced-
              | order model accuracy and truncation error, whereas the gradient condition also placed requirements
              | on the sensitivity/adjoint accuracy. This implies only primal snapshots are required during the
              | greedy construction of the reduced-order model.
              |    For a given I, Œ¶, Œ®, if the truncation error conditions in (6.60), i.e., requirements on E2 , are not
              | satisfied, the sparse grid is updated according to I ‚Üê I ‚à™ {i‚àó }, where
blank         | 
text          |                   i‚àó = arg max max          ‚àÜi [Fr (¬µk ; Œ¶, Œ®)] , ‚àÜi [Fr (¬µÃÇk ; Œ¶, Œ®)]
blank         |                                                                                         
text          |                                                                                              .       (6.61)
              |                          i‚ààN (I)
blank         | 
              | 
text          | The integrands in each term is precisely the integrand of E2 at the two parameters of interest: ¬µk
              | and ¬µÃÇk . Therefore this refinement process can be repeated iteratively until the conditions on E2 in
              | (6.60) are satisfied. Following the combined dimension-adaptive greedy method introduced for the
              | gradient condition, the sparse grid refinement steps are interwoven with greedy construction of the
              | reduced-order model. For a fixed I, Œ¶, Œ®, define ¬µ‚àó ‚àà {¬µk , ¬µÃÇk } and y ‚àó ‚àà ŒûI as the quantities that
              | maximize the weighted residual-based error indicator
blank         | 
text          |                         ¬µ‚àó , y ‚àó = arg max œÅ(y) kr(Œ¶ur (¬µ; y, Œ¶, Œ®), ¬µ, y)k .                        (6.62)
              |                                    ¬µ‚àà{¬µk , ¬µÃÇk },
              |                                      y‚ààŒûI
blank         | 
text          |                                                          h                i
              | The reduced-order basis Œ¶ is updated according to Œ¶ ‚Üê Œ¶ u(¬µ‚àó , y ‚àó ) ; an optional orthogonaliza-
              | tion step is usually used to ensure the the reduced basis is full rank and the resulting reduced-order
              | model is well-conditioned. As discussed, only primal snapshot are used since the conditions in (6.60)
              | only places requirements on the primal accuracy of the reduced-order model. The argument of
              | the maximization problem in (6.62) is exactly the integrand of E1 . Assuming a minimum-residual
              | reduced-order model is used, E1 (Œ¶, Œ®, I, ¬µk ) and E1 (Œ¶, Œ®, I, ¬µÃÇk ) will monotonically decrease with
              | each iteration of the greedy method and the iterations proceed until the conditions in (6.60) are satis-
              | fied. The combined training algorithm alternates between sparse grid and reduced basis construction
              | exactly as that in Algorithm 14, namely, for a fixed sparse grid, the greedy method is applied to
              | ensure the conditions on E1 in (6.60) hold, then the reduced-order model is fixed and the sparse grid
              | is refined according to (6.62). The combined algorithm terminates when all conditions in (6.60) are
              | satisfied.
              |    Algorithm 14 summarizes the combined dimension-adaptive greedy algorithm that constructs a
              | sparse grid and reduced-order model such that the objective decrease condition (6.60) holds. Similar
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 193
blank         | 
              | 
              | 
text          | to Algorithm 13, this algorithm refines a given sparse grid and reduced basis and implicitly requires
              | initialization of each quantity. At any iteration k, the sparse grid Ik and reduced-order model Œ¶k ,
              | Œ®k are used to initialize Algorithm 14, i.e.,
blank         | 
text          |                     Œ¶0k , Œ®0k , Ik0 = two-level-refine-obj(Œ¶k , Œ®k , Ik , ¬µk , ¬µÃÇk , rk )              (6.63)
blank         | 
text          | since Ik , Œ¶k , Œ®k have been constructed to satisfy the error condition in (3.15) at ¬µk . If that
              | requirements turns out to be more restrictive than that in (3.22), the algorithm will not modify the
              | sparse grid or reduced-order basis, i.e., Ik0 = Ik and Œ¶0k = Œ¶k , and the actual-to-predicted ratio is
              | unity and acceptance of the step is guaranteed.
blank         | 
text          | Algorithm 14 Refine reduced-order basis and sparse grid for objective decrease condition
blank         | 
text          |                            Œ¶, Œ®, I = two-level-refine-obj(Œ¶, Œ®, I, ¬µ1 , ¬µ2 , s)
              |  1:   Initialization: Given
blank         | 
text          |                                        Œ¶, Œ®, I, ¬µ1 , ¬µ2 , s > 0, œâ ‚àà (0, 1)
              |  2:   while
              |                 E2 (Œ¶, Œ®, I, ¬µ1 ) + E2 (Œ¶, Œ®, I, ¬µ2 ) >
              |                               1                                                                  1/œâ
              |                                   (Œ∑ min {EI [Fr (¬µ1 ; ¬∑ , Œ¶, Œ®)] ‚àí EI [Fr (¬µ2 ; ¬∑ , Œ¶, Œ®)] , s})
              |                              2Œ±2
              |       do
              |  3:    Refine index set: Add index set with largest contribution to truncation error
blank         | 
text          |       I ‚Üê I ‚à™ {i‚àó }            i‚àó = arg max max ‚àÜi Fr (¬µ1 ; ¬∑ , Œ¶, Œ®) , ‚àÜi Fr (¬µ2 ; ¬∑ , Œ¶, Œ®)
blank         |                                                                                              
text          |                      where
              |                                             i‚ààN (I)
blank         | 
text          |  4:     while
              |                 E1 (Œ¶, Œ®, I, ¬µ1 ) + E1 (Œ¶, Œ®, I, ¬µ2 ) >
              |                               1                                                                  1/œâ
              |                                   (Œ∑ min {EI [Fr (¬µ1 ; ¬∑ , Œ¶, Œ®)] ‚àí EI [Fr (¬µ2 ; ¬∑ , Œ¶, Œ®)] , s})
              |                              2Œ±1
              |       do
              |  5:        Evaluate error indicator: Greedily select ¬µ ‚àà {¬µ1 , ¬µ2 }, y ‚àà Œûi‚àó with the largest error
blank         | 
text          |                             ¬µ‚àó , y ‚àó = arg max œÅ(y) kr(Œ¶ur (¬µ; y, Œ¶, Œ®); ¬µ, y)k
              |                                        ¬µ‚àà{¬µ1 , ¬µ2 }
              |                                          y‚ààŒûi‚àó
blank         | 
text          |  6:        Reduced-order model construction: Update reduced basis with new snapshot
blank         | 
text          |                                      Œ¶ = Œ¶ u(¬µ‚àó ; y ‚àó )
blank         |                                                        
              | 
              | 
meta          |  7:     end while
              |  8:   end while
title         | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 194
blank         | 
              | 
              | 
title         | 6.3.3     Summary
text          | The proposed multifidelity trust region method for efficient stochastic PDE-constrained optimization
              | leverages the trust region framework of Chapter 3 and the two-level approximation of risk-averse
              | measures of PDE quantities of interest using anisotropic sparse grids and projection-based model
              | reduction. The ingredients required for the trust region algorithm in the present context were
              | introduced in Section 6.3.1 and summarized below
blank         | 
text          |                        mk (¬µ) = EIk [Fr (¬µ; ¬∑ , Œ¶k , Œ®k )]
              |                         œëk (¬µ) = k¬µ ‚àí ¬µk k
              |                         œïk (¬µ) = Œ≤1 E1 (Œ¶k , Œ®k , Ik , ¬µ) + Œ≤2 E3 (Œ¶k , Œ®k , Ik , ¬µ)+
              |                                  Œ≤3 E4 (Œ¶k , Œ®k , Ik , ¬µ)                                            (6.64)
              |                         œàk (¬µ) = EIk0 Fr (¬µ; ¬∑ , Œ¶0k , Œ®0k )
blank         |                                                            
              | 
text          |                         Œ∏k (¬µ) = Œ±1 (E1 (Œ¶0k , Œ®0k , Ik0 , ¬µk ) + E1 (Œ¶0k , Œ®0k , Ik0 , ¬µ))+
              |                                   Œ±2 (E2 (Œ¶0k , Œ®0k , Ik0 , ¬µk ) + E2 (Œ¶0k , Œ®0k , Ik0 , ¬µ)).
blank         | 
text          | The sparse grid Ik and reduced-order model Œ¶k , Œ®k are constructed using the dimension-adaptive
              | greedy algorithm (Algorithm 13) to ensure the gradient condition (6.53) is satisfied. The algorithm
              | is initialized from the sparse grid and reduced-order model from the previous iteration
blank         | 
text          |                    Œ¶k , Œ®k , Ik = two-level-refine-grad(Œ¶k‚àí1 , Œ®k‚àí1 , Ik‚àí1 , ¬µk )                    (6.65)
blank         | 
text          | in the event the quantities satisfy the gradient condition without refinement, e.g., if a small step is
              | taken, which would save queries to the high-dimensional model. Once the dimension-adaptive greedy
              | algorithm has been applied to satisfy the gradient condition (3.15), the objective decrease condition
              | in (3.14) holds trivially since œëk (¬µ) is taken as the classical trust region constraint (Section 3.1.1).
              | The sparse grid Ik0 and reduced-order model Œ¶0k , Œ®0k defining the inexact objective decrease used
              | in the computation of œÅk are constructed using a similar dimension-adaptive greedy algorithm (Al-
              | gorithm 13). In this case, primal high-dimensional model snapshots are used to reduce the error
              | terms E1 and E2 at two parameters‚Äîthe trust region center ¬µk and candidate step ¬µÃÇk ‚Äîto satisfy
              | the objective error condition (6.60)
blank         | 
text          |                    Œ¶0k , Œ®0k , Ik0 = two-level-refine-obj(Œ¶k , Œ®k , Ik , ¬µk , ¬µÃÇk , rk ).            (6.66)
blank         | 
text          | This initialization of the dimension-adaptive algorithm will take Ik0 = Ik , Œ¶0k = Œ¶k , Œ®0k = Œ®k if
              | they satisfy the objective error condition, which may save substantial computational resources as
              | it will eliminate (possibly many) queries to realizations of the high-dimensional model. With these
              | choices, it is clear from (6.43) and (6.49) that mk (¬µk ) = œàk (¬µk ) and mk (¬µÃÇk ) = œàk (¬µÃÇk ). This implies
              | œÅk is unity and the step can be accepted with no additional work.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 195
blank         | 
              | 
              | 
text          |    The complete multifidelity trust region algorithm, including inexact evaluation of the actual-
              | to-predicted ratio using œàk (¬µ), is presented in Algorithm 16. Global convergence is not strictly
              | guaranteed since the error indicators œïk (¬µ) and Œ∏k (¬µ) do not necessarily lead to bounds of the form
              | (3.13) and (3.21) due to the approximation EI c ‚âà EN (I) in (6.35) and (6.36)-(6.37). However, even
              | though the bounds cannot be rigorously established in the general case, the fact that the bounds are
              | only required up to an arbitrary constant leaves hope they will hold in specific situations of interest.
              | The numerical results in the next section provide evidence that this is the case since convergence is
              | observed.
              |    Future work will consider the incorporation of partially converged solutions as snapshots in the
              | construction of Œ¶k and Œ¶0k in Algorithms 15 and 16 to further improve the efficiency of the proposed
              | multifidelity trust region method. This will build on the idea introduced in Chapter 5; however, the
              | implications on global convergence of the trust region framework will be more complicated to analyze
              | since another layer of complexity is present, i.e., risk-averse measures of quantities of interest.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 196
blank         | 
              | 
              | 
              | 
text          | Algorithm 15 Trust region method based on reduced-order models and sparse grids
              |  1:   Initialization: Given
              |                              ¬µ0 , ‚àÜ0 , 0 < Œ≥ < 1, ‚àÜmax > 0, 0 < Œ∑1 < Œ∑2 < 1, Œ∫œï > 0,
blank         |                                                                       
text          |                                            I‚àí1 = {0}, Œ¶‚àí1 = u(¬µ0 ; 0)
blank         | 
text          |  2:   Model and constraint update: If the previous model is sufficient for convergence
blank         | 
text          |                                        œïk‚àí1 (¬µk ) ‚â§ Œ∫œï min{k‚àámk‚àí1 (¬µk )k , ‚àÜk },
blank         | 
text          |       re-use for the current iteration: mk (¬µ) := mk‚àí1 (¬µ) and œëk (¬µ) = k¬µ ‚àí ¬µk k. Otherwise, refine
              |       the reduced-order model and sparse grid using two-level dimension adaptive greedy method
blank         | 
text          |                      Œ¶k , Œ®k , Ik = two-level-refine-grad(Œ¶k‚àí1 , Œ®k‚àí1 , Ik‚àí1 , ¬µk , ‚àÜk )
blank         | 
text          |       and define model and constraint as
              |                                     mk (¬µ) = EIk [f (Œ¶k ur (¬µ; ¬∑ , Œ¶k , Œ®k ), ¬µ, ¬∑ )]
              |                                        œëk (¬µ) = k¬µ ‚àí ¬µk k
blank         | 
text          |  3:   Step computation: Approximately solve the trust region subproblem
blank         | 
text          |                                        min mk (¬µ)       subject to      œëk (¬µ) ‚â§ ‚àÜk
              |                                     ¬µ‚ààRN
blank         | 
text          |     for a candidate, ¬µÃÇk , to satisfy the fraction of Cauchy decrease
              |  4: Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio
blank         | 
text          |                                                         F (¬µk ) ‚àí F (¬µÃÇk )
              |                                                 œÅk =
              |                                                        mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
text          |  5:   Step acceptance:
blank         | 
text          |                 if        œÅk ‚â• Œ∑1        then       ¬µk+1 = ¬µÃÇk       else     ¬µk+1 = ¬µk    end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                      if     œÅk ‚â§ Œ∑ 1              then        ‚àÜk+1 ‚àà (0, Œ≥œëk (¬µÃÇk )]      end if
              |                      if     œÅk ‚àà (Œ∑1 , Œ∑2 )       then        ‚àÜk+1 ‚àà [Œ≥œëk (¬µÃÇk ), ‚àÜk ]    end if
              |                      if     œÅk ‚â• Œ∑ 2              then        ‚àÜk+1 ‚àà [‚àÜk , ‚àÜmax ]         end if
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 197
blank         | 
              | 
              | 
              | 
text          | Algorithm 16 Trust region method based on reduced-order models and sparse grids with inexact
              | objective evaluations
              |  1:   Initialization: Given
              |                              ¬µ0 , ‚àÜ0 , 0 < Œ≥ < 1, ‚àÜmax > 0, 0 < Œ∑1 < Œ∑2 < 1, Œ∫œï > 0,
              |                             = {0}, Œ¶‚àí1 = u(¬µ0 ; 0) , œâ ‚àà (0, 1), {rk }‚àû
blank         |                                                    
text          |                       I‚àí1                                             k=1 such that rk ‚Üí 0
blank         | 
text          |  2:   Model and constraint update: If the previous model is sufficient for convergence
blank         | 
text          |                                         œïk‚àí1 (¬µk ) ‚â§ Œ∫œï min{k‚àámk‚àí1 (¬µk )k , ‚àÜk },
blank         | 
text          |       re-use for the current iteration: mk (¬µ) := mk‚àí1 (¬µ) and œëk (¬µ) = k¬µ ‚àí ¬µk k. Otherwise, refine
              |       the reduced-order model and sparse grid using two-level dimension adaptive greedy method
blank         | 
text          |                      Œ¶k , Œ®k , Ik = two-level-refine-grad(Œ¶k‚àí1 , Œ®k‚àí1 , Ik‚àí1 , ¬µk , ‚àÜk )
blank         | 
text          |       and define model and constraint as
              |                                      mk (¬µ) = EIk [f (Œ¶k ur (¬µ; ¬∑ , Œ¶k , Œ®k ), ¬µ, ¬∑ )]
              |                                         œëk (¬µ) = k¬µ ‚àí ¬µk k
blank         | 
text          |  3:   Step computation: Approximately solve the trust region subproblem
blank         | 
text          |                                         min mk (¬µ)       subject to       œëk (¬µ) ‚â§ ‚àÜk
              |                                      ¬µ‚ààRN
blank         | 
text          |     for a candidate, ¬µÃÇk , to satisfy the fraction of Cauchy decrease
              |  4: Computed-to-predicted reduction: Compute computed-to-predicted reduction ratio
              |                      Ô£±
              |                      Ô£¥
              |                      Ô£¥             1            if œëk (¬µÃÇk )œâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }
              |                      Ô£¥
              |                      Ô£≤
              |                œÅk =
              |                          œà (¬µ ) ‚àí œàk (¬µÃÇk )
              |                      Ô£≥ k k
              |                      Ô£¥
              |                                                 otherwise
              |                      Ô£¥
              |                      Ô£¥
              |                         mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
text          |       where
              |                                 œàk (¬µ) := EIk0 f (Œ¶0k ur (¬µ; ¬∑ , Œ¶0k , Œ®0k ), ¬µ, ¬∑ )
blank         |                                                                                    
              | 
text          |                           Œ¶0k , Œ®0k , Ik0 = two-level-refine-obj(Œ¶k , Œ®k , Ik , ¬µk , ¬µÃÇk , rk )
              |  5:   Step acceptance:
blank         | 
text          |                 if        œÅk ‚â• Œ∑ 1        then       ¬µk+1 = ¬µÃÇk       else       ¬µk+1 = ¬µk    end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                      if      œÅk ‚â§ Œ∑ 1              then        ‚àÜk+1 ‚àà (0, Œ≥œëk (¬µÃÇk )]        end if
              |                      if      œÅk ‚àà (Œ∑1 , Œ∑2 )       then        ‚àÜk+1 ‚àà [Œ≥œëk (¬µÃÇk ), ‚àÜk ]      end if
              |                      if      œÅk ‚â• Œ∑ 2              then        ‚àÜk+1 ‚àà [‚àÜk , ‚àÜmax ]           end if
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 198
blank         | 
              | 
              | 
title         | 6.4     Numerical Experiment: Optimal Control of the Viscous
              |         Burgers‚Äô Equation with Uncertain Coefficients
text          | This section studies the performance of the proposed algorithms (Algorithms 15 and 16) on a simple
              | stochastic PDE-constrained optimization problem: optimal control of the one-dimensional viscous
              | Burgers‚Äô equation with uncertain coefficients. This is precisely the stochastic counterpart to the
              | problem in Section 5.5.2 used to study the deterministic trust region algorithm based on reduced-
              | order models in Chapter 5. The optimization problem takes the form
              |                                       Z   1                                        1              
              |                                                1                            Œ±
              |                          Z                                                      Z
              |              minimize          œÅ(y)              (u(¬µ, y, x) ‚àí uÃÑ(x))2 dx +             z(¬µ, x)2 dx dy   (6.67)
              |                  n¬µ
              |                ¬µ‚ààR         Œû           0       2                            2   0
blank         | 
              | 
text          | where u(¬µ, y, x) is the solution of the following parametrization of the one-dimensional viscous
              | Burgers‚Äô equation
blank         | 
text          |         ‚àíŒΩ(y)‚àÇxx u(¬µ, y, x) + u(¬µ, y, x)‚àÇx u(¬µ, y, x) = z(¬µ, x) x ‚àà (0, 1),                      y‚ààŒû
              |                                                                                                          (6.68)
              |                       u(¬µ, y, 0) = d0 (y)            u(¬µ, y, 1) = d1 (y).
blank         | 
text          | corresponding to the realization y ‚àà Œû . As in Section 5.5.2, the target state is uÃÑ(x) ‚â° 1 and the
              | regularization parameter is Œ± = 10‚àí3 . This is the risk-neutral optimal control problem. A three-
              | dimensional stochastic space, Œû = [‚àí1, 1]3 , is chosen to introduce stochasticity into the viscosity
              | and boundary conditions
blank         | 
text          |                                                                  y2                      y3
              |                       ŒΩ(y) = 10y1 ‚àí2             d0 (y) = 1 +            d1 (y) =            .
              |                                                                 1000                    1000
blank         | 
text          | A uniform probability distribution, œÅ(y)dy = 2‚àí3 dy, is chosen for simplicity, although any distribu-
              | tion could be used. The source term, or control, z(¬µ, x) is defined by 50 cubic splines with clamped
              | boundary conditions, which leads to 53 optimization variables. This stochastic optimal control prob-
              | lem is nearly identical to the one studied in [108, 109], with two exceptions being that the authors
              | in [108, 109]: (1) considered one additional stochastic parameter governing a forcing term in (6.68)
              | and (2) used a larger optimization parameter space consisting of the nodes of the underlying finite
              | element shape functions. In all numerical experiments, the partial differential equation in (6.68) is
              | discretized with 500 linear finite elements for a state space of dimension Nu = 499, after application
              | of the essential boundary conditions.
              |    The initial guess for the optimal control problem taken in all numerical experiments is the
              | constant: z(¬µ0 , x) ‚â° 1. Figure 6.5 contains several different controls and the corresponding solution
              | statistics of (6.68), including those corresponding to the optimal deterministic (y = 0) and stochastic
              | control. It is clear that the including stochasticity in the optimization formulation has a non-trivial
              | impact on the optimal solution obtained. Furthermore, the stochastic formulation allows statistics
              | of the solution and quantities of interest to be considered. The remainder of this section is devoted
              | to studying the methods proposed in Algorithms 15 and 16 and comparing its performance to three
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 199
blank         | 
              | 
              | 
text          |            4                                                            1.5
blank         | 
              | 
              | 
              | 
text          |                                                        E[u(¬µ, ¬∑ , x)]
              | z(¬µ, x)                                                                  1
              |            2
              |                                                                         0.5
              |            0                                                             0
              |                                                                     ‚àí0.5
              |                0   0.2    0.4       0.6   0.8    1                            0   0.2   0.4       0.6   0.8   1
              |                                 x                                                             x
blank         | 
              | 
text          | Figure 6.5: Left: the control defining the initial guess for the optimization problem (       ), the
              | solution of the deterministic optimal control problem, i.e., with the stochastic variables fixed at
              | their mean value y = 0 (      ), and the solution of the stochastic optimal control problem (      ).
              | Right: the mean solution of the viscous Burgers‚Äô equation in (6.68) at the initial control (       ),
              | optimal deterministic control (    ), and the optimal stochastic control. One (    ) and two (      )
              | standard deviations about the mean solution corresponding to the optimal stochastic control are
              | also included.
blank         | 
              | 
text          | baseline methods.
              |           The first method applied to the solve the stochastic optimization problem in (6.67) is Algo-
              | rithm 15. Since the true function evaluations F (¬µ) are unavailable (the expectation cannot be
              | computed exactly), it is approximated using a level 5 isotropic sparse grid. This amounts to a trust
              | region method where inexactness is only used for the gradients, i.e., Algorithm 1 of Chapter 3.
              | The work in [108] considers an identical trust region method for stochastic optimization, except
              | the authors use an approximation model based solely on dimension-adaptive sparse grids, while the
              | proposed method also employs projection-based reduced-order models. The reduced-order models
              | considered in all numerical experiments use a Galerkin projection and, due to the large number
              | of optimization variables, the adjoint method is used to compute gradients of reduced quantities
              | of interest. To promote accuracy of the primal and adjoint solutions with respect to the HDM
              | counterparts, the trial basis is constructed from primal and adjoint snapshots according. Such a
              | reduced-order model does not guarantee the minimum-residual property (Definition 4.1) since the
              | Jacobians of (6.68) are not SPD. However, the numerical experiments suggest that it is important
              | for the reduced-order model gradients to possess discrete consistency to properly converge the trust
              | region subproblem and ensure global convergence, particularly when there are a large number of op-
              | timization variables. This will be referred to a method MI in the remainder. The second stochastic
              | optimization solver employed is exactly the method in Algorithm 16, including the approximation
              | of the actual-to-predicted reduction ratio. This will be referred to as method MII. These two meth-
              | ods are expected to converge similarly since they are built on the same approximation framework;
              | however, MI will require far more queries to the HDM since it employs a fine isotropic sparse grid
              | to evaluate œÅk . It is possible that method MII will generate an inaccurate approximation of œÅk
              | and will incorrectly accept or reject a step. It will be seen that does not occur in this numerical
              | experiment.
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 200
blank         | 
              | 
              | 
              | 
text          | 10‚àí2                                                     10‚àí2
blank         | 
              | 
text          | 10‚àí5                                                     10‚àí5
blank         | 
              | 
text          | 10‚àí8                                                     10‚àí8
              |        0        1         2         3         4                 0       1         2         3         4
              |                     Major iteration                                         Major iteration
blank         | 
              | 
text          | Figure 6.6: Convergence history of the objective error quantities using MI (left) and MII (right):
              | |F (¬µk ) ‚àí F (¬µ‚àó )| ( ), |F (¬µÃÇk ) ‚àí F (¬µ‚àó )| ( ), |mk (¬µk ) ‚àí F (¬µ‚àó )| ( ), |mk (¬µÃÇk ) ‚àí F (¬µ‚àó )| ( ).
              | Rapid progress is made toward the optimal solution, despite poor agreement between the objective
              | and model at early iterations.
blank         | 
              | 
text          |    To assess the performance of these proposed stochastic optimization solvers, three baseline meth-
              | ods will be used for comparison. The first method is the naive approach of using a fixed level 5
              | isotropic sparse grid to integrate the quantity of interest and the optimization problem is solved with
              | the L-BFGS algorithm. This method will be denoted BI. The second, BII, and third, BIII, methods
              | are the dimension-adaptive sparse grid approaches of [108] and [109], respectively. The stochastic
              | optimization solvers MI, MII, BII, BIII are all trust region methods that use the Steihaug-Toint
              | CG [48] method to approximate the solution of the trust region subproblem and use the parameters
              | in (5.57).
              |    The convergence history of the proposed trust region methods MI and MII are shown in Fig-
              | ure 6.6 and Tables 6.1‚Äì6.2. Both methods are converging to a first-order critical point (k‚àáF (¬µk )k ‚Üí
              | 0); after only 5 trust region iterations the first-order optimality condition has reduced 4 orders of
              | magnitude from the initial, sub-optimal control. At early iterations, the approximation model,
              | mk (¬µ), and true objective, F (¬µ), do not exhibit good agreement, even at trust region centers. In
              | fact, from the tables, they do not even agree in the first digit. Despite this lack of agreement, the
              | candidate step found by the approximation model leads to reasonable reduction in the true objective
              | function. As a local minima is approached, the bound in (3.15) places more stringent requirements
              | on the model error and, as a result, the approximation model shows excellent agreement with the
              | objective function. The behavior of methods MI and MII are nearly identical since they rely on
              | the same approximation model and error indicators in the trust region method. The only difference
              | is the computation of œÅk and, even though MII uses the approximation in (3.20) to compute œÅk , it
              | never falsely accepts or rejects a step; see Tables 6.1‚Äì6.2.
              |    In contrast to the values of the approximation model and objective function, the gradient norms
              | do show reasonable agreement, even at early iterations, which can be seen in Figure 6.7. This is
              | to be expected since the refinement method in Algorithm 13 targets the gradient error. Both the
              | values and gradients of the approximation model and objective do not show good agreement at the
              | candidate step ¬µÃÇk , which is also expected since, at iteration k, the sparse grid and reduced-order
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 201
blank         | 
              | 
              | 
              | 
text          |  10‚àí2                                                                        10‚àí2
blank         | 
              | 
text          |  10‚àí4                                                                        10‚àí4
blank         | 
              | 
text          |  10‚àí6                                                                        10‚àí6
              |                            0        1         2         3      4                                   0        1         2         3   4
              |                                         Major iteration                                                         Major iteration
blank         | 
              | 
text          | Figure 6.7: Convergence history of the gradient quantities using MI (left) and MII (right):
              | k‚àáF (¬µk )k (  ), k‚àáF (¬µÃÇk )k (  ), k‚àámk (¬µk )k (  ), k‚àámk (¬µÃÇk )k ( ).
blank         | 
text          |                          104                                                                      104
blank         | 
              | 
              | 
              | 
text          |                                                                         Adjoint HDM evaluations
              | Primal HDM evaluations
blank         | 
              | 
              | 
              | 
text          |                          103                                                                      103
blank         | 
text          |                          102                                                                      102
blank         | 
text          |                          101                                                                      101
blank         | 
text          |                          100                                                                      100
              |                                1          2            3           4                                    1         2            3        4
              |                                          Major iterations                                                        Major iterations
blank         | 
              | 
text          | Figure 6.8: Cumulative number of HDM primal and adjoint evaluations as the major iterations in
              | the various trust region algorithms progress: BII ( ), BIII ( ), MI (     ), MII (     ).
blank         | 
              | 
text          | model were only trained at the trust region center.
              |                          Figure 6.8 provide further insight to behavior of the two proposed methods (MI and MII) in
              | comparison to the trust region-based baseline methods (BII and BIII). All methods are based on the
              | trust region framework in Algorithms 1 and 2 of Chapter 3 that are adapted from the work in [108,
              | 109] and therefore possess the same concept of a major iteration. Figure 6.8 shows the cumulative
              | number of queries to the high-dimensional model as the major iterations progress. The methods
              | BII and BIII require more HDM queries (primal and adjoint) than their counterparts in MI and
              | MII since their trust region model problems rely solely on HDM queries on an anisotropic sparse
              | grid while MI, MII replace these with ROM queries. Another observation is that the BIII requires
              | fewer primal HDM queries than BII and the same number of adjoint queries. This is expected since
              | they both use the same approximation model in the trust region subproblem (implies same number
              | of adjoint queries), but BIII uses inexact objective evaluations to evaluate the actual-to-predicted
              | reduction ratio (implies fewer primal queries). A similar observation holds when comparing MI and
              | MII for the same reason.
              |                          The reduction in the number of HDM queries realized by the proposed methods MI and MII
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 202
blank         | 
              | 
              | 
text          |                                                                              400
              | Primal ROM evaluations
              |                                3,000                                         300
blank         | 
text          |                                2,000
              |                                                                              200
              |                                1,000
              |                                                                              100
              |                                         0
              |                                                                                0
              |                                             0   1          2        3    4         0    20       40       60
blank         | 
              | 
text          |                                                                              200
              |             Adjoint ROM evaluations
blank         | 
              | 
              | 
              | 
text          |                                       400
              |                                                                              150
blank         | 
text          |                                                                              100
              |                                       200
blank         | 
text          |                                                                               50
              |                                         0
              |                                                                                0
              |                                             0   1          2         3   4         0    20       40       60
              |                                                     Major iterations                     ROM size (ku )
blank         | 
              | 
text          | Figure 6.9: Left: Cumulative number of primal and adjoint ROM evaluations as the major iterations
              | in the various trust region algorithms progress. Right: Number of primal and adjoint ROM queries
              | organized according to the size of the reduced-order basis (ku ). Trust region methods considered:
              | MI (     ), MII (     ).
blank         | 
              | 
text          | comes at the price of a large number of ROM queries. This can be seen from Figure 6.9 that includes
              | the cumulative number of queries to the primal and adjoint ROM. Since the size of the reduced-order
              | model constantly changes as these algorithms progress, the number of queries to a reduced-order
              | model of a given size is also presented in Figure 6.9. Method MII requires nearly three times as
              | many primal reduced-order queries as MI, but nearly the same number of adjoint queries. This
              | comes from the fact that MII uses a (possibly) refined reduced-order model to approximation œÅk ,
              | while MI uses high-dimensional model queries to compute it exactly. This also explains the fact
              | that larger reduced-order models are required for MII and nearly all of these large reduced-order
              | models are only called upon for a primal solve only, i.e., few adjoint solves for reduced-order models
              | of size > 40.
              |                           Since the proposed method MI and MII and the baseline methods BI‚ÄìBIII have different
              | sources of cost, i.e., HDM and ROM evaluations versus only HDM evaluations, care must be taken
              | when assessing the performance of the methods. The ultimate cost metric of interest is wall time;
              | however, this one-dimensional model problem will not be representative of the speedups that can be
              | CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 203
blank         | 
              | 
              | 
              | 
text          |                 10‚àí2                                                10‚àí2
              | |F (¬µ) ‚àí F (¬µ‚àó )|
              |                                                                     10‚àí3
blank         | 
              | 
              | 
              | 
text          |                                                              k‚àáF (¬µ)k
              |                 10‚àí3
              |                                                                     10‚àí4
              |                 10‚àí4                                                10‚àí5
              |                                                                     10‚àí6
              |                      ‚àí5
              |                 10
              |                                                                     10‚àí7 1
              |                       101   102   103      104       105               10     102   103     104        105
              |                                   Cost                                              Cost
blank         | 
              | 
text          | Figure 6.10: Convergence of the objective function (left) and gradient (right) as a function of the
              | cost metric in (6.69) for method MIII for several values of the speedup factor of the reduced-order
              | model: œÑ = 1 (      ), œÑ = 10 (     ), œÑ = 100 (  ), œÑ = ‚àû (      ). The baseline methods used for
              | comparison: BI (       ) and BIII (      ).
blank         | 
              | 
text          | realized by methods MI and MII. Due to the small problem size and the fact that hyperreduction
              | has not been included to reduce the complexity associated with the nonlinear term, queries to the
              | reduced-order model are only marginally less expensive than the HDM queries. For larger problems
              | that include hyperreduction, the motivation for this work, ROM queries have been shown [198] to
              | be one to five orders of magnitude less expensive than HDM queries. To assess the speedups that
              | can be realized by this method, the following simple cost model is introduced
blank         | 
text          |                                      C = nhp + nha /2 + œÑ ‚àí1 (nrp + nra /2)                   (6.69)
blank         | 
text          | where C is the total cost associated with a particular method in the units of equivalent number
              | of primal HDM queries, nhp is the number of primal HDM queries, nha is the number of adjoint
              | HDM queries, nrp is the number of primal ROM queries, nra is the number of adjoint ROM queries,
              | and œÑ is the ratio of the cost of a primal HDM query to a primal ROM query. This cost model
              | assumes a primal solve is twice as expensive as an adjoint solve and a primal HDM solve is œÑ times
              | as expensive as a primal ROM solve. Figure 6.10 shows the evolution of the objective function
              | and gradient as a function of the cost metric in (6.69) for the baseline methods BI, BIII and the
              | proposed method MII for œÑ = 1, 10, 100, ‚àû. Even for slow reduced-order models (œÑ = 1), MII
              | exhibits faster convergence than the brute-force baseline method BI; however, it converges more
              | slowly than the state-of-the-art method BIII. For a modest ROM speedup of œÑ = 10, MII is more
              | than 5√ó less expensive than BIII, i.e., for a given value of the objective function or gradient, the
              | cost of MII is less than a fifth of BIII. For fast reduced-order models (œÑ = 100), MII is an order
              | of magnitude more efficient than BIII. An upper bound on the improvement attainable by MII
              | compared to BIII is slightly greater than an order of magnitude, which is seen from the limiting
              | case of free reduced-order model (œÑ = ‚àû) in Figure 6.10.
              | Table 6.1: Convergence history of Algorithm 15 applied to the optimal control of the stochastic Burgers‚Äô equation in (6.67).
blank         | 
text          |           F (¬µk )      mk (¬µk )      F (¬µÃÇk )    mk (¬µÃÇk )    k‚àáF (¬µk )k       œÅk            ‚àÜk         Success?
              |         6.6506e-02    7.2694e-02   5.3655e-02   5.9922e-02    2.2959e-02   1.0062e+00   1.0000e+02     1.0000e+00
              |         5.3655e-02    5.9593e-02   5.0783e-02   5.7152e-02    2.3424e-03   1.1765e+00   2.0000e+02     1.0000e+00
              |         5.0783e-02    5.0670e-02   5.0412e-02   5.0292e-02    1.9724e-03   9.8344e-01   4.0000e+02     1.0000e+00
              |         5.0412e-02    5.0292e-02   5.0405e-02   5.0284e-02    9.2654e-05   8.7408e-01   8.0000e+02     1.0000e+00
              |         5.0405e-02    5.0404e-02   5.0403e-02   5.0401e-02    8.3139e-05   9.9873e-01   1.6000e+03     1.0000e+00
              |         5.0403e-02    5.0401e-02        -            -        2.2846e-06        -            -              -
blank         | 
              | 
              | 
              | 
text          | Table 6.2: Convergence history of Algorithm 16 applied to the optimal control of the stochastic Burgers‚Äô equation in (6.67).
blank         | 
text          |            F (¬µk )     mk (¬µk )      F (¬µÃÇk )     mk (¬µÃÇk )   k‚àáF (¬µk )k       œÅk            ‚àÜk         Success?
              |          6.6506e-02   7.2694e-02   5.3655e-02   5.9922e-02    2.2959e-02   1.0257e+00    1.0000e+02    1.0000e+00
              |          5.3655e-02   5.9593e-02   5.0783e-02   5.7152e-02    2.3424e-03   9.7512e-01    2.0000e+02    1.0000e+00
              |          5.0783e-02   5.0670e-02   5.0412e-02   5.0292e-02    1.9724e-03   9.8351e-01    4.0000e+02    1.0000e+00
              |          5.0412e-02   5.0292e-02   5.0405e-02   5.0284e-02    9.2654e-05   8.7479e-01    8.0000e+02    1.0000e+00
              |          5.0405e-02   5.0404e-02   5.0403e-02   5.0401e-02    8.3139e-05   9.9946e-01    1.6000e+03    1.0000e+00
              |          5.0403e-02   5.0401e-02        -            -        2.2846e-06        -             -             -
meta          |                                                                                                                                CHAPTER 6. STOCHASTIC PDE OPTIMIZATION WITH ROMS AND SPARSE GRIDS 204
title         | Chapter 7
blank         | 
title         | Conclusions
blank         | 
title         | 7.1     Summary and Conclusions
text          | The primary contributions of this thesis are two-fold: (1) the development of an efficient solver
              | for deterministic PDE-constrained optimization problems that leverages projection-based reduced-
              | order models and partially converged PDE solutions and (2) the development of an efficient solver
              | for stochastic PDE-constrained optimization problems that leverages projection-based reduced-order
              | models and anisotropic sparse grids. These primary contributions were built on two independent
              | auxiliary contributions that have applications that extend well beyond the scope of this thesis:
              | (1) the introduction of a globally convergent, highly flexible generalized trust region method for
              | managing efficient approximation models and (2) the generalization and extension of minimum-
              | residual projection-based reduced-order models [115, 28, 31, 89] to sensitivity and adjoint PDEs.
              |    The multifidelity trust region method introduced in Chapter 3 extends traditional trust region
              | methods by allowing generalized trust region constraints to be used, provided the relationship in
              | (3.12) between the approximation model decrease error and the trust region constraint can be
              | established. This method is said to be a ‚Äúgeneralized‚Äù trust region since the traditional trust
              | region constraint, i.e., a ball in RN¬µ , satisfies the required relationships and is therefore a valid
              | constraint in the proposed method. The trust region method is closely based on the methods in
              | [108, 109] that does not require first-order consistency with the objective function and allows an
              | approximation model to be used in the computation of the actual-to-predicted reduction. This
              | flexibility is significant since the resulting method in Algorithm 2 does not explicitly depend on the
              | expensive objective function F (¬µ); however, construction of the approximation models mk (¬µ) and
              | œàk (¬µ) will likely require (inexact) evaluations of F (¬µ). Furthermore, the inexactness conditions
              | adopted from [93, 108, 109] allow for asymptotic error bounds between the true and approximated
              | quantities, which provides considerable flexibility in the approximation models that can be used.
              | Even though the trust region framework was developed in the unconstrained setting, it can be
              | embedded in an augmented Lagrangian framework to handle nonlinear equality constraints. This
blank         | 
              | 
              | 
meta          |                                                  205
              | CHAPTER 7. CONCLUSIONS                                                                            206
blank         | 
              | 
              | 
text          | multifidelity trust region method, or trust region model management framework, constitutes one of
              | the pillars of this thesis from which the primary contributions regarding deterministic and stochastic
              | PDE-constrained optimization in Chapters 5 and 6 follow. The second pillar is the primary PDE
              | approximation technology employed in this work: projection-based model reduction.
              |    While the concept of minimum-residual projection-based reduced-order models is not new [115,
              | 28, 31, 89], this work contributes to the understanding of this technology and extends it to apply
              | to sensitivity and adjoint PDEs. The primary factors that motivate the use of minimum-residual
              | reduced-order models‚Äîoptimality, monotonicity, and interpolation‚Äîare stated and proved in Propo-
              | sition 4.1, 4.2, 4.4 for the primal, sensitivity, and adjoint PDEs. For the primal PDE, these concepts
              | are well-known from previous work [31], but have only been sparingly explored [210] in the sensi-
              | tivity/adjoint settings. These properties are crucial when reduced-order models are combined with
              | the generalized trust region method of Chapter 3 as the trust region convergence theory places
              | specific requirements on the accuracy of the approximation model at trust region centers, which
              | is closely linked to these minimum-residual properties and the construction of the reduced-order
              | bases. Propositions 4.3 and 4.5 are particularly important contributions of this thesis to the model
              | reduction literature as they state conditions under which minimum-residual sensitivities/adjoints co-
              | incide with the true sensitivities/adjoints of the reduced-order model. These results provide insight
              | into the construction of the reduced-order basis and ensures the true reduced-order model sensitivi-
              | ties/adjoints possess the minimum-residual properties (optimality, monotonicity, and interpolation).
              | This is particularly important in the context of optimization since it guarantees the minimum-
              | residual sensitivities/adjoints will lead to consistent gradients of QoIs based on the reduced-order
              | model, which is extensively leveraged in the deterministic and stochastic PDE-constrained optimiza-
              | tion methods of Chapters 5 and 6. Finally, the minimum-residual sensitivities/adjoints are much
              | easier to implement and compute than their exact counterparts when a non-constant test basis is
              | used since they do not require second derivatives of the governing equations. These results sur-
              | rounding minimum-residual reduced-order models were extended to the case of collocation-based
              | hyperreduction where the residual minimization occurs only over the subset of the degrees of free-
              | dom in the mask. Weaker versions of the crucial propositions mentioned above were established in
              | this setting (Propositions 4.6 ‚Äì 4.8).
              |    These two technologies‚Äîthe generalized trust region method and minimum-residual projection-
              | based reduced-order models‚Äîserve as pillars for the primary contributions of the thesis: efficient
              | optimization methods for deterministic and stochastic PDE-constrained optimization. The proposed
              | method for deterministic PDE-constrained optimization uses projection-based reduced-order mod-
              | els as the approximation model in the generalized trust region method and residual-based error
              | indicators (Appendix B justifies the use of residual-based error indicators as error bounds). The
              | minimum-residual properties of the reduced-order models, as well as the compression algorithms in
              | Section 4.3, are used to build a ROM that exactly satisfies the error conditions in (3.14), (3.15),
              | which guarantees global convergence. The flexibility of the trust region framework is leveraged to
              | use partially converged PDE solutions as snapshots for the reduced-order model and to approximate
meta          | CHAPTER 7. CONCLUSIONS                                                                              207
blank         | 
              | 
              | 
text          | the actual-to-predicted reduction. The proposed method is applied to a number of PDE-constrained
              | optimization problems in fluid mechanics. The large-scale industrial example of aerodynamic shape
              | optimization of the Common Research Model demonstrated the potential of the proposed method
              | to be 1.6√ó faster than a state-of-the-art PDE-constrained optimization solver.
              |    The multifidelity trust region method proposed as an efficient solver for stochastic PDE-constrained
              | optimization problems in Chapter 6 requires a second level of inexactness to efficiently integrate quan-
              | tities of interest over the stochastic space to form risk measures (Section 2.2.1). This lead to the
              | development of the two-level approximation of risk measures of PDE quantities of interest that uses
              | dimension-adaptive anisotropic sparse grids to perform efficient integration in the stochastic space
              | and model reduction for efficient PDE queries at each collocation node. This two-level approximation
              | was used to define the approximation model in the multifidelity trust region method and suitable
              | error indicators were derived that take both the model reduction error and integral truncation er-
              | ror into account. Global convergence is established by employing a two-level dimension-adaptive
              | greedy algorithm to simultaneously construct the sparse grid and reduced-order basis to satisfy the
              | error conditions (3.14), (3.15). The proposed method directly extends the work in [108, 109] that
              | only defines the approximation model using dimension-adaptive sparse grids with PDE queries at
              | collocation nodes performed using the HDM. It is also similar to [44, 42, 43] that employs the same
              | two-level approximation, but embeds it in an offline-online framework and claims regarding conver-
              | gence only apply to simple PDEs. The numerical experiment in Chapter 6 demonstrate the promise
              | of this method as a 500-fold reduction in the cost metric (6.69), compared to using a fine isotropic
              | sparse grid without reduced-order models to perform the stochastic optimization, was realized. Even
              | compared to the method in [109] that is considered state-of-the-art, a 10-fold reduction in the cost
              | metric was realized.
blank         | 
              | 
title         | 7.2     Prospective Future Work
text          | This thesis leaves a variety of research issues and spin-off projects that constitute promising avenues
              | of future research. These research direction include:
blank         | 
text          |    ‚Ä¢ Possible improvements to the proposed methods. A number of possible improvements to the
              |       various methods proposed in this thesis are apparent. The first is a theoretical matter to
              |       extend the liminf statement on global convergence in Appendix A to the stronger lim conver-
              |       gence. Another independent issue that should be addressed is the complete formulation of the
              |       minimum-residual adjoint equations for the collocation-based hyperreduced models. As was
              |       pointed out in Section 4.2.6, this is delicate due to the differences between the mask and sample
              |       mesh that become a factor when considering the transpose of the Jacobian (as required by the
              |       adjoint residual). Another possible enhancement that would have a positive and widespread
              |       impact across the methods proposed in this thesis is the use of improved, faster, and possibly
              |       probabilistic, [15] error indicators. In the trust region framework, these can either be used as
meta          | CHAPTER 7. CONCLUSIONS                                                                          208
blank         | 
              | 
              | 
text          |     the trust region constraint or as the gradient error indicator. Finally, in the context of opti-
              |     mization under uncertainty, the use of sparse grids implicitly assumes the risk measures are
              |     sufficiently smooth, which eliminates many of the most interesting and relevant risk measures
              |     in Section 2.2.1. To enable the use of these alternate risk measures, future work should focus
              |     on an alternative construction of collocation nodes that explicitly deals with non-smoothness.
blank         | 
text          |   ‚Ä¢ Extension to problems with large-scale parameter spaces, N¬µ = O(Nu ). All of the methods
              |     developed in this document assume there are few parameters compared to the dimension
              |     of the state vector, i.e., N¬µ  Nu , since reduction was only applied to the state vector.
              |     To handle the more complicated case where N¬µ = O(Nu ) that arises in applications such
              |     as topology optimization or inverse problems, reduction of some form must be applied to
              |     the parameter space as well. Evaluation of the reduced-order model will require at least
              |     O(N¬µ ) operations, particularly if the parameters define coefficient of the underlying PDE,
              |     e.g., material properties, and this will constitute a major bottleneck when there are O(Nu )
              |     parameters. One possible method that exploits low-dimensional search spaces employed by
              |     individual iterations of linesearch and subspace methods is outlined in Appendix C.
blank         | 
text          |   ‚Ä¢ Extension to time-dependent problems, possibly with periodicity constraints. In this thesis, all
              |     problems considered in Chapters 5 and 6 were static. However, optimization problems governed
              |     by time-dependent PDEs (Appendix D) would benefit most from a multifidelity approach such
              |     as the ones proposed in this thesis due to their extreme computational cost and the plethora of
              |     training information generated, even after a single query to the HDM. Extension of this work
              |     to time-dependent problems will require the development of inexpensive error indicators for
              |     the primal and sensitivity/dual; however, the optimization methods themselves do not need
              |     to be modified since they are agnostic to the form of the underlying PDE (only work with
              |     quantities of interest and their gradients).
meta          | Appendix A
blank         | 
title         | Global Convergence Proof:
              | Error-Aware Trust Region Method
blank         | 
text          | This section provides the global convergence theory for the error-aware, multifidelity trust region
              | method in Algorithm 2 for the solution of the unconstrained optimization problem
blank         | 
text          |                                           minimize F (¬µ).
              |                                             ¬µ‚ààRN¬µ
blank         | 
              | 
text          | It largely parallels the convergence theory in [133, 108, 109] with required changes to handle the
              | error-aware trust regions. At iteration k, define the approximation model mk : RN¬µ ‚Üí R and the
              | error indicators œëk : RN¬µ ‚Üí R+ and œïk : RN¬µ ‚Üí R+ such that
blank         | 
text          |                     |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ Œ∂œëk (¬µ)ŒΩ      ¬µ ‚àà Rk                (A.1)
              |                                  k‚àáF (¬µk ) ‚àí ‚àámk (¬µk )k ‚â§ Œæœïk (¬µk )                             (A.2)
blank         | 
text          | where Œ∂, Œæ > 0 are arbitrary constants, ŒΩ > 1, {¬µk } is the sequences of iterates produced by the
              | Algorithm 2, and Rk = {¬µ ‚àà RN¬µ | œëk (¬µ) ‚â§ ‚àÜk } are the sublevel sets of the error indicator œëk (¬µ).
              | Furthermore, require the approximation is refined such that the error indicators satisfy the following
              | conditions at trust region centers
blank         | 
text          |                                 œëk (¬µk ) ‚â§ Œ∫œë ‚àÜk                                                (A.3)
              |                                 œïk (¬µk ) ‚â§ Œ∫œï min{k‚àámk (¬µk )k , ‚àÜk },                           (A.4)
blank         | 
text          | where Œ∫œë ‚àà (0, 1) and Œ∫œï > 0 are algorithmic constants. Additionally, define an approximation
              | model for the objective function œàk : RN¬µ ‚Üí R and corresponding error indicator Œ∏k : RN¬µ ‚Üí R+
              | that satisfy
              |                             |F (¬µk ) ‚àí F (¬µ) + œàk (¬µ) ‚àí œàk (¬µk )| ‚â§ œÉŒ∏k (¬µ)                     (A.5)
blank         | 
              | 
              | 
meta          |                                                    209
              |   APPENDIX A. GLOBAL CONVERGENCE PROOF                                                              210
blank         | 
              | 
              | 
text          |   where œÉ > 0 is an arbitrary constant. Finally, require the objective approximation is refined such
              |   that the error indicators satisfies
blank         | 
text          |                                  Œ∏k (¬µÃÇk )œâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }                    (A.6)
blank         | 
text          |   where Œ∑ < min{Œ∑1 , 1 ‚àí Œ∑2 }, œâ ‚àà (0, 1) and 0 < Œ∑1 < Œ∑2 < 1 are algorithmic constants, {rk }‚àû
              |                                                                                               k=1 such
              |   that rk ‚Üí 0 is a forcing sequence, and ¬µÃÇk is the solution of the trust region subproblem at iteration
              |   k
              |                                           minimize         mk (¬µ)
              |                                            ¬µ‚ààRN¬µ
blank         | 
text          |                                           subject to œëk (¬µ) ‚â§ ‚àÜk .
blank         | 
text          |   Before proceeding the main content of this section, the global convergence proof of Algorithm 2,
              |   additional assumptions are introduced on the regularity and boundedness of the objective function
              |   F (¬µ) and approximation model mk (¬µ) in Assumptions A.1 and A.2.
blank         | 
text          |   Assumption A.1 (Objective function assumptions).
blank         | 
text          | (AF1) F : RN¬µ ‚Üí R is twice-continuously differentiable on RN¬µ
blank         | 
text          | (AF2) F (¬µ) is bounded below on RN¬µ , i.e., there exists Œ∫lbf > 0 such that, for all ¬µ ‚àà RN¬µ ,
blank         | 
text          |                                                      F (¬µ) ‚â• Œ∫lbf
blank         | 
              | 
text          |   Assumption A.2 (Approximation model assumptions).
blank         | 
text          | (AM1) mk : RN¬µ ‚Üí R is twice-continuously differentiable on RN¬µ
blank         | 
text          | (AM2) The Hessian of the model remains bounded within the trust region, i.e.,
blank         | 
text          |                                         Œ≤k := 1 + sup       ‚àá2 mk (¬µ) ‚â§ Œ∫umh
              |                                                     ¬µ‚ààRk
blank         | 
              | 
text          |         where Œ∫umh ‚â• 1
blank         | 
text          | (AM3) œëk : RN¬µ ‚Üí R is continuously differentiable on RN¬µ
blank         | 
text          | (AM4) The directional derivative of the constraint in any direction pk is bounded in the trust region,
              |         i.e.,
              |                                                      ‚àÇœëk
              |                                               sup        (¬µ)pk ‚â§ Œ∫‚àáœë
              |                                              ¬µ‚ààRk    ‚àÇ¬µ
              |         where Œ∫‚àáœë > 0
blank         | 
text          | (AM5) The trust region subproblem
              |                                              minimize        mk (¬µ)
              |                                                ¬µ‚ààRN¬µ
blank         | 
text          |                                              subject to œëk (¬µ) ‚â§ ‚àÜk .
meta          | APPENDIX A. GLOBAL CONVERGENCE PROOF                                                             211
blank         | 
              | 
              | 
text          |       has a local solution, which is guaranteed if mk (¬µ) has a local minima in the interior of Rk or
              |       Rk is compact and a local solution lies on the boundary of the trust region ‚àÇRk := {¬µ ‚àà RN¬µ |
              |       œëk (¬µ) = ‚àÜk }.
blank         | 
text          | Lemma A.1 (Circumscribe ball with radius proportional to ‚àÜk inside Rk ). Assume (AM3)‚Äì(AM4)
              | hold. Then,
              |                         Dk := {¬µ ‚àà RN¬µ | k¬µ ‚àí ¬µk k2 ‚â§ (1 ‚àí Œ∫œë )Œ∫‚àí1
              |                                                                 ‚àáœë ‚àÜk } ‚äÜ Rk .                  (A.7)
blank         | 
text          | Proof. Let pk be an arbitrary unit vector and take ¬µ ‚àà Dk such that ¬µ = ¬µk + Œ±pk . From the
              | definition of Dk in (A.7), Œ± ‚â§ (1 ‚àí Œ∫œë )Œ∫‚àí1
              |                                          ‚àáœë ‚àÜk . The mean value theorem, bound on œëk (¬µk ), and
              | bound on the directional derivatives of œëk (¬µ) lead to
blank         | 
text          |                                                             ‚àÇœëk
              |                    œëk (¬µ) = œëk (¬µk + Œ±pk ) = œëk (¬µk ) + Œ±       (Œ∂)pk ‚â§ Œ∫œë ‚àÜk + Œ±Œ∫‚àáœë            (A.8)
              |                                                             ‚àÇ¬µ
blank         | 
text          | where Œ∂ = ¬µk + œÑ Œ±pk for some œÑ ‚àà [0, 1]. The bound on Œ± that results from ¬µ ‚àà Dk , along with the
              | relation in (A.8) leads to
              |                                              œëk (¬µ) ‚â§ ‚àÜk
blank         | 
text          | and therefore ¬µ ‚àà Rk . Thus, Dk ‚äÜ Rk .
blank         | 
text          | Lemma A.2 (Fraction of Cauchy Decrease). Assume (AM1) and (AM3)‚Äì(AM4) hold. Then there
              | exists ¬µ ‚àà Rk such that
blank         | 
text          |                                                             k‚àámk (¬µk )k
blank         |                                                                                            
text          |               mk (¬µk ) ‚àí mk (¬µ) ‚â• Œ∫s k‚àámk (¬µk )k min                    , (1 ‚àí Œ∫œë )Œ∫‚àí1
              |                                                                                     ‚àáœë ‚àÜk       (A.9)
              |                                                                Œ≤k
blank         | 
text          | for Œ∫s ‚àà (0, 1).
blank         | 
text          | Proof. From Theorem 6.3.3 of [48], there exists ¬µ ‚àà Dk such that holds
blank         | 
text          |                                                                     k‚àámk (¬µk )k
blank         |                                                                                  
text          |                        mk (¬µk ) ‚àí mk (¬µ) ‚â• Œ∫s k‚àámk (¬µk )k min                   ,Œ¥ ,
              |                                                                        Œ≤k
blank         | 
text          | where Œ∫s ‚àà (0, 1) and Œ¥ = (1 ‚àí Œ∫œë )Œ∫‚àí1
              |                                     ‚àáœë ‚àÜk is the radius of Dk . From Lemma A.1, Dk ‚äÜ Rk .
              | Therefore there exists ¬µ ‚àà Rk such that (A.9) holds.
blank         | 
text          | Lemma A.3. Assume (AM1), (AM3)‚Äì(AM4), (AM5) hold. Then the solution of the optimization
              | problem
              |                                         minimize   mk (¬µ)
              |                                          ¬µ‚ààRN¬µ
blank         | 
text          |                                         subject to œëk (¬µ) ‚â§ ‚àÜk
blank         | 
text          | satisfies
              |                                                             k‚àámk (¬µk )k
blank         |                                                                                            
text          |               mk (¬µk ) ‚àí mk (¬µ) ‚â• Œ∫s k‚àámk (¬µk )k min                    , (1 ‚àí Œ∫œë )Œ∫‚àí1
              |                                                                                     ‚àáœë ‚àÜk
              |                                                                Œ≤k
meta          | APPENDIX A. GLOBAL CONVERGENCE PROOF                                                                          212
blank         | 
              | 
              | 
text          | Proof. From Assumption (AM5), a solution of the optimization problem exists. By Lemma A.2,
              | there exists a point in the feasible set of the optimization problem, i.e., Rk , that satisfies (A.9).
              | The (global) solution of the optimization problem must realize (at least) the same reduction in the
              | objective function, which leads to the desired result.
blank         | 
text          | Lemma A.4. If the objective approximation error bound (A.5) and accuracy condition (A.6) hold,
              | then for k sufficiently large
blank         | 
text          |                 |F (¬µk ) ‚àí F (¬µÃÇk ) + œàk (¬µÃÇk ) ‚àí œàk (¬µk )| ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }
blank         | 
text          | Proof. The forcing sequence, {rk }, in the bound on Œ∏k implies Œ∏k ‚Üí 0. Therefore, for sufficiently
              | large k, Œ∏k ‚â§ œÉ ‚àí1/(1‚àíœâ) . Then, (A.5), Œ∏k ‚â§ œÉ ‚àí1/(1‚àíœâ) , and (A.6) lead to the desired result
blank         | 
text          |                                                                (1‚àíœâ)
              |   |F (¬µk ) ‚àí F (¬µÃÇk ) + œàk (¬µÃÇk ) ‚àí œàk (¬µk )| ‚â§ œÉŒ∏k = œÉŒ∏kœâ Œ∏k          ‚â§ Œ∏kœâ ‚â§ Œ∑ min{mk (¬µk ) ‚àí mk (¬µÃÇk ), rk }.
blank         | 
              | 
              | 
              | 
text          | Lemma A.5. If the objective approximation error bound (A.5) and accuracy condition (A.6) hold,
              | then for k sufficiently large
blank         | 
text          |                                           F (¬µk ) ‚àí F (¬µÃÇk )
              |                                 œÅ‚àók :=                        ‚àà [œÅk ‚àí Œ∑, œÅk + Œ∑],
              |                                          mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
text          | where
              |                                                    œàk (¬µk ) ‚àí œàk (¬µÃÇk )
              |                                             œÅk =                        .
              |                                                    mk (¬µk ) ‚àí mk (¬µÃÇk )
              | Proof. For sufficiently large k,
blank         | 
text          |                                                       F (¬µk ) ‚àí F (¬µÃÇk ) + œàk (¬µÃÇk ) ‚àí œàk (¬µk )
              |                    œÅ‚àók = œÅk + (œÅ‚àók ‚àí œÅk ) = œÅk +                                                .
              |                                                                mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
text          | Then, Lemma A.4 leads to the desired result
blank         | 
text          |                                          |F (¬µk ) ‚àí F (¬µÃÇk ) + œàk (¬µÃÇk ) ‚àí œàk (¬µk )|
              |                          |œÅ‚àók ‚àí œÅk | ‚â§                                               ‚â§ Œ∑.
              |                                                    mk (¬µk ) ‚àí mk (¬µÃÇk )
blank         | 
              | 
              | 
text          | Lemma A.6. Assume (AF2) and (AM1)‚Äì(AM4) hold and suppose there exists  > 0 such that
              | k‚àámk (¬µk )k ‚â•  for k sufficiently large. Then the sequence of trust region radii {‚àÜk } produced by
              | Algorithm 2 satisfies
              |                                                    ‚àû
              |                                                    X
              |                                                          ‚àÜk < ‚àû.
              |                                                    k=1
meta          | APPENDIX A. GLOBAL CONVERGENCE PROOF                                                                                   213
blank         | 
              | 
              | 
text          | Proof. If there are only a finite number of successful iterations, there exists K > 0 such that all
              | iterations k > K are unsuccessful. Then,
blank         | 
text          |                                                    ‚àû
              |                                                    X            K
              |                                                                 X             ‚àû
              |                                                                               X
              |                                                          ‚àÜk =         ‚àÜk +           ‚àÜk
              |                                                    k=1          k=1          k=K+1
              |                                                                        ‚àû
              |                                                                        X
              |                                                            =C+                ‚àÜk
              |                                                                       k=K+1
blank         | 
              | 
text          |                  K
              |                  X                                                                                                ‚àû
              |                                                                                                                   X
              | where C =              ‚àÜk < ‚àû. Since iterations k > K are unsuccessful, ‚àÜk+1 ‚â§ Œ≥‚àÜk and                                ‚àÜk is
              |                  k=1                                                                                          k=K+1
              | bounded above by a geometric series, implying the infinite sum is finite. Therefore, the result holds
              | if there are a finite number of successful iterations. If there is an infinite sequence of successful
              | iterations {ki }, then for sufficiently large i
blank         | 
text          |              F (¬µki ) ‚àí F (¬µÃÇki ) ‚â• œàki (¬µki ) ‚àí œàki (¬µÃÇki ) ‚àí Œ∑(mki (¬µki ) ‚àí mki (¬µÃÇki ))
              |                                    ‚â• (Œ∑1 ‚àí Œ∑)(mki (¬µki ) ‚àí mki (¬µÃÇki ))
              |                                                                     (                                             )
              |                                                                       ‚àámki (¬µki )
              |                                    ‚â• (Œ∑1 ‚àí Œ∑)Œ∫s ‚àámki (¬µki ) min                   , (1 ‚àí Œ∫œë )Œ∫‚àí1
              |                                                                                               ‚àáœë ‚àÜki
              |                                                                           Œ≤k
blank         |                                                                               
text          |                                                                         ‚àí1
              |                                    ‚â• (Œ∑1 ‚àí Œ∑)Œ∫s  min        , (1 ‚àí Œ∫œë )Œ∫‚àáœë ‚àÜki .
              |                                                         Œ∫umh
blank         | 
text          | The first inequality follows from Lemma A.5, the second from the step acceptance condition in Al-
              | gorithm 2, the third from the fraction of Cauchy decrease (A.9), and the fourth from the assumption
              | that k‚àámk (¬µk )k ‚â• . Summing over all i sufficiently large
blank         |                                                                              
text          |                                 X                  
              |                  (Œ∑1 ‚àí Œ∑)Œ∫s          min              , (1 ‚àí   Œ∫œë )Œ∫‚àí1
              |                                                                      ‚àáœë ‚àÜki       ‚â§ F (¬µkI ) ‚àí lim F (¬µki ) < ‚àû
              |                                                 Œ∫umh                                             i‚Üí‚àû
              |                                 i‚â•I
blank         | 
              | 
text          | where the finiteness of the limit follows from F being bounded below. Since /Œ∫umh is bounded
              |                                                  P‚àû
              | away from zero, the inequality above implies that i=1 ‚àÜki < ‚àû.
              |       Let S ‚äÇ N be the ordered set of indicies of successful iterations. For every k ‚àà
              |                                                                                      / S, ‚àÜk ‚â§
              |     k‚àíj(k)
              | Œ≥            ‚àÜj(k) where j(k) ‚àà S is the largest index such that j(k) < k, i.e. j(k) represents the last
              | successful iteration before the unsuccessful iteration k. Summing over all k ‚àà
              |                                                                              / S,
              |                                          ‚àû                                                  ‚àû              ‚àû
              |  X              X                        X             X                               1 X              1 X
              |        ‚àÜk ‚â§           Œ≥ k‚àíj(k) ‚àÜj(k) =                           Œ≥ k‚àíj(i) ‚àÜj(i) ‚â§              ‚àÜj(i) =       ‚àÜk < ‚àû.
              |                                             i=1 j(i)<k<j(i+1)
              |                                                                                      1 ‚àí Œ≥ i=1         1‚àíŒ≥
              |  k‚ààS
              |   /             k‚ààS
              |                  /                                                                                          k‚ààS
blank         | 
              | 
text          | Then,
              |                              ‚àû               ‚àû                                        ‚àû
              |                                                                                       X
              |                              X               X            X                    1
              |                                    ‚àÜk =            ‚àÜk +         ‚àÜk ‚â§    1+                      ‚àÜk < ‚àû.
              |                                                                               1‚àíŒ≥
              |                              k=1             k‚ààS          k‚ààS
              |                                                            /                              k‚ààS
blank         | 
text          | This proves the desired result.
meta          | APPENDIX A. GLOBAL CONVERGENCE PROOF                                                                  214
blank         | 
              | 
              | 
text          | Lemma A.7. Assume (AF2) and (AM1)‚Äì(AM4) hold and suppose there exists  > 0 such that
              | k‚àámk (¬µk )k ‚â•  for k sufficiently large. Then the ratios, {œÅk }, produced by Algorithm 2, converge
              | to one.
blank         | 
text          | Proof. From the asymptotic error bound on the approximation model in (A.1) and the fact that the
              | candidate step lies within the trust region, i.e., ¬µÃÇk ‚àà Rk , it follows that
blank         | 
text          |                       |F (¬µk ) ‚àí F (¬µÃÇk ) + mk (¬µÃÇk ) ‚àí mk (¬µk )| ‚â§ Œ∂œëk (¬µÃÇk )ŒΩ ‚â§ Œ∂‚àÜŒΩk .           (A.10)
blank         | 
text          | From the Lemma A.2 and the convergence criteria on the trust region subproblem in Algorithm 2,
              | we have
              |                                                               k‚àámk (¬µk )k
blank         |                                                                                             
text          |               mk (¬µk ) ‚àí mk (¬µÃÇk ) ‚â• Œ∫s k‚àámk (¬µk )k min                   , (1 ‚àí Œ∫œë )Œ∫‚àí1
              |                                                                                       ‚àáœë ‚àÜ k   .
              |                                                                  Œ≤k
              | Then, for sufficiently large k,
blank         | 
text          |                                   mk (¬µk ) ‚àí mk (¬µÃÇk ) ‚â• (1 ‚àí Œ∫œë )Œ∫‚àí1
              |                                                                    ‚àáœë Œ∫s ‚àÜk
blank         | 
              | 
text          | due to Lemma A.6 and the assumption that k‚àámk (¬µ)k ‚â• . Combining these above inequalities
              | leads to
blank         | 
text          |               F (¬µk ) ‚àí F (¬µÃÇk ) + mk (¬µÃÇk ) ‚àí mk (¬µk )          Œ∂‚àÜŒΩk                  Œ∂
              |  |œÅk ‚àí 1| =                                             ‚â§           ‚àí1        =                   ‚àÜkŒΩ‚àí1 .
              |                         mk (¬µk ) ‚àí mk (¬µÃÇk )              (1 ‚àí Œ∫œë )Œ∫‚àáœë Œ∫s ‚àÜk   (1 ‚àí Œ∫œë )Œ∫‚àí1
              |                                                                                           ‚àáœë Œ∫s 
blank         | 
              | 
text          | Therefore, œÅk ‚Üí 1 since ‚àÜk ‚Üí 0 (Lemma A.6) and ŒΩ > 1.
blank         | 
text          | Theorem A.1. Assume (AF1)‚Äì(AF2), (AM1)‚Äì(AM4) hold. Let {¬µk } be the sequence of iterates
              | produced by Algorithm 2 and {mk } the corresponding models. Then
blank         | 
text          |                               lim inf k‚àámk (¬µk )k = lim inf k‚àáF (¬µk )k = 0.
              |                                k‚Üí‚àû                     k‚Üí‚àû
blank         | 
              | 
text          | Proof. For contradiction, suppose there exists  > 0 such that kmk (¬µk )k ‚â• . By Lemma A.7, there
              | exists K > 0 such that for all k > K, œÅk is sufficiently close to 1 and the corresponding step is
              | successful. From Algorithm 2, this implies ‚àÜK ‚â§ ‚àÜk ‚â§ ‚àÜmax . This result contradicts Lemma A.6
              | and we must have
              |                                           lim inf k‚àámk (¬µk )k = 0.
              |                                            k‚Üí‚àû
blank         | 
text          | From the triangle inequality and (A.2),
blank         | 
text          |               k‚àáF (¬µk )k ‚â§ k‚àámk (¬µk )k + k‚àámk (¬µk ) ‚àí ‚àáF (¬µk )k ‚â§ (1 + Œæ) k‚àámk (¬µk )k
blank         | 
text          | which implies
              |                                            lim inf k‚àáF (¬µk )k = 0.
              |                                             k‚Üí‚àû
meta          | Appendix B
blank         | 
title         | Residual-Based Error Bounds
blank         | 
text          | In this section, computable error bounds on quantities of interest and their gradients are sought that
              | will enable reduced-order models to be used in the multifidelity trust region framework introduced in
              | Chapter 3. Global convergence (Appendix A) of the trust region method in Algorithms 1‚Äì2 requires
              | asymptotic error bounds of the form
blank         | 
text          |                    |F (¬µk ) ‚àí F (¬µ) + mk (¬µ) ‚àí mk (¬µk )| ‚â§ Œ∂œëk (¬µ)ŒΩ           ¬µ ‚àà Rk
              |                                   k‚àáF (¬µ) ‚àí ‚àámk (¬µ)k ‚â§ Œæœïk (¬µ)                ¬µ ‚àà Nk ,
blank         | 
text          | where Œ∂, Œæ > 0 are arbitrary constants, ŒΩ > 1, Rk = {¬µ ‚àà RN¬µ | œëk (¬µ) ‚â§ ‚àÜk }, and Nk is any
              | open neighborhood of ¬µk . The constants Œ∂, Œæ do not need to be small or even computable since
              | they are never used in the trust region algorithm and global convergence is only predicated on their
              | existence. Two key points about these error bounds that substantially reduces the burden of deriving
              | error indicators œëk (¬µ) and œïk (¬µ) are: (1) they do not need to have high effectivity to ensure global
              | convergence and (2) they are not required to hold in the entire parameter space, only in the bounded
              | sets Rk (assumed) and Nk . Therefore, this section will consider general residual-based error bounds
              | that hold in bounded subsets of the parameter space since they are easily derived and computed,
              | even though they are known to have poor effectivity.
              |    To facilitate the derivation of the residual-based error bounds, recall the following definition
              | from (2.90) that uses an approximate primal solution u ‚àà RNu and sensitivity w ‚àà RNu √óN¬µ to
              | reconstruct the gradient of the quantity of interest
blank         | 
text          |                                                  ‚àÇf          ‚àÇf
              |                               g ‚àÇ (u, w, ¬µ) :=      (u, ¬µ) +    (u, ¬µ)w.                        (B.1)
              |                                                  ‚àÇ¬µ          ‚àÇu
blank         | 
text          | Similarly, from (2.102), an approximate adjoint solution z ‚àà RNu can be used to approximate the
              | gradient of the QoI as
              |                                                  ‚àÇf              ‚àÇr
              |                              g Œª (u, z, ¬µ) :=       (u, ¬µ) + z T    (u, ¬µ).                     (B.2)
              |                                                  ‚àÇ¬µ              ‚àÇ¬µ
blank         | 
              | 
              | 
meta          |                                                     215
              |  APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                                216
blank         | 
              | 
              | 
text          |  Given these definitions, it is clear that
blank         |                                                        
text          |                                                ‚àÇu
              |                             ‚àáF (¬µ) = g ‚àÇ u(¬µ),    (¬µ), ¬µ = g Œª (u(¬µ), Œª(¬µ), ¬µ).                       (B.3)
              |                                                ‚àÇ¬µ
blank         | 
text          |      Before proceeding to the derivation of the residual-based error bounds, an operator D is defined
              |  in Definition B.1 that represents the Jacobian of the nonlinear residual integrated between states
              |  u1 and u2 for a fixed parameter ¬µ.
blank         | 
text          |  Definition B.1. Define D : RNu √ó RNu √ó RN¬µ ‚Üí RNu √ó RNu as
blank         | 
text          |                                                       1
              |                                                           ‚àÇr
              |                                                   Z
              |                                D(u1 , u2 , ¬µ) =              (u2 + t(u1 ‚àí u2 ), ¬µ) dt.                (B.4)
              |                                                   0       ‚àÇu
blank         | 
text          |  Remark. In the special case where r(u, ¬µ) is linear in its first argument, i.e., r(u, ¬µ) = A(¬µ)u+b,
              |  then D(u1 , u2 , ¬µ) = A(¬µ).
blank         | 
text          |      The following assumptions are introduced on the nonlinear operator defining the system of equa-
              |  tions and the quantity of interest.
blank         | 
text          |  Assumption B.1 (Nonlinear system assumptions). Let U ‚äÇ RNu and V ‚äÇ RN¬µ be bounded subsets
blank         | 
text          | (AR1) r : U √ó V ‚Üí RNu is continuously differentiable
              |                      ‚àÇr
              | (AR2) The Jacobian,     (u, ¬µ), is invertible for all u ‚àà U and ¬µ ‚àà V
              |                      ‚àÇu
              |                                     ‚àÇr
              | (AR3) The inverse of the Jacobian,     (u, ¬µ)‚àí1 is bounded for all u ‚àà U and ¬µ ‚àà V
              |                                     ‚àÇu
              | (AR4) The matrix D(u1 , u2 , ¬µ) defined in (B.4) is invertible for all u1 , u2 ‚àà U and ¬µ ‚àà V
blank         | 
text          | (AR5) The matrix D(u1 , u2 , ¬µ)‚àí1 is bounded for all u1 , u2 ‚àà U and ¬µ ‚àà V
              |                                      ‚àÇr
              | (AR6) The parameter Jacobian,           (u, ¬µ), is bounded for all u ‚àà U and ¬µ ‚àà V
              |                                      ‚àÇ¬µ
              |                                            ‚àÇr
              | (AR7) The Jacobian and its transpose,          (u, ¬µ), are Lipschitz continuous in its first argument over
              |                                            ‚àÇu
              |        U , i.e., there exists a constant c‚àÇu r > 0 such that
blank         | 
text          |                                      ‚àÇr            ‚àÇr
              |                                         (u1 , ¬µ) ‚àí    (u2 , ¬µ) ‚â§ c‚àÇu r ku1 ‚àí u2 k
              |                                      ‚àÇu            ‚àÇu
blank         | 
text          |        for all u1 , u2 ‚àà U and ¬µ ‚àà V
              |                                     ‚àÇr
              | (AR8) The parameter Jacobain,           (u, ¬µ), is Lipschitz continuous in its first argument over U , i.e.,
              |                                     ‚àÇ¬µ
              |        there exists c‚àÇ¬µ r   > 0 such that
blank         | 
text          |                                      ‚àÇr            ‚àÇr
              |                                         (u1 , ¬µ) ‚àí    (u2 , ¬µ) ‚â§ c‚àÇ¬µ r ku1 ‚àí u2 k
              |                                      ‚àÇ¬µ            ‚àÇ¬µ
blank         | 
text          |        for all u1 , u2 ‚àà U and ¬µ ‚àà V
meta          |  APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                                  217
blank         | 
              | 
              | 
text          |  Assumption B.2 (Quantity of interest assumptions).              Let U ‚äÇ RNu and V ‚äÇ RN¬µ be bounded
              |  subsets
blank         | 
text          | (AQ1) f : U √ó V ‚Üí R is continuously differentiable
blank         | 
text          | (AQ2) f : U √ó V ‚Üí R is Lipschitz continuous with respect to its first argument, i.e., there exists a
              |         constant cf > 0 such that
blank         | 
text          |                                       |f (u1 , ¬µ) ‚àí f (u2 , ¬µ)| ‚â§ cf ku1 ‚àí u2 k                         (B.5)
blank         | 
text          |         ‚àÇf
              | (AQ3)       : U √ó V ‚Üí RNu is bounded and Lipschitz continuous with respect to its first argument, i.e.,
              |         ‚àÇu
              |         there exists a constant c‚àÇu f > 0 such that
blank         | 
text          |                                     ‚àÇf            ‚àÇf
              |                                        (u1 , ¬µ) ‚àí    (u2 , ¬µ) ‚â§ c‚àÇu f ku1 ‚àí u2 k                        (B.6)
              |                                     ‚àÇu            ‚àÇu
blank         | 
text          |         ‚àÇf
              | (AQ4)       : U √ó V ‚Üí RN¬µ is Lipschitz continuous with respect to its first argument, i.e., there exists
              |         ‚àÇ¬µ
              |         a constant c‚àÇ¬µ f > 0 such that
blank         | 
text          |                                     ‚àÇf            ‚àÇf
              |                                        (u1 , ¬µ) ‚àí    (u2 , ¬µ) ‚â§ c‚àÇ¬µ f ku1 ‚àí u2 k                        (B.7)
              |                                     ‚àÇ¬µ            ‚àÇ¬µ
blank         | 
text          |      Finally, the sets U ‚àó , W ‚àó , Z ‚àó are introduced as the set of primal, sensitivity, and adjoint solutions
              |  of the governing equations over a bounded set V of the parameter space.
blank         | 
text          |  Definition B.2. Let V ‚äÜ RN¬µ be a bounded set and define
blank         | 
text          |                                            U ‚àó = {u(¬µ) | ¬µ ‚àà V }.
blank         | 
text          |  Furthermore, it is assumed that U ‚àó is bounded.
blank         | 
text          |  Definition B.3. Let V ‚äÜ RN¬µ be a bounded set and define
blank         |                                                                     
text          |                                             ‚àó       ‚àÇu
              |                                          W =           (¬µ) | ¬µ ‚àà V       .
              |                                                     ‚àÇ¬µ
blank         | 
text          |  Boundedness of W ‚àó is established in Lemma B.1.
blank         | 
text          |  Definition B.4. Let V ‚äÜ RN¬µ be a bounded set and define
blank         | 
text          |                                            Z ‚àó = {Œª(¬µ) | ¬µ ‚àà V }.
blank         | 
text          |  Boundedness of Z ‚àó is established in Lemma B.2.
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                                218
blank         | 
              | 
              | 
text          | Lemma B.1. Assume (AR1)‚Äì(AR3), (AR6) hold and V ‚äÜ RN¬µ is a bounded subset. Then there
              | exists a constant Œ∫ > 0 such that
              |                                                   ‚àÇu
              |                                            sup       (¬µ) ‚â§ Œ∫                                          (B.8)
              |                                            ¬µ‚ààV    ‚àÇ¬µ
              |         ‚àÇu
              | where      (¬µ) is the solution of r ‚àÇ (u(¬µ), ¬∑ , ¬µ) = 0 and u(¬µ) is the solution of r( ¬∑ , ¬µ) = 0.
              |         ‚àÇ¬µ
              |                               ‚àÇu
              | Proof. From the definition of    (¬µ) in (2.87), boundedness of U ‚àó , and assumptions (AR2), (AR3),
              |                               ‚àÇ¬µ
              | (AR6), there exists a constant Œ∫ > 0 such that
blank         | 
text          |                            ‚àÇu       ‚àÇr                      ‚àÇr
              |                               (¬µ) ‚â§    (u(¬µ), ¬µ)‚àí1             (u(¬µ), ¬µ) ‚â§ Œ∫.
              |                            ‚àÇ¬µ       ‚àÇu                      ‚àÇ¬µ
blank         | 
              | 
              | 
text          | Lemma B.2. Assume (AR1)‚Äì(AR3), (AQ3) hold and V ‚äÜ RN¬µ is a bounded subset. Then there
              | exists a constant Œ∫ > 0 such that
              |                                              sup kŒª(¬µ)k ‚â§ Œ∫                                           (B.9)
              |                                             ¬µ‚ààV
blank         | 
text          | where Œª(¬µ) is the solution of r Œª (u(¬µ), ¬∑ , ¬µ) = 0 and u(¬µ) is the solution of r( ¬∑ , ¬µ) = 0.
blank         | 
text          | Proof. From the definition of Œª(¬µ) in (2.92), boundedness of U ‚àó , and assumptions (AR2), (AR3),
              | (AQ3), there exists a constant Œ∫ > 0 such that
blank         | 
text          |                                      ‚àÇr                  ‚àÇf
              |                            Œª(¬µ) ‚â§       (u(¬µ), ¬µ)‚àíT         (u(¬µ), ¬µ)T ‚â§ Œ∫.
              |                                      ‚àÇu                  ‚àÇu
blank         | 
              | 
              | 
text          | Lemma B.3. Assume (AR1), (AR4), (AR5) hold and U ‚äÜ RNu , V ‚äÜ RN¬µ are bounded subsets.
              | Then there exists a constant Œ∫ > 0 such that
blank         | 
text          |                                  ku(¬µ) ‚àí uk ‚â§ Œ∫ kr(u, ¬µ)k         ¬µ‚ààV                                (B.10)
blank         | 
text          | for any u ‚àà U , where u(¬µ) is the solution of r( ¬∑ , ¬µ) = 0.
blank         | 
text          | Proof. Consider any u ‚àà U . A variant of the mean value theorem gives
blank         | 
text          |                           r(u(¬µ), ¬µ) ‚àí r(u, ¬µ) = D(u(¬µ), u, ¬µ)(u(¬µ) ‚àí u).
blank         | 
text          | The boundedness of U ‚àó and assumptions (AR4)‚Äì(AR5) imply the existence of a constant Œ∫ > 0 such
              | that
              |                                       ku(¬µ) ‚àí uk ‚â§ Œ∫ kr(u, ¬µ)k .
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                               219
blank         | 
              | 
              | 
text          | Lemma B.4. Assume (AR1), (AR4), (AR5), (AQ2) hold and U ‚äÜ RNu , V ‚äÜ RN¬µ are bounded
              | subsets. Then there exists a constant Œ∫ > 0 such that
blank         | 
text          |                              |f (u(¬µ), ¬µ) ‚àí f (u, ¬µ)| ‚â§ Œ∫ kr(u, ¬µ)k      ¬µ‚ààV                       (B.11)
blank         | 
text          | for any u ‚àà U , where u(¬µ) is the solution of r( ¬∑ , ¬µ) = 0.
blank         | 
text          | Proof. Consider any u ‚àà U . Lipschitz continuity of f (AQ2) gives
blank         | 
text          |                                 |f (u(¬µ), ¬µ) ‚àí f (u, ¬µ)| ‚â§ cf ku(¬µ) ‚àí uk .                         (B.12)
blank         | 
text          | The bound in Lemma B.3 leads to the desired result.
blank         | 
text          | Lemma B.5. Assume (AR1)‚Äì(AR8) hold and U ‚äÜ RNu , V ‚äÜ RN¬µ , W ‚äÜ RNu √óN¬µ are bounded
              | subsets. Then there exists constants Œ∫, œÑ > 0 such that
blank         | 
text          |                        ‚àÇu
              |                           (¬µ) ‚àí w ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r ‚àÇ (u, w, ¬µ)             ¬µ‚ààV                  (B.13)
              |                        ‚àÇ¬µ
blank         | 
text          |                                          ‚àÇu
              | for any u ‚àà U and w ‚àà W , where             (¬µ) is the solution of r ‚àÇ (u(¬µ), ¬∑ , ¬µ) = 0 and u(¬µ) is the
              |                                          ‚àÇ¬µ
              | solution of r( ¬∑ , ¬µ) = 0.
              |                                   ‚àÇu
              | Proof. From the definition of        (¬µ) in (2.87) and r ‚àÇ in (2.89), the following relation holds for any
              |                                   ‚àÇ¬µ
              | w ‚àà RNu √óN¬µ
blank         |                                                                                
text          |                   ‚àÇu             ‚àÇr          ‚àí1 ‚àÇr                ‚àÇr
              |                      (¬µ) ‚àí w = ‚àí    (u(¬µ), ¬µ)         (u(¬µ), ¬µ) +    (u(¬µ), ¬µ)w
              |                   ‚àÇ¬µ             ‚àÇu                ‚àÇ¬µ             ‚àÇu
              |                                                                                                    (B.14)
              |                                  ‚àÇr
              |                              =‚àí     (u(¬µ), ¬µ)‚àí1 r ‚àÇ (u(¬µ), w, ¬µ).
              |                                  ‚àÇu
blank         | 
text          | Existence and boundedness of the Jacobian inverse over U ‚àó leads to the bound
blank         | 
text          |                                     ‚àÇu
              |                                        (¬µ) ‚àí w ‚â§ Œ∫1 r ‚àÇ (u(¬µ), w, ¬µ)                               (B.15)
              |                                     ‚àÇ¬µ
blank         | 
text          | for any w ‚àà RNu √óN¬µ , where Œ∫1 > 0 is a constant. The desired bound will be obtained by bounding
              | the sensitivity residual evaluated at the exact primal solution, r ‚àÇ (u(¬µ), w, ¬µ), by a combination of
              | the primal and sensitivity residuals at an approximate primal and sensitivity solution. For w ‚àà W
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                              220
blank         | 
              | 
              | 
text          | there exists a constant Œ∫2 > 0 such that
blank         | 
text          |  r ‚àÇ (u(¬µ), w, ¬µ) ‚â§ r ‚àÇ (u, w, ¬µ) + r ‚àÇ (u(¬µ), w, ¬µ) ‚àí r ‚àÇ (u, w, ¬µ)
              |                                            ‚àÇr             ‚àÇr              ‚àÇr             ‚àÇr
              |                     ‚â§ r ‚àÇ (u, w, ¬µ) +         (u(¬µ), ¬µ) ‚àí    (u, ¬µ) kwk +    (u(¬µ), ¬µ) ‚àí    (u, ¬µ)
              |                                            ‚àÇu             ‚àÇu              ‚àÇ¬µ             ‚àÇ¬µ
              |                     ‚â§ r ‚àÇ (u, w, ¬µ) + (c‚àÇ¬µ r + c‚àÇu r kwk) ku(¬µ) ‚àí uk
              |                     ‚â§ r ‚àÇ (u, w, ¬µ) + Œ∫2 kr(u, ¬µ)k .
              |                                                                                                   (B.16)
              | The first two inequalities follow from the triangle inequality and definition of the sensitivity residual
              | r ‚àÇ in (2.89). The third inequality follows from Lipschitz continuity of the partial derivatives of r
              | (AR7)‚Äì(AR8). The final inequality follows from the boundedness of the subset W and Lemma B.3.
              | Combining (B.15) and (B.16), the desired result follows.
blank         | 
text          | Lemma B.6. Assume (AR1)‚Äì(AR7), (AQ1), (AQ3) hold and U ‚äÜ RNu , V ‚äÜ RN¬µ , Z ‚äÜ RNu .
              | Then there exists constants Œ∫, œÑ > 0 such that
blank         | 
text          |                       kŒª(¬µ) ‚àí zk ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r Œª (u, z, ¬µ)           ¬µ‚ààV                    (B.17)
blank         | 
text          | for any z ‚àà Z, where Œª(¬µ) is the solution of r Œª (u(¬µ), ¬∑ , ¬µ) = 0 and u(¬µ) is the solution of
              | r( ¬∑ , ¬µ) = 0.
blank         | 
text          | Proof. From the definition of Œª(¬µ) in (2.92) and r Œª in (2.101), the following relation holds for any
              | z ‚àà RNu                                                                      
              |                             ‚àÇr          ‚àíT      ‚àÇf           T ‚àÇr          T
              |                  Œª(¬µ) ‚àí z =    (u(¬µ), ¬µ)     ‚àí     (u(¬µ), ¬µ) +    (u(¬µ), ¬µ) z
              |                             ‚àÇu                  ‚àÇu             ‚àÇu
              |                                                                                                   (B.18)
              |                             ‚àÇr
              |                           =    (u(¬µ), ¬µ)‚àíT r Œª (u(¬µ), z, ¬µ).
              |                             ‚àÇu
              | Existence and boundedness of the Jacobian inverse over U ‚àó leads to the bound
blank         | 
text          |                                   kŒª(¬µ) ‚àí zk ‚â§ Œ∫1 r Œª (u(¬µ), z, ¬µ)                                (B.19)
blank         | 
text          | for any z ‚àà RNu , where Œ∫1 > 0 is a constant. The desired bound will be obtained by bounding
              | the adjoint residual evaluated at the exact primal solution, r Œª (u(¬µ), z, ¬µ), by a combination of the
              | primal and adjoint residuals at an approximate primal and adjoint solution. Then for z ‚àà Z, there
              | exists a constant Œ∫2 > 0 such that
blank         | 
text          |  r Œª (u(¬µ), z, ¬µ) ‚â§ r Œª (u, z, ¬µ) + r Œª (u(¬µ), z, ¬µ) ‚àí r Œª (u, z, ¬µ)
              |                                           ‚àÇr              ‚àÇr               ‚àÇf             ‚àÇf
              |                    ‚â§ r Œª (u, z, ¬µ) +         (u(¬µ), ¬µ)T ‚àí    (u, ¬µ)T kzk +    (u(¬µ), ¬µ) ‚àí    (u, ¬µ)
              |                                           ‚àÇu              ‚àÇu               ‚àÇu             ‚àÇu
              |                    ‚â§ r Œª (u, z, ¬µ) + (c‚àÇu f + c‚àÇu r kzk) ku(¬µ) ‚àí uk
              |                    ‚â§ r Œª (u, z, ¬µ) + Œ∫2 kr(u, ¬µ)k .
              |                                                                                                   (B.20)
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                               221
blank         | 
              | 
              | 
text          | The first two inequalities follow from the triangle inequality and definition of the adjoint residual r Œª
              | in (2.101). The third inequality follows from Lipschitz continuity of the Jacobian of r (AR7) and
              | f (AQ3). The final inequality follows from the boundedness of the set Z. Combining (B.18) and
              | (B.20), the desired result follows.
blank         | 
text          | Lemma B.7. Assume (AR1)‚Äì(AR8), (AQ1)‚Äì(AQ4) hold and U ‚äÜ RNu , V ‚äÜ RN¬µ , W ‚äÜ RNu √óN¬µ
              | are bounded subsets. Then there exists constant Œ∫, œÑ > 0 such that
blank         |                        
text          |                ‚àÇu
              |      g ‚àÇ u(¬µ),    (¬µ), ¬µ ‚àí g ‚àÇ (u, w, ¬µ) ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r ‚àÇ (u, w, ¬µ)                 ¬µ‚ààV       (B.21)
              |                ‚àÇ¬µ
blank         | 
text          |                                                                                  ‚àÇu
              | for any u ‚àà U , w ‚àà W , where u(¬µ) is the solution of r( ¬∑ , ¬µ) = 0 and             (¬µ) is the solution of
              |                                                                                  ‚àÇ¬µ
              | r ‚àÇ (u(¬µ), ¬∑ , ¬µ) = 0.
blank         | 
text          | Proof. From the definition of g ‚àÇ in (2.90) and the triangle inequality
blank         |                                   
text          |             ‚àÇ             ‚àÇu                          ‚àÇf              ‚àÇf
              |         g           u(¬µ),    (¬µ), ¬µ ‚àí g ‚àÇ (u, w, ¬µ) ‚â§    (u(¬µ), ¬µ) ‚àí     (u, ¬µ) +
              |                           ‚àÇ¬µ                          ‚àÇ¬µ             ‚àÇ¬µ
              |                                                                                                    (B.22)
              |                                                       ‚àÇf           ‚àÇu        ‚àÇf
              |                                                          (u(¬µ), ¬µ)    (¬µ) ‚àí     (u, ¬µ)w .
              |                                                       ‚àÇu           ‚àÇ¬µ        ‚àÇu
blank         | 
text          |                                                            ‚àÇf
              | for any u ‚àà U and w ‚àà W . Lipschitz continuity of             ( ¬∑, ¬µ) leads to
              |                                                            ‚àÇ¬µ
blank         |                        
text          |                ‚àÇu
              |      g ‚àÇ u(¬µ),    (¬µ), ¬µ ‚àí g ‚àÇ (u, w, ¬µ) ‚â§ c‚àÇ¬µ f ku(¬µ) ‚àí uk +
              |                ‚àÇ¬µ
              |                                                                                                    (B.23)
              |                                                   ‚àÇf           ‚àÇu       ‚àÇf
              |                                                      (u(¬µ), ¬µ)    (¬µ) ‚àí    (u, ¬µ)w .
              |                                                   ‚àÇu           ‚àÇ¬µ       ‚àÇu
blank         | 
text          |                                ‚àÇf        ‚àÇu
              | Adding and subtracting            (u, ¬µ)    (¬µ) leads to
              |                                ‚àÇu        ‚àÇ¬µ
blank         |                                  
text          |            ‚àÇ             ‚àÇu
              |        g           u(¬µ),    (¬µ), ¬µ ‚àí g ‚àÇ (u, w, ¬µ) ‚â§ c‚àÇ¬µ f ku(¬µ) ‚àí uk +
              |                          ‚àÇ¬µ
blank         |                                                                                      
text          |                                                               ‚àÇf             ‚àÇf         ‚àÇu
              |                                                                  (u(¬µ), ¬µ) ‚àí    (u, ¬µ)     (¬µ) +
              |                                                               ‚àÇu             ‚àÇu         ‚àÇ¬µ
blank         |                                                                                  
text          |                                                             ‚àÇf          ‚àÇu
              |                                                                (u, ¬µ)      (¬µ) ‚àí w .
              |                                                             ‚àÇu          ‚àÇ¬µ
blank         | 
text          |                                   ‚àÇf
              |                                      ( ¬∑, ¬µ) gives
              | Lipschitz continuity and boundedness of
              |                                   ‚àÇu
blank         |                                                       
text          |   ‚àÇ      ‚àÇu           ‚àÇ                            ‚àÇu                     ‚àÇu
              |  g u(¬µ),    (¬µ), ¬µ ‚àí g (u, w, ¬µ) ‚â§ c‚àÇ¬µ f + c‚àÇu f      (¬µ) ku(¬µ) ‚àí uk + œÑ1    (¬µ) ‚àí v ,
              |          ‚àÇ¬µ                                        ‚àÇ¬µ                     ‚àÇ¬µ
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                                 222
blank         | 
              | 
              | 
text          | where œÑ1 > 0 is a constant. The boundedness of W ‚àó leads to
blank         |                                           
text          |                     ‚àÇ             ‚àÇu                                             ‚àÇu
              |                 g           u(¬µ),    (¬µ), ¬µ ‚àí g ‚àÇ (u, w, ¬µ) ‚â§ Œ∫1 ku(¬µ) ‚àí uk + œÑ1    (¬µ) ‚àí w
              |                                   ‚àÇ¬µ                                             ‚àÇ¬µ
blank         | 
text          | where Œ∫1 > 0 is a constant. Combining the above bound with Lemmas B.3 and B.5 gives the desired
              | result.
blank         | 
text          | Lemma B.8. Assume (AR1)‚Äì(AR8), (AQ1)‚Äì(AQ4) hold and U ‚äÜ RNu , V ‚äÜ RN¬µ , Z ‚äÜ RNu are
              | bounded subsets. Then there exists constant Œ∫, œÑ > 0 such that
blank         | 
text          |           g Œª (u(¬µ), Œª(¬µ), ¬µ) ‚àí g Œª (u, z, ¬µ) ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r Œª (u, z, ¬µ)               ¬µ‚ààV       (B.24)
blank         | 
text          | for any u ‚àà U , z ‚àà Z, where u(¬µ) is the solution of r( ¬∑ , ¬µ) = 0 and Œª(¬µ) is the solution of
              | r Œª (u(¬µ), ¬∑ , ¬µ) = 0.
blank         | 
text          | Proof. From the definition of g Œª in (2.102) and the triangle inequality
blank         | 
text          |                                                          ‚àÇf             ‚àÇf
              |             g Œª (u(¬µ), Œª(¬µ), ¬µ) ‚àí g Œª (u, z, ¬µ) ‚â§           (u(¬µ), ¬µ) ‚àí    (u, ¬µ) +
              |                                                          ‚àÇ¬µ             ‚àÇ¬µ
              |                                                                                                       (B.25)
              |                                                                ‚àÇr                 ‚àÇr
              |                                                          Œª(¬µ)T    (u(¬µ), ¬µ) ‚àí z T    (u, ¬µ) .
              |                                                                ‚àÇ¬µ                 ‚àÇ¬µ
blank         | 
text          |                                                              ‚àÇf
              | for any u ‚àà U and z ‚àà Z. Lipschitz continuity of                ( ¬∑, ¬µ) over U leads to
              |                                                              ‚àÇ¬µ
blank         | 
text          |           g Œª (u(¬µ), Œª(¬µ), ¬µ) ‚àí g Œª (u, z, ¬µ) ‚â§ c‚àÇ¬µ f ku(¬µ) ‚àí uk +
              |                                                                      ‚àÇr                 ‚àÇr            (B.26)
              |                                                              Œª(¬µ)T      (u(¬µ), ¬µ) ‚àí z T    (u, ¬µ) .
              |                                                                      ‚àÇ¬µ                 ‚àÇ¬µ
blank         | 
text          |                                         ‚àÇr
              | Adding and subtracting Œª(¬µ)T               (u, ¬µ) leads to
              |                                         ‚àÇ¬µ
blank         | 
text          |             g ‚àÇ (u(¬µ), Œª(¬µ), ¬µ) ‚àí g ‚àÇ (u, z, ¬µ) ‚â§ c‚àÇ¬µ f ku(¬µ) ‚àí uk +
blank         |                                                                                          
text          |                                                              T   ‚àÇr              ‚àÇr
              |                                                          Œª(¬µ)       (u(¬µ), ¬µ) ‚àí     (u, ¬µ) +
              |                                                                  ‚àÇ¬µ             ‚àÇ¬µ
              |                                                                    T ‚àÇr
              |                                                          (Œª(¬µ) ‚àí z)     (u, ¬µ) .
              |                                                                      ‚àÇ¬µ
blank         | 
text          |                                                   ‚àÇr
              | Lipschitz continuity and boundedness of              ( ¬∑, ¬µ) gives
              |                                                   ‚àÇ¬µ
blank         | 
text          |           g ‚àÇ (u(¬µ), Œª(¬µ), ¬µ) ‚àí g ‚àÇ (u, z, ¬µ) ‚â§ c‚àÇ¬µ f + c‚àÇ¬µ r kŒª(¬µ)k ku(¬µ) ‚àí uk + œÑ1 kŒª(¬µ) ‚àí zk ,
blank         |                                                                     
meta          | APPENDIX B. RESIDUAL-BASED ERROR BOUNDS                                                    223
blank         | 
              | 
              | 
text          | where œÑ1 > 0 is a constant. The boundedness of Z ‚àó leads to
blank         | 
text          |                 g ‚àÇ (u(¬µ), Œª(¬µ), ¬µ) ‚àí g ‚àÇ (u, z, ¬µ) ‚â§ Œ∫1 ku(¬µ) ‚àí uk + œÑ1 kŒª(¬µ) ‚àí zk
blank         | 
text          | where Œ∫1 > 0 is a constant. Combining the above bound with Lemmas B.3 and B.6 gives the desired
              | result.
meta          | Appendix C
blank         | 
title         | Adaptive State and Parameter
              | Space Reduction for Large-Scale
              | Optimization
blank         | 
text          | All of the optimization methods introduced in Chapters 5‚Äì6 were developed under the assump-
              | tion that the number of optimization parameters is small compared to the size of the state vector
              | (N¬µ  Nu ) and therefore the dominant cost is attributed to the PDE solves. However, there are
              | a large a number of relevant optimization problems, including topological optimization and inverse
              | problems, where this is not the case. In these problems, the number of parameters is of the same
              | order of magnitude as the number of degrees of freedom in the PDE, i.e., N¬µ = O(Nu ). In such
              | settings, the cost of the optimization problem cannot be notably decreased if the state vector alone
              | is reduced, e.g., with projection-based reduced-order models. This can be attributed to two main
              | sources of computational cost. The first comes from the fact that the linear algebra involved in
              | the optimization solver is non-negligible due to the large number of parameters. Therefore, even
              | the reduced-space approach to PDE-constrained optimization (Section 2.3.2) will yield a large-scale
              | optimization problem. Second, the evaluation of reduced-order model residual and Jacobian depend
              | on O(Nu ) parameters and will require at least O(Nu ) operations and can not be expected to enjoy
              | the dramatic reduction in computational resources that has been exploited in non-parametric or
              | few-parameter settings [17, 171, 114, 125, 52].
              |    The approach taken to eliminate the bottlenecks associated with large parameter spaces adap-
              | tively restricts the parameter space to a low-dimensional affine subspace of dimension k¬µ , where
              | k¬µ  N¬µ . While similar approaches have been taken in the past [168, 167, 117, 120], the proposed
              | method focuses on establishing global convergence (not considered in [117, 120]) without requiring
              | first-order consistency, a requirement in [168, 167], for increased efficiency. The proposed restriction
              | converts the N¬µ -parameter optimization problem to one in k¬µ parameters. The resulting opti-
              | mization problem with few parameters is solved using the globally convergent multifidelity trust
blank         | 
              | 
meta          |                                                   224
              | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                 225
blank         | 
              | 
              | 
text          | region method that leverages projection-based reduced-order models (Chapter 5). This results in
              | a two-level, nested reduction where, at the outermost level, the parameter space is restricted to a
              | low-dimensional affine subspace to yield an optimization problem in few variables and, at the in-
              | ner level, the projection-based model reduction reduces the dimensionality of the PDE itself. The
              | inexactness introduced at the innermost level through the use of projection-based reduced-order
              | models is managed using the multifidelity trust region of Chapter 5. Once the solution of the re-
              | stricted optimization problem is found, the low-dimensional parameter subspace is adapted at the
              | new point in ¬µ-space. Such an approach to numerical optimization is called a subspace method
              | [54, 119, 137, 143, 207]; the popular linesearch methods [143] correspond to the special case with
              | k¬µ = 1. Convergence theory from the subspace/linesearch optimization literature will be recycled
              | to formulate a minimum requirement on the updated low-dimensional affine parameter subspace to
              | ensure a globally convergent method. The proposed subspace update will satisfy this minimum re-
              | quirement, thereby ensuring global convergence, while providing sufficient flexibility to incorporate
              | generic optimization-based vectors (such as the steepest descent direction, quasi-Newton directions,
              | and directions of negative curvature) as well as problem specific information. In applications such
              | as topology optimization and inverse problems, the parameter vector has a strong connection to the
              | geometry of the underlying PDE and its discretization, which can be exploited to yield a rapidly
              | converging algorithm.
              |    General subspace methods (k¬µ > 1) have not been widely adopted by the optimization com-
              | munity because of the inherent difficulty/expense required to search a k¬µ -dimensional subspace
              | compared to a one-dimensional subspace as in linesearch methods. This is one reason linesearch
              | methods have enjoyed considerable success. In contrast, trust region methods search the entire
              | N¬µ -dimensional space at each optimization iteration; however, the expensive objective function is
              | usually replaced with a quadratic approximation that is inexpensive to query. The use of the more
              | expensive subspace methods are justified in this work for two reasons. First, an efficient method has
              | been developed in Chapter 5 to solve PDE-constrained optimization problems with few parameters
              | and it is desirable to use this method to do as much work as possible before adapting the param-
              | eter space. Additionally, restricting the parameter space to few parameters will ensure evaluation
              | of the reduced-order model does not involve operations that scale with N¬µ = O(Nu ). To develop
              | the ideas of this section in a simple setting, only the deterministic case will be considered; future
              | work will consider stochastic optimization problems with large-dimensional parameter spaces and a
              | strategy that combines this approach with the method of Chapter 6, i.e., three-level approximation:
              | reduction of the state space via model reduction, reduction of the parameter space via subspace and
              | linesearch techniques, and approximation of integrals using dimension-adaptive sparse grids.
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                   226
blank         | 
              | 
              | 
title         | C.1      Two-Level Nested Reduction of Parametrized Partial
              |          Differential Equations
text          | This section proposes a two-level, nested reduction strategy for parametrized partial differential
              | equations. In the first level of reduction, the high-dimensional parameter space is restricted to
              | a low-dimensional affine subspace. In the context of optimization, this amounts to a restriction
              | of the search space to the chosen affine subspace; however, it does not introduce any error into
              | the pointwise evaluation of the PDE. The second level of reduction uses projection-based model
              | reduction (Chapter 4) to the reduce the number of degrees of freedom in the PDE, i.e., reduction
              | of the state space. Unlike the reduction of the parameter space, the state space restriction does
              | introduce error into the evaluation of the PDE, as seen previously in Chapter 4. These two types
              | of reduction will be nested to efficiently solve a PDE-constrained optimization problem as follows:
              | first, the parameter space restriction will be applied to reduce the optimization problem over N¬µ
              | parameters to one over k¬µ  N¬µ parameters and the trust region method of Chapter 5 that leverages
              | projection-based reduced-order models will be applied to solve the reduced optimization problem.
              | Adaptation of the parameter space, discussed in Section C.2.1, will be required to yield a globally
              | convergent method. The remainder of this section will consider each layer of reduction, in isolation,
              | which will be combined in Section C.2 to develop the nested optimization algorithm.
blank         | 
              | 
title         | C.1.1     Outer Layer of Reduction: Restriction of Parameter Space
text          | The PDE-constrained optimization problem that motivates this work takes the form (reduced-space
              | formulation)
              |                                    minimize F (¬µ) := f (u(¬µ), ¬µ)                                  (C.1)
              |                                     ¬µ‚ààRN¬µ
blank         | 
text          | where u(¬µ) is the unique (Assumption 2.2), continuously differentiable (Theorem 2.1) solution of
              | the fully discrete parametrized partial differential equation r( ¬∑ , ¬µ) = 0. Unlike previous chapters,
              | here it is assumed that N¬µ is large, i.e., N¬µ = O(Nu ). The gradient of the objective function can be
              | computed using either the sensitivity (Section 2.3.3) or adjoint (Section 2.3.4) method; however, due
              | to the large number of parameters N¬µ = O(Nu ), the adjoint method is the only feasible approach.
              |    The reduction of the parameter space proceeds in an identical manner to the state reduction in
              | Chapter 4, i.e., with the ansatz that the parameter lies in a low-dimensional (affine) subspace
blank         | 
text          |                                             ¬µ = ¬µÃÑ + Œ•Œ∑                                           (C.2)
blank         | 
text          | where ¬µÃÑ ‚àà RN¬µ is the affine offset, Œ• ‚àà RN¬µ √ók¬µ a basis for the chosen low-dimensional subspace,
              | Œ∑ ‚àà Rk¬µ are the reduced coordinates of ¬µ in the affine subspace A(¬µÃÑ, Œ•) := {¬µÃÑ+Œ•Œ∑ | Œ∑ ‚àà Rk¬µ }, and
              | k¬µ  N¬µ . For the remainder of this section, ¬µÃÑ and Œ• will be assumed given and fixed; Section C.2
              | will provide details pertaining to their construction and adaptation. Substitution of the ansatz in
              | (C.2) into the parametrized PDE r(u, ¬µ) = 0 with N¬µ parameters leads to a parametrized PDE in
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                     227
blank         | 
              | 
              | 
              | 
text          |                                     span(Œ•)
blank         | 
text          |                                          ¬µÃÑ
              |                                                                    ¬µ‚àó
blank         | 
              | 
text          |                                                   ¬µÃÑ + Œ•Œ∑ ‚àó
blank         | 
              | 
              | 
              | 
text          | Figure C.1: Schematic of restriction of parameter space RN¬µ to affine subspace A(¬µÃÑ, Œ•) of dimension
              | k¬µ , in the special case where N¬µ = 2 and k¬µ = 1. The optimal solution ¬µ‚àó in the parameter space,
              | as well as the optimal solution over A(¬µÃÑ, Œ•) are also depicted.
blank         | 
              | 
text          | k¬µ parameters
              |                                           r(u, ¬µÃÑ + Œ•Œ∑) = 0.                                       (C.3)
blank         | 
text          | In the above setting, the affine offset ¬µÃÑ and basis Œ• are fixed and the PDE parameter is varied
              | through variations in the reduced coordinates Œ∑. For the remainder of this document, let u(Œ∑; ¬µÃÑ, Œ•)
              | be the solution of the restricted PDE in (C.3), i.e., r( ¬∑ , ¬µÃÑ + Œ•Œ∑) = 0. Uniqueness and continuous
              | differentiability of u(Œ∑; ¬µÃÑ, Œ•) follow immediately from the corresponding properties of u(¬µ) and
              | the affine relationship between ¬µ and Œ∑. Following the discussion at the beginning of this section,
              | the approximation in (C.3) does not introduce error into the evaluation of the PDE since it is clear
              | that u(Œ∑; ¬µÃÑ, Œ•) = u(¬µÃÑ + Œ•Œ∑). Rather, it limits the possible variations of the parameter that can
              | be realized, i.e., any ¬µ ‚àà RN¬µ such that ¬µ 6‚àà A(¬µÃÑ, Œ•) cannot be considered by the restricted PDE
              | in (C.3). With the ansatz in (C.2), the PDE-constrained optimization problem in N¬µ parameters
              | reduces to one in k¬µ parameters
blank         | 
text          |                           minimize F (Œ∑; ¬µÃÑ, Œ•) := f (u(Œ∑; ¬µÃÑ, Œ•), ¬µÃÑ + Œ•Œ∑)                        (C.4)
              |                             Œ∑‚ààRk¬µ
blank         | 
              | 
text          | that amounts to a search for the optimal solution in the affine subspace A(¬µÃÑ, Œ•), i.e., (C.4) is
              | equivalent to
              |                                               minimize F (¬µ).                                      (C.5)
              |                                               ¬µ‚ààA(¬µÃÑ, Œ•)
blank         | 
text          | This situation is illustrated in Figure C.1 for the case of k¬µ = 1. In general, a local minima of
              | (C.5), call it ¬µ‚àó , will not be lie in A(¬µÃÑ, Œ•) for an a priori selection of ¬µÃÑ and Œ•. This motivates the
              | adaptation strategy for ¬µÃÑ and Œ• that will be introduced in Section C.2.1.
blank         | 
text          | Remark. As previously discussed, the idea of restricting the optimization problem to a low-dimensional
              | affine subspace generalizes linesearch methods that consider a one-dimensional affine search space.
              | Such methods are known as subspace methods. In linesearch methods, the subspace is defined by any
              | descent direction pk (possibly the steepest descent or a quasi-Newton direction) and offset to include
              | the current optimization iterate, ¬µk , i.e., the search space is {¬µk + Œ±pk | Œ± > 0}. In the notation of
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                     228
blank         | 
              | 
              | 
text          | this section, linesearch methods amount to the selection ¬µÃÑ = ¬µk and Œ• = [pk ].
blank         | 
text          |    Since the number of parameters has been dramatically reduced, either the sensitivity or adjoint
              | method are feasible approaches to compute the gradient of F . Following the procedure outlined in
              | Section 2.3.3, the expression for ‚àáF (Œ∑; ¬µÃÑ, Œ•) based on the sensitivity method is
blank         |                                                                                     
text          |                                          ‚àÇ                    ‚àÇu
              |                      ‚àáF (Œ∑; ¬µÃÑ, Œ•) = g           u(Œ∑; ¬µÃÑ, Œ•),    (Œ∑; ¬µÃÑ, Œ•), Œ∑; ¬µÃÑ, Œ• ,            (C.6)
              |                                                               ‚àÇŒ∑
blank         | 
text          | where the definition of g ‚àÇ varies slightly from that in (2.90)
blank         | 
text          |                                                  ‚àÇf                 ‚àÇf
              |                   g ‚àÇ (u, wr , Œ∑; ¬µÃÑ, Œ•) :=         (u, ¬µÃÑ + Œ•Œ∑)Œ• +    (u, ¬µÃÑ + Œ•Œ∑)wr .            (C.7)
              |                                                  ‚àÇ¬µ                 ‚àÇu
blank         | 
text          |                                                      ‚àÇu   ‚àÇu
              | The sensitivity of u with respect to Œ∑, i.e.,           =    (Œ∑; ¬µÃÑ, Œ•) is defined as the solution of the
              |                                                      ‚àÇŒ∑   ‚àÇŒ∑
              | sensitivity equations
              |                                     r ‚àÇ (u(Œ∑; ¬µÃÑ, Œ•), ¬∑ , Œ∑; ¬µÃÑ, Œ•) = 0,                           (C.8)
blank         | 
text          | where the sensitivity residual is defined as
blank         | 
text          |                                               ‚àÇr                 ‚àÇr
              |                   r ‚àÇ (u, wr , Œ∑; ¬µÃÑ, Œ•) :=      (u, ¬µÃÑ + Œ•Œ∑)Œ• +    (u, ¬µÃÑ + Œ•Œ∑)wr .               (C.9)
              |                                               ‚àÇ¬µ                 ‚àÇu
blank         | 
text          | Thus, the sensitivity computation requires the solution of k¬µ linear systems of equations defined by
              |                                                    ‚àÇr
              | the Jacobian matrix with the kth right-hand side      Œ•ek . For comparison, the sensitivity approach
              |                                                    ‚àÇ¬µ
              | to compute ‚àáF (¬µ) would require the solution of N¬µ linear systems defined by the Jacobian matrix
              |                      ‚àÇr
              | and right-hand side     ek . From (C.2), the following relationship between the sensitivity of u with
              |                      ‚àÇ¬µ
              | respect to ¬µ and Œ∑ holds
              |                                   ‚àÇu               ‚àÇu
              |                                      (Œ∑; ¬µÃÑ, Œ•) =     (¬µÃÑ + Œ•ur )Œ•                             (C.10)
              |                                   ‚àÇŒ∑               ‚àÇ¬µ
              |    Even though ku is much smaller than Nu , it may still be sufficiently large to prefer gradient
              | computations via the adjoint method. Following any of the three procedures outlined in Section 2.3.4,
              | the expression for ‚àáF (Œ∑; ¬µÃÑ, Œ•) based on the adjoint method is
blank         | 
text          |                         ‚àáF (Œ∑; ¬µÃÑ, Œ•) = g Œª (u(Œ∑; ¬µÃÑ, Œ•), Œª(Œ∑; ¬µÃÑ, Œ•), Œ∑; ¬µÃÑ, Œ•) ,                (C.11)
blank         | 
text          | where the definition of g Œª varies slightly from that in (2.102)
blank         | 
text          |                                              ‚àÇf                     ‚àÇr
              |                   g Œª (u, z, Œ∑; ¬µÃÑ, Œ•) :=       (u, ¬µÃÑ + Œ•Œ∑)Œ• + z T    (u, ¬µÃÑ + Œ•Œ∑)Œ•.             (C.12)
              |                                              ‚àÇ¬µ                     ‚àÇ¬µ
blank         | 
text          | The adjoint state, Œª = Œª(Œ∑; ¬µÃÑ, Œ•) is defined as the solution of the adjoint equations
blank         | 
text          |                                    r Œª (u(Œ∑; ¬µÃÑ, Œ•), ¬∑ , Œ∑; ¬µÃÑ, Œ•) = 0,                           (C.13)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                    229
blank         | 
              | 
              | 
text          | where the adjoint residual is defined as
blank         | 
text          |                                               ‚àÇf                ‚àÇr
              |                     r Œª (u, z, Œ∑; ¬µÃÑ, Œ•) :=      (u, ¬µÃÑ + Œ•Œ∑) +    (u, ¬µÃÑ + Œ•Œ∑)T z.              (C.14)
              |                                               ‚àÇu                ‚àÇu
blank         | 
text          | Thus, the adjoint computation requires the solution of one linear system of equations defined by the
              | transpose of the Jacobian matrix, regardless of k¬µ .
blank         | 
              | 
text          | C.1.2     Inner Layer of Reduction: Projection-Based Model Reduction
              | While the first layer of reduction reduces the number of optimization variables, the large cost as-
              | sociated with solving the PDE for any ¬µ ‚àà A(¬µÃÑ, Œ•) remains since the dimensionality of the state
              | space, i.e., number of equations and unknowns, is Nu  1. The second layer of reduction aims
              | to address this source computational expense through the application of projection-based model
              | reduction (Chapter 4).
              |    Let Œ¶ and Œ® be a given trial and test basis defining a minimum-residual projection-based reduced-
              | order model. Introduction of the model reduction ansatz u = Œ¶ur into the discretized PDE defined
              | over the parameter space A(¬µÃÑ, Œ•) and subsequent projection onto the columnspace of Œ® leads to
              | the projection-based reduced-order model
blank         | 
text          |                          rr (ur , Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®) := Œ®T r(Œ¶ur , ¬µÃÑ + Œ•Œ∑) = 0.                    (C.15)
blank         | 
text          | Denote the unique, continuously differentiable solution of the fully reduced model in (C.15) as
              | ur (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®). Substitution of the reconstructed primal reduced-order model solution into the
              | quantity of interest leads to its fully reduced form
blank         | 
text          |                       Fr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®) := f (Œ¶ur (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®), ¬µÃÑ + Œ•Œ∑).                  (C.16)
blank         | 
text          | The gradient of the reduced quantity of interest is computed via the sensitivity (Section 2.3.3) or
              | adjoint (Section 2.3.4) method as
blank         |                                                                                
text          |                                                   ‚àÇur
              |                   ‚àáFr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®) = g ‚àÇ u, Œ¶     (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®), ¬µÃÑ + Œ•Œ∑
              |                                                   ‚àÇŒ∑                                             (C.17)
              |                                         = g Œª (u, Œ®Œªr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®), ¬µÃÑ + Œ•Œ∑)
blank         | 
text          |                                                                        ‚àÇur
              | where u = Œ¶ur (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®) is the reconstructed primal solution,        (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®) is the
              |                                                                         ‚àÇŒ∑
              | solution of the reduced-order model sensitivity equations in (4.19), and Œªr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®) is the
              | solution of the reduced-order model adjoint equations in (4.45). While the adjoint equations are
              | identical to those in Section 4.1.3, the sensitivity equations require the following substitutions since
              | we seek sensitivities with respect to Œ∑ instead of ¬µ:
blank         | 
text          |                   ‚àÇf          ‚àÇf                            ‚àÇr          ‚àÇr
              |                      (u, ¬µ) ‚Üê    (u, ¬µ)Œ•            and        (u, ¬µ) ‚Üê    (u, ¬µ)Œ•               (C.18)
              |                   ‚àÇ¬µ          ‚àÇ¬µ                            ‚àÇ¬µ          ‚àÇ¬µ
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                  230
blank         | 
              | 
              | 
text          | for any u ‚àà RNu and ¬µ ‚àà RN¬µ . If the test basis is non-constant, following the developments of
              | Sections 4.1.2 and 4.1.3, the minimum-residual approximation of the gradient ‚àáF
              |                                                                              dr (¬µ) can be used
              | to avoid computations involving second derivatives of r
              |                                                                                     !
              |                                              ‚àÇ‚àÇu
              |                                               dr
              |          dr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®) = g ‚àÇ
              |          ‚àáF                              u, Œ¶    (Œ∑; ¬µÃÑ, Œ•, Œ¶‚àÇ , Œò‚àÇ , u), ¬µÃÑ + Œ•Œ∑
              |                                               ‚àÇŒ∑                                               (C.19)
blank         |                                                                                    
text          |                                                  Œª             Œª    Œª
              |                                 = g Œª u( ¬∑ ), Œ¶ ŒªÃÇr (Œ∑; ¬µÃÑ, Œ•, Œ¶ , Œò , u), ¬µÃÑ + Œ•Œ∑ ,
blank         | 
              | 
text          |                                                                       ‚àÇu
              |                                                                       dr
              | where u = Œ¶ur (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®) is the reconstructed primal solution,      (Œ∑; ¬µÃÑ, Œ•, Œ¶‚àÇ , Œò‚àÇ , u) is
              |                                                                        ‚àÇŒ∑
              | the solution of the minimum-residual sensitivity reduced-order model in (4.28), and
              | ŒªÃÇr (Œ∑; ¬µÃÑ, Œ•, Œ¶Œª , ŒòŒª , u) is the solution of the minimum-residual adjoint reduced-order model in
              | (4.56). For the minimum-residual sensitivity ROM in (4.28), the substitutions in (C.18) are required
              | to directly compute sensitivities with respect to Œ∑.
              |    This section closes by stating the residual-based error bounds for the fully reduced quantity of
              | interest, its gradient, and minimum-residual gradient approximation. The error bounds are given
              | with respect to the first level of reduction as we are only concerned with the error for a fixed
              | ¬µ ‚àà A(¬µÃÑ, Œ•). These will be used in the multifidelity trust region framework of Chapter 5 to
              | solve (C.4), i.e., the optimization problem after the first layer of reduction. From Lemma B.4, the
              | residual-based error bound on the quantity of interest takes the form
blank         | 
text          |          |F (Œ∑; ¬µÃÑ, Œ•) ‚àí Fr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®)| ‚â§ Œ∂ kr(Œ¶ur (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®), ¬µÃÑ + Œ•Œ∑)k           (C.20)
blank         | 
text          | for an arbitrary constant Œ∂ > 0. The residual-based error indicator for the gradient ‚àáFr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®)
              | computed with the sensitivity method is
blank         | 
text          |              k‚àáF (Œ∑; ¬µÃÑ, Œ•) ‚àí ‚àáFr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®)k ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r ‚àÇ (u, w, ¬µ)            (C.21)
blank         | 
text          | where u = Œ¶ur (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®Œ•) is the reconstructed primal solution,
              |        ‚àÇur
              | w=Œ¶        (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®), is the reconstructed sensitivity solution, and Œ∫, œÑ > 0 are arbitrary
              |        ‚àÇŒ∑
              | constants. The corresponding bound for gradients computed with the adjoint method is
blank         | 
text          |              k‚àáF (Œ∑; ¬µÃÑ, Œ•) ‚àí ‚àáFr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®)k ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r Œª (u, z, ¬µ)            (C.22)
blank         | 
text          | where u = Œ¶ur (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®Œ•) is the reconstructed primal solution, z = Œ®Œªr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®) is
              | the reconstructed adjoint solution, and Œ∫, œÑ > 0 are arbitrary constants. The residual-based error
              | bounds for the minimum-residual approximation of the gradient of Fr are
blank         | 
text          |       ‚àáF (Œ∑; ¬µÃÑ, Œ•) ‚àí ‚àáFr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®, Œ¶‚àÇ , Œò‚àÇ ) ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r ‚àÇ (u, w, ¬µ)
              |                                                                                                (C.23)
              |       ‚àáF (Œ∑; ¬µÃÑ, Œ•) ‚àí ‚àáFr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®, Œ¶Œª , ŒòŒª ) ‚â§ Œ∫ kr(u, ¬µ)k + œÑ r Œª (u, z, ¬µ)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                  231
blank         | 
              | 
              | 
text          | where u = Œ¶ur (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®Œ•) is the reconstructed primal solution,
              |        ‚àÇu
              |         dr
              | w = Œ¶‚àÇ      (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®, Œ¶‚àÇ , Œò‚àÇ ), is the reconstructed minimum-residual sensitivity solution,
              |         ‚àÇŒ∑
              | z = Œ¶Œª ŒªÃÇr (Œ∑; ¬µÃÑ, Œ•, Œ¶, Œ®, Œ¶Œª , ŒòŒª ) is the reconstructed minimum-residual adjoint solution, and
              | Œ∫, œÑ > 0 are arbitrary constants.
blank         | 
text          | Remark. The I-norm used in the error bounds (5.18) can be replaced with the minimum-residual
              | metrics Œò, Œò‚àÇ , ŒòŒª as done in Chapter 5 for greater consistency with the minimum-residual inter-
              | pretation of the reduced-order model. This will be necessary if partially converged solutions are used
              | as snapshots in construction of the reduced-order basis, as discussed in Section 5.2. This will be
              | deferred to future work and the simpler (and less expensive) I-norm will be used.
blank         | 
              | 
title         | C.2      Globally Convergent Multifidelity Trust Region Method
text          | The two-level nested reduction of parametrized partial differential equations with a high-dimensional
              | state and parameter space will serve as a pillar for an efficient method to solve optimization prob-
              | lems constrained by such PDEs. The first layer of reduction restricts the parameter space to the
              | k¬µ -dimensional affine subspace A(¬µÃÑ, Œ•) to yield an optimization problem in k¬µ variables. The
              | second layer of reduction uses projection-based reduced-order models, embedded in the globally
              | convergent multifidelity trust region framework of Chapter 3, i.e., the method developed in Chap-
              | ter 5, to efficiently solve the k¬µ -dimensional optimization problem. To ensure the method is globally
              | convergent, the restricted parameter space A(¬µÃÑ, Œ•) is adapted using ideas from linesearch methods.
              | The proposed optimization algorithm based on this nested reduction strategy consists of two types of
              | iterations: (1) an inner iteration where the affine subspace for the parameter, A(¬µÃÑ, Œ•) is fixed and
              | the multifidelity trust region method based on projection-based reduced-order models (Chapter 5)
              | is applied to solve the optimization problem in (C.4) and (2) an outer iteration that adapts the
              | parameter subspace A(¬µÃÑ, Œ•) to ensure global convergence of the complete algorithm. The inner
              | iteration is guaranteed to converge to the solution of (C.4) since the multifidelity method introduced
              | in Chapter 5 is globally convergent. The parameter subspace adaptation in the outer iteration will
              | be constructed such that global convergence to the solution of (C.1) is guaranteed. The next two
              | sections detail both the inner and outer iterations.
blank         | 
              | 
text          | C.2.1     Outer Iteration: Globally Convergent Parameter Space Adapta-
              |           tion
              | It is not reasonable to expect an a-prior selection of the restricted parameter subspace A(¬µÃÑ, Œ•) to
              | lead to a globally convergent algorithm since, in general, ¬µ‚àó ‚àà
              |                                                               / A(¬µÃÑ, Œ•) where ¬µ‚àó is a local minima
              | of F (¬µ). Therefore, keeping with the theme of this document, this section develops an adaptation
              | strategy for the affine offset ¬µÃÑ and subspace Œ• defining the restricted parameter space. That is, an
              |                                                                                       j
              | algorithm that constructs a sequence of affine subspaces {A(¬µÃÑj , Œ•j )} of dimension k¬µ  N¬µ such
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                          232
blank         | 
              | 
              | 
text          | that the iterates {¬µj }, computed as the solution of the restricted optimization problem
blank         | 
text          |                                         ¬µj+1 :=     arg min        F (¬µ),                            (C.24)
              |                                                   ¬µ‚ààA(¬µÃÑj , Œ•j )
blank         | 
              | 
text          | converge to a stationary point of F (¬µ) over RN¬µ , i.e., lim ‚àáF (¬µj ) = 0. From the discussion in
              | Section C.1, the definition in (C.24) is equivalent to
blank         | 
text          |                      ¬µj+1 = ¬µÃÑj + Œ•j Œ∑ j+1             Œ∑ j+1 = arg min F (Œ∑; ¬µÃÑj , Œ•j ),             (C.25)
              |                                                                         j
              |                                                                     Œ∑‚ààRk¬µ
blank         | 
text          |                          j
              | i.e., the search in the k¬µ -dimensional subspace embedded in N¬µ is equivalent to an optimization
              |             j
              | problem in k¬µ variables.
              |    Before launching into the details of the proposed adaptation strategy, recall two standard results
              | from optimization theory stated in Lemma C.1 and Theorem C.1. Theorem C.1 states that any
              | iteration of the form ¬µj+1 = ¬µj + Œ±j pj , where pj is a descent direction at ¬µj and Œ±j > 0 satisfies
              | the Wolfe conditions (C.28), constitutes a globally convergent optimization method and Lemma C.1
              | establishes the existence of a point satisfying the Wolfe conditions for any descent direction. These
              | results are combined to arrive at the following conclusion: if ¬µÃÑj = ¬µj and col(Œ•j ) contains a descent
              | direction of F at ¬µj , the sequence {¬µj } produced by (C.24) will satisfy limj‚Üí‚àû ‚àáF (¬µj )                = 0.
              | This claim is justified since ¬µj+1 is the exact solution of the optimization problem restricted to
              | A(¬µÃÑj , Œ•j ) (which contains ¬µj and a descent direction of F (¬µj )) and, since a point exists that satisfies
              | the sufficient decrease conditions (Lemma C.1), ¬µj+1 must also satisfy them and the iteration is
              | globally convergent (Theorem C.1). This argument is justified rigorously by showing ¬µj+1 satisfies
              | the strong Wolfe conditions since Theorem C.1 guarantees global convergence if these conditions
              | hold. The choice ¬µÃÑj = ¬µj implies the affine subspace A(¬µÃÑj , Œ•j ) contains points of the form
              |                                                                                                      j
              | ¬µ = ¬µj + Œ•j Œ∑. Since col(Œ•j ) contains a descent direction of F at ¬µj , there must exist Œ∑ ‚àà Rk¬µ such
              | that pj = (1/Œ±j )Œ•j Œ∑ is a descent direction of F at ¬µj for any Œ±j > 0. Thus, the affine subspace
              | A(¬µÃÑj , Œ•j ) contains vectors of the form ¬µ = ¬µj + Œ±j pj and Lemma C.1 guarantees the existence of
              | an interval of step sizes (Œ±j ) that satisfies the strong Wolfe conditions. Let Œ±j‚àó be any such step size.
              | Then the following relations hold
blank         | 
text          |                         F (¬µj+1 ) ‚â§ F (¬µj + Œ±j‚àó pj ) ‚â§ F (¬µj ) + c1 Œ±j‚àó pTj ‚àáF (¬µj ),                (C.26)
blank         | 
text          | where the first inequality follows from ¬µj+1 being the solution of the optimization problem in (C.24)
              | and the second holds since Œ±j‚àó satisfies the strong Wolfe conditions. This establishes the first strong
              | Wolfe condition in (C.28). For the remaining Wolfe condition, observe that pTj ‚àáF (¬µj+1 ) = 0. This
              | follows from the fact that pj = (1/Œ±j )Œ•j Œ∑ and the first-order optimality condition of (C.24), i.e.,
              | Œ•Tj ‚àáF (¬µj+1 ) = 0. Therefore, the following relationships hold
blank         | 
text          |                       pTj ‚àáF (¬µj+1 ) = 0 ‚â§ |pTj ‚àáF (¬µj + Œ±j‚àó pj )| ‚â§ c2 |pTj ‚àáF (¬µj )|,              (C.27)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                    233
blank         | 
              | 
              | 
text          | which establishes that ¬µj+1 satisfies the second Wolfe condition. Therefore, by Theorem C.1, global
              | convergence of the sequence {¬µj } is guaranteed.
blank         | 
text          | Lemma C.1. Let {¬µj } be a sequence of iterations that satisfy the update formula ¬µj+1 = ¬µj +Œ±j pj ,
              | where pj is any descent direction at ¬µj . Suppose F (¬µ) is continuously differentiable and bounded
              | below along the ray {¬µj + Œ±pj | Œ± > 0}. Then, if 0 < c1 < c2 < 1, there exist intervals of step
              | lengths satisfying the strong Wolfe conditions
blank         | 
text          |                                   F (¬µj + Œ±j pj ) ‚â§ F (¬µj ) + c1 Œ±j pTj ‚àáF (¬µj )
              |                                                                                                  (C.28)
              |                            |pTj ‚àáF (¬µj + Œ±j pj )| ‚â§ c2 |pTj ‚àáF (¬µj )|.
blank         | 
text          | Proof. Lemma 3.1 of [143].
blank         | 
text          | Theorem C.1. Let {¬µj } be a sequence of iterations that satisfies the update formula ¬µj+1 =
              | ¬µj + Œ±j pj , where pj is any descent direction at ¬µj and Œ±j satisfies the strong Wolfe conditions
              | (C.28) with 0 < c1 < c2 < 1. Suppose the F is bounded below in RN¬µ and continuously differentiable
              | in an open set N containing the level set {¬µ ‚àà RN¬µ | F (¬µ) ‚â§ F (¬µ0 )}. Assume also its gradient is
              | Lipschitz continuous on N . Then
              |                                           lim    ‚àáF (¬µj ) = 0.                                   (C.29)
              |                                           j‚Üí‚àû
blank         | 
text          | Proof. Theorem 3.2 of [143].
blank         | 
text          | Remark. In linesearch and subspace methods, it is usually considered difficult or expensive to solve
              | the low-dimensional optimization problem, e.g., (C.24), exactly. This lead to the introduction of the
              | Wolfe conditions (C.28) that define a criteria for sufficient decrease in the objective function that
              | will lead to global convergence (Theorem C.1). As a result, a slew of linesearch methods have been
              | developed to locate points that satisfy the Wolfe conditions [143]. The proposed method deviates from
              | this accepted strategy by solving the restricted optimization problem exactly to leverage the efficient
              | method developed in Chapter 5 for solving PDE-constrained optimization problems in few variables
              | using projection-based reduced-order models in the multifidelity trust region method of Chapter 3. To
              | align with standard practices, the inner iteration can be terminated once the strong Wolfe conditions
              | are satisfied without destroying global convergence.
blank         | 
text          |    Let pj be any descent direction to F at ¬µj . From Theorem C.1, the following requirements on the
              | affine subspace A(¬µÃÑj , Œ•j ) are sufficient to guarantee the iteration in (C.24) is globally convergent
blank         | 
text          |                                     ¬µÃÑj = ¬µj           pj ‚àà col(Œ•j ).                            (C.30)
blank         | 
text          | The simplest affine subspace that fulfills these requirements is defined by
              |                                                             h        i
              |                                ¬µÃÑj = ¬µj          Œ•j = Œ•gj := ‚àáF (¬µj ) ,                          (C.31)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                     234
blank         | 
              | 
              | 
text          | which reduces the iteration in (C.24) to a steepest descent method with an exact linesearch. While
              | this choice will result in a globally convergent iteration, steepest descent methods are well-known
              | to suffer from slow convergence. The remainder of the section will construct a more sophisticated
              | affine subspace such that the iteration in (C.24) quickly converges to a local minima.
              |    From the requirements in (C.30), the affine offset will always be taken as the previous iterate
              | ¬µÃÑj = ¬µj . While the requirement in (C.30) provides considerable flexibility in the definition of Œ•j , we
              | impose the stronger requirement that the affine subspace must contain the steepest descent space:
              | A(¬µj , Œ•gj ) ‚äÜ A(¬µÃÑj , Œ•j ). This is accomplished by taking the first column of Œ•j to be ‚àáF (¬µj )
              | and guarantees global convergence regardless of the other basis vectors that comprise Œ•j . These
              | auxiliary basis vectors in Œ•j will serve to improve the convergence rate of the iteration in (C.24). We
              | will consider two types of auxiliary vectors: (1) optimization-based vectors that are defined for any
              | optimization problem and (2) problem-specific information that exploits any knowledge or structure
              | of the optimization variables ¬µ.
              |    The optimization-based vectors will consist of any variety of descent directions, i.e., Newton or
              | quasi-Newton direction, or directions of negative curvature at the current iterate ¬µj . Let Pj be a
              | matrix consisting of such all optimization-based vectors and define
              |                                             h           i
              |                                         Œ•j = ‚àáF (¬µj ) Pj .                                        (C.32)
blank         | 
text          | This construction is general since the aforementioned directions can be constructed for any opti-
              | mization problem.
              |    In many applications, particularly those related to PDEs, it may be advantageous to incorporate
              | problem-specific information in the affine subspace. This is particularly true for topology optimiza-
              | tion and inverse problems where the optimization vectors have a strong connection to the underlying
              | PDE mesh. The proposed framework is sufficiently flexible to incorporate such information without
              | destroying global convergence by building Œ•j according to
              |                                           h                        i
              |                                       Œ•j = ‚àáF (¬µj ) Pj        Qj                                  (C.33)
blank         | 
text          | where Qj is a matrix whose columns consist of problem-specific vectors. Future work will develop
              | problem-specific information for various in structural and acoustic inverse problems. Algorithm 17
              | provides the complete outer iteration algorithm.
blank         | 
              | 
title         | C.2.2     Inner Iteration: Multifidelity Optimization with Reduced-Order
              |           Models
text          | Each iteration of the affine parameter space adaptation requires the solution of the PDE-constrained
              | optimization problem (C.24), which can be written as an optimization problem in few variables
              | (k¬µ  N¬µ ). Even though the optimization problem contains few variables, it is still expensive
              | to solve since each objective evaluation requires the solution of a potentially large-scale partial
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                         235
blank         | 
              | 
              | 
text          | Algorithm 17 Outer iteration: adaptive reduction of parameter space
              |  1:   Initialization: Given
              |                                                        ¬µÃÑ0 , Œ•0
              |  2:   Inner iteration: Solve restricted optimization problem (Algorithm 18)
blank         | 
text          |                                             minimize
              |                                                  j
              |                                                      F (¬µÃÑj + Œ•j Œ∑)
              |                                               Œ∑‚ààRk¬µ
blank         | 
text          |       for Œ∑ ‚àój , the optimal solution in the restricted parameter space and define ¬µ‚àój = ¬µÃÑj + Œ•j Œ∑ ‚àój
              |  3:   Update search space: Compute ‚àáF (¬µ‚àój ), the optimization-based vectors P (¬µ‚àój ), and the
              |       problem-specific vectors Q(¬µ‚àój ) and update the restricted parameter space
blank         | 
text          |                            ¬µÃÑj+1 = ¬µ‚àój           Œ•j+1 = ‚àáF (¬µ‚àój ) P (¬µ‚àój ) Q(¬µ‚àój )
blank         |                                                                                  
              | 
              | 
              | 
              | 
text          | differential equation, and the gradient requires a sensitivity or adjoint solution. The multifidelity
              | trust region method based on projection-based model reduction proposed in Chapter 5 has been
              | shown to be an efficient method to handle exactly these types of problems. This section will consider
              | a special case of the method proposed in Chapter 5 to solve each k¬µ -variable optimization problem
              | encountered in the iteration (C.24).
              |       Consider the optimization problem that arises at iteration j of (C.24)
blank         | 
text          |                                                minimize        F (¬µ)                                 (C.34)
              |                                               ¬µ‚ààA(¬µÃÑj , Œ•j )
blank         | 
              | 
text          |                                                                     j
              | which, from the definition of F and A(¬µÃÑ, Œ•), is equivalent to the k¬µ -dimensional optimization
              | problem
              |                                           minimize
              |                                                j
              |                                                    F (Œ∑; ¬µÃÑj , Œ•j ).                                 (C.35)
              |                                             Œ∑‚ààRk¬µ
blank         | 
text          | We propose to solve this PDE-constrained optimization problem in few parameters using the method
              | proposed in Chapter 5, i.e., the the multifidelity trust region method using reduced-order/hyperreduced
              | approximation models. For a fixed outer iteration j, the approximation model at iteration k of the
              | trust region method is defined as
blank         | 
text          |                                   mj, k (Œ∑) = Fr (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k ),                     (C.36)
blank         | 
text          | where Fr is defined in (C.16) and Œ¶j, k , Œ®j, k are presumed given and define a projection-based
              | reduced-order model that possesses the minimum-residual property. Details pertaining to the con-
              | struction of the trial basis Œ¶j, k (and implicitly the test basis Œ®j, k based on the minimum-residual
              | requirement (4.14)) are provided at the end of this section. The gradient of the approximation model
              | is computed exactly as
              |                                 ‚àámj, k (Œ∑) = ‚àáFr (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k )                      (C.37)
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                               236
blank         | 
              | 
              | 
text          | according to the sensitivity or adjoint method as defined in Section 2.3. In situations where the test
              | basis is not constant, the exact gradient is cumbersome to compute and may be approximated with
blank         | 
text          |                           ‚àám           d r (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k , Œ¶‚àÇj, k , Œò‚àÇj, k )
              |                           d j, k (Œ∑) = ‚àáF
              |                                                                                                             (C.38)
              |                                          d r (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k , Œ¶Œª
              |                                        = ‚àáF                                          Œª
              |                                                                              j, k , Œòj, k )
blank         | 
              | 
text          | using minimum-residual sensitivity or adjoint reduced-order models.
              |    A critical component of the multifidelity trust region method of Chapter 3 is the introduction
              | of an objective decrease error indicator œëk (¬µ) and gradient error indicator œïk (¬µ) that lead to the
              | error bounds
blank         | 
text          |                 |F (Œ∑ j, k ; ¬µÃÑj , Œ•j ) ‚àí F (Œ∑; ¬µÃÑj , Œ•j ) + mj, k (Œ∑) ‚àí mj, k (Œ∑ j, k )| ‚â§ Œ∂œëj, k (Œ∑)
              |                                                                                                             (C.39)
              |                                                    ‚àáF (Œ∑; ¬µÃÑj , Œ•j ) ‚àí ‚àámj, k (Œ∑) ‚â§ Œæœïj, k (Œ∑),
blank         | 
text          | where Œ∂, Œæ > 0 are arbitrary constants and Œ∑ j, k is the trust region center in the reduced parameter
              | space. Two options are considered for the objective decrease error indicator: the classical trust region
              | constraint œëj, k (Œ∑) = Œ∑ ‚àí Œ∑ j, k and the residual-based error indicator introduced in Section 5.1.1
blank         | 
text          |                    œëj, k (Œ∑) = r(Œ¶j, k ur (Œ∑ j, k ; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k ), ¬µÃÑj + Œ•j Œ∑ j, k ) +
              |                                                                                                             (C.40)
              |                                 r(Œ¶j, k ur (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k ), ¬µÃÑj + Œ•j Œ∑) .
blank         | 
text          | From the discussion in Section 5.1.1 that refers to the proof in Appendix B, the residual-based error
              | indicator satisfies the bound in (3.12). The classical trust region satisfies this bound, provided the
              | gradient bound and condition hold (see Chapter 3 for a complete discussion). For simplicity, only
              | the classical trust region constraint will be considered in the remainder; see Chapter 5 for a complete
              | discussion regarding the use of the residual-based error indicator. From the bounds on the gradient
              | error derived in Lemmas B.7 and B.8, the gradient error indicator is taken as
blank         | 
text          |  œïj, k (Œ∑) =Œ±1 r(Œ¶j, k ur (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k ), ¬µÃÑj + Œ•j Œ∑) +
blank         |                                                                                                             
text          |                 ‚àÇ                                                ‚àÇur
              |             Œ±2 r Œ¶j, k ur (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k ), Œ¶j, k      (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k ), ¬µÃÇ + Œ•Œ∑
              |                                                                   ‚àÇŒ∑
              |                                                                                                            (C.41)
              | if the sensitivity approach is used to compute ‚àámj, k (Œ∑) and
blank         | 
text          |    œïj, k (Œ∑) =Œ±1 r(Œ¶j, k ur (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k ), ¬µÃÑj + Œ•j Œ∑) +
              |                Œ±2 r Œª Œ¶j, k ur (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k ), Œ®j, k Œªr (Œ∑; ¬µÃÑj , Œ•j , Œ¶j, k , Œ®j, k ), ¬µÃÇ + Œ•Œ∑
blank         |                                                                                                                 
              | 
text          |                                                                                                               (C.42)
              | if the adjoint approach is used. These indicators can be modified accordingly if the minimum-
              | residual sensitivity or adjoint approach is used to compute the gradient approximation ‚àám
              |                                                                                        d j, k (¬µ).
              | Finally, the trust region method of Chapter 3 provides the flexibility to introduce an inexpensive
              | approximation of the objective decrease œàj, k (Œ∑) and corresponding error indicator Œ∏j, k (Œ∑) to mitigate
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                          237
blank         | 
              | 
              | 
text          | the computational burden of computing the actual-to-predicted reduction at each trust region step.
              | Section 5.3 details an approach that defines œàj, k (Œ∑) based on partially converged PDE solutions and
              | Œ∏j, k (Œ∑) as the residual-based error indicator. This construction can be used in this context without
              | modification and does not need to be discussed further.
              |    With the definition of the necessary approximations and corresponding error indicators, the only
              | remaining conditions that are left to satisfy are the error conditions in (3.14) and (3.15), restated
              | here for convenience
              |                               œëj, k (Œ∑ j, k ) ‚â§ Œ∫œë ‚àÜj, k
              |                                                                                                       (C.43)
              |                               œïj, k (Œ∑ j, k ) ‚â§ Œ∫œï min{ ‚àámj, k (Œ∑ j, k ) , ‚àÜj, k }.
blank         | 
text          | Since the classical trust region constraint is used to define œëj, k (Œ∑), the first condition is always satis-
              | fied since œëj, k (Œ∑ j, k ) = 0. The second condition, called the gradient condition, is not always satisfied
              | a priori and relies critically on the construction of the reduced-order model. The strategy taken con-
              | structs the reduced-order model such that the reconstructed primal and sensitivity/adjoint solutions
              | exactly match the corresponding high-dimensional model quantity. This will obviously guarantee
              | œïj, k (Œ∑ j, k ) = 0 and the gradient condition will be satisfied. Without repeating the details from
              | Section 5.1.2, the reduced-order model and its sensitivity/adjoint will be possess these interpola-
              | tion properties provided primal and sensitivity/adjoint minimum-residual reduced-order models are
              | used, the relationships between the reduced-order bases in (4.35) and (4.63) hold, and the trial basis
              | possesses the following properties
blank         | 
text          |                                                        u(Œ∑ j, k ; ¬µÃÑj , Œ•j ) ‚àà col(Œ¶j, k )
              |                                                      ‚àÇu
              |                                                         (Œ∑ ; ¬µÃÑ , Œ•j ) ‚àà col(Œ¶j, k )                  (C.44)
              |                                                      ‚àÇŒ∑ j, k j
              |                                           ‚àÇr
              |                            ŒòŒª
              |                             j, k (u, ¬µ)      (u, ¬µ)T Œª(Œ∑ j, k ; ¬µÃÑj , Œ•j ) ‚àà col(Œ¶j, k )
              |                                           ‚àÇu
blank         | 
text          | where ¬µ = ¬µÃÑj + Œ•j Œ∑ and u = u(¬µ). The conditions in (4.14), (4.35), and (4.63) completely specify
              | the test Œ®j, k , sensitivity Œ¶‚àÇj, k , and adjoint Œ¶Œª
              |                                                    j, k bases in terms of the trial basis Œ¶j, k and optimality
              | metrics Œòj, k , Œò‚àÇj, k , ŒòŒª
              |                           j, k . Therefore, the reduced-order model will possess the required interpolation
              | properties provided the trial basis is constructed such that (C.44) holds.
blank         | 
text          | Remark. The requirement that the reduced-order model is exact at the trust region center leads
              | to the stronger condition œïj, k (Œ∑ j, k ) = 0 than required by (3.15) and may result in wasted effort.
              | The weaker condition in (3.15) can be enforced directly using partially converged solutions in the
              | construction of Œ¶j, k as detailed in Section 5.2; however, this is not considered in this section.
blank         | 
text          |    Before continuing with the construction of Œ¶j, k , the following notation is introduced to allow
              | the sensitivity and adjoint method to be treated simultaneously and compactly:
blank         | 
text          |                                ‚àÇu
              |                              Ô£±
              |                              Ô£¥
              |                              Ô£≤    (Œ∑; ¬µÃÑ, Œ•)                   sensitivity method
              |                vÃÇ(Œ∑; ¬µÃÑ, Œ•) = ‚àÇŒ∑                                                                      (C.45)
              |                              Ô£≥ŒòŒª (u, ¬µ) ‚àÇr (u, ¬µ)T Œª(Œ∑; ¬µÃÑ, Œ•) adjoint method
              |                              Ô£¥
              |                                 j, k
              |                                            ‚àÇu
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                           238
blank         | 
              | 
              | 
text          | where ¬µ = ¬µÃÑj + Œ•j Œ∑ and u = u(¬µ). Since the sensitivity and adjoint method are rarely employed
              | simultaneously the condition in (C.44) is weakened to
blank         | 
text          |                                         u(Œ∑ j, k ; ¬µÃÑj , Œ•j ) ‚àà col(Œ¶j, k )
              |                                                                                                          (C.46)
              |                                         vÃÇ(Œ∑ j, k ; ¬µÃÑj , Œ•j ) ‚àà col(Œ¶j, k ).
blank         | 
text          | Next, define primal and dual snapshot matrices according to the recursive relationships
              |                            h                                                                         i
              |                    Uj, k = Uj‚àí1, nj‚àí1        u(Œ∑ j, 0 , ¬µÃÑj , Œ•j ) ¬∑ ¬∑ ¬∑    u(Œ∑ j, k‚àí1 , ¬µÃÑj , Œ•j )
              |                            h                                                                         i   (C.47)
              |                    VÃÇj, k = VÃÇj‚àí1, nj‚àí1      vÃÇ(Œ∑ j, 0 , ¬µÃÑj , Œ•j ) ¬∑ ¬∑ ¬∑   vÃÇ(Œ∑ j, k‚àí1 , ¬µÃÑj , Œ•j )
blank         | 
text          | where U‚àí1, k = ‚àÖ, VÃÇ‚àí1, k = ‚àÖ, and nj is the number of inner iterations corresponding to outer
              | iteration j. Then, the reduced-order basis is defined according to the heterogeneous, span-preserving
              | variant of POD (Algorithm 7) as
blank         | 
text          |                    Œ¶j, k = PODHSP(u(Œ∑ j, k , ¬µÃÑj , Œ•j ), Uj, k , vÃÇ(Œ∑ j, k , ¬µÃÑj , Œ•j ), VÃÇj, k ).       (C.48)
blank         | 
text          | By construction, the basis satisfies (C.44) and possesses additional information to improve the para-
              | metric robustness of the reduced-order model. The complete inner iteration algorithm is provided
              | in Algorithm 18.
meta          | APPENDIX C. TWO-LEVEL REDUCTION FOR LARGE-SCALE PDE OPTIMIZATION                                                                      239
blank         | 
              | 
              | 
text          | Algorithm 18 Inner iteration: trust region method based on reduced-order models in reduced
              | parameter space
              |  1:   Initialization: Given
              |             ¬µÃÑj , Œ•j , Œ∑ j, 0 , Uj‚àí1, nj‚àí1 , VÃÇj‚àí1, nj‚àí1 , ‚àÜj, 0 , 0 < Œ≥ < 1, ‚àÜmax > 0, 0 < Œ∑1 < Œ∑2 < 1,
              |                              0 < Œ∫œë < 1, 0 < Œ∫œï , 0 < œâ < 1, {rk }‚àû     k=0 such that rk ‚Üí 0
blank         | 
text          |  2:   Model and constraint update: If previous model and constraint are sufficient for convergence
blank         | 
text          |              œëj, k‚àí1 (Œ∑ j, k ) ‚â§ Œ∫œë ‚àÜj, k                 œïj, k‚àí1 (Œ∑ j, k ) ‚â§ Œ∫œï min{ ‚àámj, k‚àí1 (Œ∑ j, k ) , ‚àÜj, k },
blank         | 
text          |       re-use for the current iteration: mj, k (Œ∑) := mj, k‚àí1 (Œ∑) and œëj, k (Œ∑) := œëj, k‚àí1 (Œ∑). Otherwise,
              |       evaluate primal and sensitivity or adjoint solution of high-dimensional model
              |                                                 ‚àÇu                                   ‚àÇr
              |         uj, k := u(¬µj, k )          vÃÇj, k :=      (¬µ ) or ŒòŒª
              |                                                             j, k (u(¬µj, k ), ¬µj, k )    (u(¬µj, k ), ¬µj, k )T Œª(¬µj, k )
              |                                                 ‚àÇ¬µ j, k                              ‚àÇu
              |       where ¬µj, k = ¬µÃÑj + Œ•j Œ∑ j, k and compute reduced-order basis via span-preserving variant of POD
              |       (Algorithm 7)
              |                                      Œ¶j, k = PODHSP(uj, k , Uj, k , vÃÇj, k , VÃÇj, k ),
              |       define model and constraint as
              |                        mj, k (Œ∑) = f (Œ¶j, k ur (¬µÃÑj + Œ•j Œ∑; Œ¶j, k , Œ®j, k ), ¬µÃÑj + Œ•j Œ∑)
              |                        œëj, k (Œ∑) = r(Œ¶j, k ur (¬µÃÑj + Œ•j Œ∑ j, k ; Œ¶j, k , Œ®j, k ), ¬µÃÑj + Œ•j Œ∑ j, k )            Œòj, k
              |                                                                                                                        +
              |                                        r(Œ¶j, k ur (¬µÃÑj + Œ•j Œ∑; Œ¶j, k , Œ®j, k ), ¬µÃÑj + Œ•j Œ∑)           Œòj, k
              |                                                                                                               ,
blank         | 
text          |       and update snapshot matrices
blank         |                                                                                                                               
text          |           Uj, k+1 ‚Üê Uj‚àí1, nj‚àí1 uj, 0                ¬∑¬∑¬∑   uj, k           VÃÇj, k+1 ‚Üê VÃÇj‚àí1, nj‚àí1           vÃÇj, 0   ¬∑¬∑¬∑    vÃÇj, k .
blank         | 
text          |  3:   Step computation: Solve (exactly) the trust region subproblem
blank         | 
text          |                                       min mj, k (Œ∑)           subject to           œëj, k (Œ∑) ‚â§ ‚àÜj, k
              |                                     Œ∑‚ààRk¬µ
blank         | 
text          |     for a candidate, Œ∑ÃÇ j, k , using interior-point method of Section 3.1.2.
              |  4: Actual-to-predicted reduction: Compute actual-to-predicted reduction ratio
              |            Ô£±
              |            Ô£¥
              |            Ô£≤            1                                if œëj, k (Œ∑ÃÇ j, k )œâ ‚â§ Œ∑ min{mj, k (Œ∑ j, k ) ‚àí mj, k (Œ∑ÃÇ j, k ), rk }
              |     œÅj, k = F (¬µÃÑj + Œ•j Œ∑ j, k ) ‚àí F (¬µÃÑj + Œ•j Œ∑ÃÇ j, k )
              |            Ô£¥                                             otherwise
              |                    mj, k (Œ∑ j, k ) ‚àí mj, k (Œ∑ÃÇ j, k )
              |            Ô£≥
blank         | 
text          |     where Œ∑ < min{Œ∑1 , 1 ‚àí Œ∑2 }
              |  5: Step acceptance:
blank         | 
text          |              if        œÅj, k ‚â• Œ∑1        then         Œ∑ j, k+1 = Œ∑ÃÇ j, k         else       Œ∑ j, k+1 = Œ∑ j, k          end if
blank         | 
text          |  6:   Trust region update:
blank         | 
text          |                   if      œÅj, k ‚â§ Œ∑1                  then            ‚àÜk+1 ‚àà (0, Œ≥œëj, k (Œ∑ÃÇ j, k )]                 end if
              |                   if      œÅj, k ‚àà (Œ∑1 , Œ∑2 )          then            ‚àÜk+1 ‚àà [Œ≥œëj, k (Œ∑ÃÇ j, k ), ‚àÜj, k ]            end if
              |                   if      œÅj, k ‚â• Œ∑2                  then            ‚àÜk+1 ‚àà [‚àÜj, k , ‚àÜmax ]                        end if
meta          | Appendix D
blank         | 
title         | Time-Dependent PDE-Constrained
              | Optimization under Periodicity
              | Constraints
blank         | 
text          | This appendix summarizes the work in [211, 212].
blank         | 
              | 
title         | D.1       Governing Equations and Discretization
text          | This section is devoted to the treatment of conservation laws (2.9) on a parametrized, deforming
              | domain using an Arbitrary Lagrangian-Eulerian (ALE) description of the governing equations and
              | a brief discussion of a globally high-order numerical discretization of the ALE form of the system of
              | conservation laws that closely parallels that in Chapter 2. Subsequently, Section D.2 will develop
              | the corresponding fully discrete adjoint equations and the adjoint method for constructing gradients
              | of quantities of interest.
              |    The methods introduced in this work are not necessarily limited to Partial Differential Equations
              | (PDE) that can be written as conservation laws (D.1). In Section D.1.2, the chosen spatial dis-
              | cretization (discontinuous Galerkin Arbitrary Lagrangian-Eulerian method) is applied to the PDE,
              | resulting in a system of first-order ODEs, which is the point of departure for all adjoint-related
              | derivations. Time-dependent PDEs that are not conservation laws can be written similarly at the
              | semi-discrete level after application of an appropriate spatial discretization, e.g., a continuous fi-
              | nite element method for parabolic PDEs. In this work, the scope is limited to first-order temporal
              | systems, or those which are recast as such.
blank         | 
              | 
              | 
              | 
meta          |                                                  240
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                        241
blank         | 
              | 
              | 
text          |                                                                      nda
blank         | 
              | 
              | 
text          |                                    N dA             x=x(X)
              |                                                                            v
blank         | 
text          |                                                               x2
blank         | 
text          |                                           V
blank         | 
text          |                               X2                                       x1
blank         | 
              | 
              | 
text          |                                      X1
blank         | 
              | 
text          |           Figure D.1: Time-dependent mapping between reference and physical domains.
blank         | 
              | 
text          | D.1.1     System of Conservation Laws on Deforming Domain: Arbitrary
              |           Lagrangian-Eulerian Description
              | Consider a general system of conservation laws, defined on a parametrized, deforming domain,
              | v(¬µ, t), written at the continuous level as
blank         | 
text          |                                 ‚àÇU
              |                                    + ‚àá ¬∑ F (U , ‚àáU ) = 0           in v(¬µ, t)                    (D.1)
              |                                 ‚àÇt
blank         | 
text          | where the physical flux is decomposed into an inviscid and a viscous part F (U , ‚àáU ) = F inv (U ) +
              | F vis (U , ‚àáU ), U (x, ¬µ, t) is the solution of the system of conservation laws, t ‚àà (0, T ) represents
              | time, and ¬µ ‚àà RN¬µ is a vector of parameters. This work will focus on the case where the domain
              | is parametrized by ¬µ, although extension to other types of parameters, e.g., constants defining the
              | conservation law, is straightforward. The conservation law on a deforming domain is transformed
              | into a conservation law on a fixed reference domain through the introduction of a time-dependent
              | mapping between the physical and reference domains, resulting in an Arbitrary Lagrangian-Eulerian
              | description of the governing equations.
              |    Denote the physical domain by v(¬µ, t) ‚äÇ Rnsd and the fixed, reference domain by V ‚äÇ Rnsd ,
              | where nsd is the number of spatial dimensions. At each time t, let G be a time-dependent diffeomor-
              | phism between the reference domain and physical domain: x(X, ¬µ, t) = G(X, ¬µ, t), where X ‚àà V is
              | a point in the reference domain and x(X, ¬µ, t) ‚àà v(¬µ, t) is the corresponding point in the physical
              | domain at time t and parameter configuration ¬µ.
              |    The transformed system of conservation laws from (D.1), under the mapping G, defined on the
              | reference domain takes the form
blank         | 
text          |                                ‚àÇUX
              |                                               + ‚àáX ¬∑ FX (UX , ‚àáX UX ) = 0                        (D.2)
              |                                 ‚àÇt    X
blank         | 
text          | where ‚àáX denotes spatial derivatives with respect to the reference variables, X. The transformed
              | state vector, UX , and its corresponding spatial gradient with respect to the reference configuration
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                        242
blank         | 
              | 
              | 
text          | take the form
              |                                                                   ‚àÇg
              |                           UX = gU ,          ‚àáX UX = g ‚àí1 UX         + g‚àáU ¬∑ G,                  (D.3)
              |                                                                   ‚àÇX
              |                                         ‚àÇx   ‚àÇG
              | where G = ‚àáX G, g = det(G), vG =           =    , and the arguments have been dropped, for brevity.
              |                                         ‚àÇt   ‚àÇt
              | The transformed fluxes are
blank         | 
text          |                              inv          vis
              |          FX (UX , ‚àáX UX ) = FX   (UX ) + FX   (UX , ‚àáX UX ),
              |                 FX inv
              |                        (UX ) = gF inv (g ‚àí1 UX )G‚àíT ‚àí UX ‚äó G‚àí1 vG ,                              (D.4)
blank         |                                                                          
text          |                                                                     ‚àÇg
              |          vis
              |         FX   (UX , ‚àáX UX ) = gF vis g ‚àí1 UX , g ‚àí1 ‚àáX UX ‚àí g ‚àí1 UX       G‚àí1 G‚àíT .
              |                                                                     ‚àÇX
blank         | 
text          | For details regarding the derivation of the transformed equations, the reader is referred to [152].
              |    When integrated using inexact numerical schemes, this ALE formulation does not satisfy the
              | Geometric Conservation Law (GCL) [60, 152]. This is overcome by introducing an auxiliary variable
              | gÃÑ, defined as the solution of
              |                                         ‚àÇgÃÑ
              |                                             ‚àí ‚àáX ¬∑ gG‚àí1 vG = 0.
blank         |                                                           
text          |                                                                                                  (D.5)
              |                                         ‚àÇt
              | The auxiliary variable, gÃÑ is used to modify the transformed conservation law according to
blank         | 
text          |                                  ‚àÇUXÃÑ
              |                                             + ‚àáX ¬∑ FXÃÑ (UXÃÑ , ‚àáX UXÃÑ ) = 0                       (D.6)
              |                                   ‚àÇt    X
blank         | 
text          | where the GCL-transformed state variables are
blank         | 
text          |                                                                   ‚àÇgÃÑ
              |                           UXÃÑ = gÃÑU ,        ‚àáX UXÃÑ = gÃÑ ‚àí1 UXÃÑ       + gÃÑ‚àáU ¬∑ G                 (D.7)
              |                                                                   ‚àÇX
blank         | 
text          | and the corresponding fluxes
blank         | 
text          |                                 inv           vis
              |          FXÃÑ (UXÃÑ , ‚àáX UXÃÑ ) = FXÃÑ  (UXÃÑ ) + FXÃÑ  (UXÃÑ , ‚àáX UXÃÑ ),
              |                 FXÃÑ inv
              |                         (UXÃÑ ) = gF inv (gÃÑ ‚àí1 UXÃÑ )G‚àíT ‚àí UXÃÑ ‚äó G‚àí1 vG ,                         (D.8)
blank         |                                                                                  
text          |                                                                            ‚àÇgÃÑ
              |          vis
              |         FXÃÑ  (UXÃÑ , ‚àáX UXÃÑ ) = gF vis gÃÑ ‚àí1 UXÃÑ , gÃÑ ‚àí1 ‚àáX UXÃÑ ‚àí gÃÑ ‚àí1 UXÃÑ       G‚àí1 G‚àíT .
              |                                                                            ‚àÇX
blank         | 
text          | It was shown in [152] that the transformed equations (D.6) satisfy the GCL. In the next section, the
              | ALE description of the governing equations (D.2) and (D.6) will be converted to first-order form
              | and discretized via a high-order discontinuous Galerkin method.
blank         | 
              | 
title         | D.1.2      Arbitrary Lagrangian-Eulerian Discontinuous Galerkin Method
text          | The ALE description of the conservation law without GCL augmentation will be considered first. To
              | proceed, the second-order system of partial differential equations in (D.2) is converted to first-order
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          243
blank         | 
              | 
              | 
text          | form
              |                                   ‚àÇUX
              |                                              + ‚àáX ¬∑ FX (UX , QX ) = 0
              |                                    ‚àÇt    X                                                         (D.9)
              |                                                       QX ‚àí ‚àáX UX = 0,
blank         | 
text          | where QX is introduced as an auxiliary variable to represent the spatial gradient of the UX . Equa-
              | tion (D.9) is discretized using a standard nodal discontinuous Galerkin finite element method [46],
              | which, after local elimination of the auxiliary variables QX , leads to the following system of ODEs
blank         | 
text          |                                              ‚àÇuX
              |                                       MX         = ruX (uX , ¬µ, t),                               (D.10)
              |                                               ‚àÇt
blank         | 
text          | where MX is the block-diagonal, symmetric, fixed mass matrix, uX is the vectorization of UX at
              | all nodes in the high-order mesh, and ruX is the nonlinear function defining the DG discretization
              | of the inviscid and viscous fluxes.
              |    The GCL augmentation is treated identically, i.e., conversion to first-order form and subsequent
              | application of the discontinuous Galerkin finite element method, where UXÃÑ is taken as the state
              | variable. The result is a system of ODEs corresponding to a high-order ALE scheme that satisfies
              | the GCL
              |                                           ‚àÇ gÃÑ
              |                                         MgÃÑ    = rgÃÑ (¬µ, t)
              |                                           ‚àÇt                                                      (D.11)
              |                                         ‚àÇu
              |                                       MX XÃÑ = ruXÃÑ (uXÃÑ , gÃÑ, ¬µ, t)
              |                                          ‚àÇt
              | where each term is defined according to their counterparts in (D.10). From the conservation law
              | defining gÃÑ (D.5), the corresponding flux is continuous, implying the physical flux gG‚àí1 vG can be
              | used as the numerical flux. This implies no information is required from neighboring elements and
              | (D.5) can be solved at the element level, i.e., statically condensed. Furthermore, the gÃÑ residual, rgÃÑ ,
              | does not depend on gÃÑ itself since the physical flux gG‚àí1 vG is independent of gÃÑ.
              |    Since the equation for gÃÑ does not depend on uXÃÑ , it can be solved independently of the equation
              | for uXÃÑ . This enables gÃÑ to be considered an implicit function of ¬µ, i.e., gÃÑ = gÃÑ(¬µ, t), through
              | application of the implicit function theorem. Then, (D.11) reduces to
blank         | 
text          |                                        ‚àÇuXÃÑ
              |                                  MX         = ruXÃÑ (uXÃÑ , gÃÑ(¬µ, t), ¬µ, t).                        (D.12)
              |                                         ‚àÇt
blank         | 
text          | Equations (D.10) and (D.12) are abstracted into the following system of ODEs
blank         | 
text          |                                                  ‚àÇu
              |                                              M      = r(u, ¬µ, t),                                 (D.13)
              |                                                  ‚àÇt
blank         | 
text          | for convenience in the derivation of the fully discrete adjoint equations. Evaluation of the residual,
              | r, in (D.13) at parameter ¬µ and time t requires evaluation of the mapping, x(¬µ, t) and xÃá(¬µ, t),
              | and gÃÑ(¬µ, t), if GCL augmentation is employed. The implicit dependence of gÃÑ on ¬µ requires special
              | treatment when computing derivatives with respect to ¬µ, which will be required in the adjoint
              | method (Section D.2). Treatment of such terms will be deferred to Section D.2.4.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                        244
blank         | 
              | 
              | 
text          |    A convenient property of this DG-ALE scheme is that all computations are performed on the
              | reference domain which is independent of time and parameter. This implies that the mass matrix
              | of the ODE (D.13) is also time- and parameter-independent, which simplifies all adjoint compu-
              |                                                        ‚àÇM        ‚àÇM
              | tations introduced in Section D.2 as terms involving        and       are identically zero. This,
              |                                                         ‚àÇu       ‚àÇ¬µ
              | in turn, simplifies the implementation of the adjoint method and translates to computational sav-
              | ings since contractions with these third-order tensor are not required; see [88] for a discretization
              | with parameter-dependent mass matrices and the corresponding adjoint derivation. In subsequent
              | sections, it will be assumed that the mass matrix is time- and parameter-independent.
              |    The DG-ALE scheme outlined in this section constitutes a spatial discretization, which yields a
              | system of ODEs when applied to the PDE in (D.1). The semi-discrete form of the conservation law
              | is the point of departure for the remainder of this document. The subsequent development applies to
              | any system of ODEs of the form (D.13) without relying on the specific spatial discretization scheme
              | employed. The DG-ALE scheme was chosen to provide a high-order, stable spatial discretization of
              | the conservation law (D.1).
              |    The diagonally implicit Runge-Kutta scheme introduced in Section 2.1.3 is applied to the system
              | of ODEs for a stable, high-order implicit discretization, repeated here for convenience
blank         | 
text          |                                         u(0) = u0 (¬µ)
              |                                                               s
              |                                                                         (n)
              |                                                               X
              |                                         u(n) = u(n‚àí1) +             bi ki                                       (D.14)
              |                                                               i=1
blank         |                                                                            
text          |                                          (n)            (n)
              |                                   M ki         = ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn ,
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s, where Nt are the number of time steps in the temporal dis-
              | cretization and s is the number of stages in the DIRK scheme. The temporal domain, [0, T ] is
              | discretized into Nt segments with endpoints {t0 , t1 , . . . , tNt }, with the nth segment having length
              |                                                                               (n)
              | ‚àÜtn = tn ‚àí tn‚àí1 for n = 1, . . . , Nt . Additionally, in (D.14), ui                 is used to denote the approximation
              |     (n)
              | of u      at the ith stage of time step n
blank         | 
text          |                                                                                         i
              |                         (n)       (n)               (n)                                           (n)
              |                                                                                         X
              |                       ui      = ui (u(n‚àí1) , k1 , . . . , ks(n) ) = u(n‚àí1) +                  aij kj .          (D.15)
              |                                                                                         j=1
blank         | 
              | 
text          | From (D.14), a complete time step requires the solution of a sequence of s nonlinear systems of
              | equation of size Nu .
              |    Finally, a solver-consistent discretization (Section 2.1.4) is applied to discretize output quantities
              | of interest that take the form
              |                                                       Z tZ
              |                                      F(U , ¬µ, t) =                f (U , ¬µ, œÑ ) dS dœÑ                           (D.16)
              |                                                           0   Œì
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                245
blank         | 
              | 
              | 
text          | to yield the update equations in (2.45) and the fully discrete quantity of interest
blank         | 
text          |                                                                     (1)
              |                                        F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) )
blank         | 
text          | in (2.46). The generalization to other types of quantities of interest, such as volumetric integrals and
              | instantaneous or pointwise quantities of interest, is immediate as the specific form of the quantity
              | of interest will be abstracted away at the fully discrete level. The form in (D.16) will be used in the
              | physical setup of the applications in Sections D.2.5‚ÄìD.2.6.
blank         | 
              | 
title         | D.2       Fully Discrete, Time-Dependent Adjoint Equations
text          | The purpose of this section is to derive an expression for the total derivative of the discrete quantity
              | of interest F in (2.46), which can be expanded as
blank         | 
text          |                                              N                            N    s                 (n)
              |                           dF   ‚àÇF   Xt
              |                                        ‚àÇF ‚àÇu(n) X   t X
              |                                                            ‚àÇF ‚àÇki
              |                              =    +      (n)
              |                                                 +            (n) ‚àÇ¬µ
              |                                                                     ,                                  (D.17)
              |                           d¬µ   ‚àÇ¬µ n=0 ‚àÇu     ‚àÇ¬µ   n=1 i=1 ‚àÇk                           i
blank         | 
text          |                                                                                                  (n)
              |                                                                ‚àÇu(n)       ‚àÇki
              | that depends on the sensitivities of the state variables,            and        . Each of the N¬µ state
              |                                                                 ‚àÇ¬µ          ‚àÇ¬µ
              | variable sensitivities is the solution of a linear evolution equation of the same dimension and number
              | of steps as the primal equation (D.14), rendering these quantities intractable to compute when N¬µ is
              | large. Elimination of the state variable sensitivities from (D.17) is accomplished through introduction
              | of the adjoint equations corresponding to the functional F , and the corresponding dual variables.
              | From the derivation of the adjoint equation in Section D.4.1, an expression for the reconstruction of
              | the gradient of F , independent of the state variables sensitivities, follows naturally. At this point,
              | it is emphasized that F represents any quantity of interest whose gradient is desired, such as the
              | optimization objective function or a constraint. This section concludes with a discussion of the
              | advantages of the fully discrete framework in the setting of the high-order numerical scheme.
              |    Before proceeding to the derivation of the adjoint method, the following definitions are introduced
              | for the Runge-Kutta stage equations and state updates
blank         | 
text          |                                   rÃÉ (0) (u(0) , ¬µ) = u(0) ‚àí u0 (¬µ) = 0
              |                                                                                s
              |                             (n)                                                            (i)
              |                                                                                X
              |     rÃÉ (n) (u(n‚àí1) , u(n) , k1 , . . . , ks(n) , ¬µ) = u(n) ‚àí u(n‚àí1) ‚àí                bi ki = 0         (D.18)
              |                                                                                i=1
blank         |                                                                                                
text          |               (n)           (n)           (n)                (n)            (n)
              |             Ri (u(n‚àí1) , k1 , . . . , ki , ¬µ) = M ki               ‚àí ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn = 0
blank         | 
meta          | for n = 1, . . . , n and i = 1, . . . , s. Differentiation of these expressions with respect to ¬µ gives rise to
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                      246
blank         | 
              | 
              | 
text          | the fully discrete sensitivity equations
blank         | 
text          |                                                                   ‚àÇ rÃÉ (0)   ‚àÇ rÃÉ (0) ‚àÇu(0)
              |                                                                            +                =0
              |                                                                    ‚àÇ¬µ        ‚àÇu(0) ‚àÇ¬µ
              |                                                                            s            (n)
              |                       ‚àÇ rÃÉ (n)   ‚àÇ rÃÉ (n) ‚àÇu(n)    ‚àÇ rÃÉ (n) ‚àÇu(n‚àí1) X ‚àÇ rÃÉ (n) ‚àÇkp
              |                                +                +                    +                      =0
              |                        ‚àÇ¬µ        ‚àÇu(n) ‚àÇ¬µ         ‚àÇu(n‚àí1) ‚àÇ¬µ                     (n) ‚àÇ¬µ                        (D.19)
              |                                                                          p=1 ‚àÇkp
              |                                           (n)           (n)           i     (n)           (n)
              |                                       ‚àÇRi            ‚àÇRi    ‚àÇu(n‚àí1) X ‚àÇRi ‚àÇkj
              |                                                 +                  +               =0
              |                                        ‚àÇ¬µ           ‚àÇu(n‚àí1) ‚àÇ¬µ       j=1 ‚àÇk
              |                                                                             (n) ‚àÇ¬µ
              |                                                                                   j
blank         | 
              | 
text          | where n = 1, . . . , Nt , i = 1, . . . , s, and arguments have been dropped.
blank         | 
              | 
title         | D.2.1       Derivation
text          | The derivation of the fully discrete adjoint equations corresponding to the quantity of interest, F ,
              | begins with the introduction of test variables
blank         | 
text          |                                                               (n)
              |                                              Œª(0) , Œª(n) , Œ∫i       ‚àà RNu                                      (D.20)
blank         | 
text          |                                                                                                  dF
              | for n = 1, . . . , Nt and i = 1, . . . , s. To eliminate the state sensitivities from the expression for
              |                                                                                                      in
              |                                                                                                  d¬µ
              | (D.17), multiply the sensitivity equations (D.19) by the test variables, integrate (sum in the discrete
              | setting) over the time domain, and subtract from the expression for the gradient in (D.17) to obtain
blank         | 
              | 
text          |                 Nt                      Nt X   s           (n)
              |                     ‚àÇF ‚àÇu(n) X
              |                                                                              (0)
              |                                                                                         ‚àÇ rÃÉ (0) ‚àÇu(0)
blank         |                                                                                                          
text          |      dF   ‚àÇF   X                                   ‚àÇF ‚àÇki            (0) T ‚àÇ rÃÉ
              |         =    +                      +                           ‚àíŒª                   +
              |      d¬µ   ‚àÇ¬µ n=0 ‚àÇu(n) ‚àÇ¬µ              n=1 i=1 ‚àÇki
              |                                                      (n) ‚àÇ¬µ                   ‚àÇ¬µ        ‚àÇu(0) ‚àÇ¬µ
              |                 Nt
              |                           "                                                           s
              |                                                                                                            #
              |                                  (n)          (n)    (n)         (n)       (n‚àí1)                (n)    (n)
              |                X        T   ‚àÇ rÃÉ         ‚àÇ rÃÉ     ‚àÇu        ‚àÇ rÃÉ      ‚àÇu             X    ‚àÇ  rÃÉ     ‚àÇk p
              |              ‚àí     Œª(n)               +                  +                        +                            (D.21)
              |                n=1
              |                               ‚àÇ¬µ         ‚àÇu(n) ‚àÇ¬µ          ‚àÇu(n‚àí1) ‚àÇ¬µ                p=1 ‚àÇkp
              |                                                                                                 (n) ‚àÇ¬µ
blank         | 
text          |                                Ô£Æ                                                              Ô£π
              |                 Nt Xs                  (n)          (n)                  i       (n)    (n)
              |                X         (n) T Ô£∞ ‚àÇRi              ‚àÇRi    ‚àÇu(n‚àí1) X ‚àÇRi ‚àÇkj Ô£ª
              |              ‚àí         Œ∫i                   +                        +                           .
              |                n=1 i=1
              |                                      ‚àÇ¬µ         ‚àÇu(n‚àí1) ‚àÇ¬µ             j=1 ‚àÇk
              |                                                                                 (n) ‚àÇ¬µ
              |                                                                                    j
blank         | 
              | 
text          |                                                                             dF
              | The right side of the equality in (D.21) is an equivalent expression for       for any value of the test
              |                                                                             d¬µ
              | variables since the terms in the brackets are zero, i.e., the sensitivity equations. Re-arrangement of
              |                                                         dF
              | terms in (D.21) leads to the following expression for       , where the state variable sensitivities have
              |                                                         d¬µ
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                   247
blank         | 
              | 
              | 
text          | been isolated
blank         | 
text          |                                         (Nt )                  Nt                        Nt Xs            (n)
              |                                                   ‚àÇu(Nt ) X                      (n)
blank         |                                              
text          |    dF   ‚àÇF       ‚àÇF        (Nt ) T ‚àÇ rÃÉ                               (n) T ‚àÇ rÃÉ
              |                                                                                         X
              |                                                                                                   (n) T ‚àÇRp
              |       =    +           ‚àí Œª                                  ‚àí       Œª                ‚àí           Œ∫p
              |    d¬µ   ‚àÇ¬µ     ‚àÇu(Nt )             ‚àÇu(Nt )           ‚àÇ¬µ       n=0
              |                                                                               ‚àÇ¬µ        n=1 p=1
              |                                                                                                          ‚àÇ¬µ
              |               Nt
              |                  "                                                                    s
              |                                                                                                          #
              |                                                    (n‚àí1)                    (n)                      (n)
              |              X        ‚àÇF           (n‚àí1)  T   ‚àÇ rÃÉ             (n) T   ‚àÇ rÃÉ          X     (n) T ‚àÇR
              |                                                                                                      i     ‚àÇu(n‚àí1)
              |            +                ‚àí   Œª                         ‚àí  Œª                    ‚àí       Œ∫i
              |              n=1
              |                    ‚àÇu(n‚àí1)                    ‚àÇu(n‚àí1)                ‚àÇu(n‚àí1) i=1                ‚àÇu(n‚àí1)       ‚àÇ¬µ
              |                      Ô£Æ                                                          Ô£π
              |               Nt Xs                             (n)       s                (n)       (n)
              |                      Ô£∞ ‚àÇF ‚àí Œª(n) ‚àÇ rÃÉ                          (n) T ‚àÇRi Ô£ª ‚àÇkp
              |              X                           T              X
              |            +              (n)                    (n)
              |                                                       ‚àí      Œ∫ i          (n)
              |                                                                                          .
              |              n=1 p=1 ‚àÇkp                   ‚àÇkp           i=p          ‚àÇkp           ‚àÇ¬µ
              |                                                                                                                             (D.22)
              |                            (n)              (n)
              | The dual variables, Œª            and       Œ∫i ,   which have remained arbitrary to this point, are chosen as the
              | solution to the following equations
blank         | 
text          |                                                                T
              |                                                   ‚àÇ rÃÉ (Nt )           ‚àÇF
              |                                                        (N  )
              |                                                              Œª(Nt ) =
              |                                                   ‚àÇu     t            ‚àÇu(Nt )
              |                                  T                         T                                         s              (n) T
              |                    ‚àÇ rÃÉ (n)   (n)   ‚àÇ rÃÉ (n‚àí1)   (n‚àí1)     ‚àÇF T X ‚àÇRi           (n)
              |                             Œª     +            Œª       =         ‚àí             Œ∫i                                           (D.23)
              |                   ‚àÇu(n‚àí1)           ‚àÇu(n‚àí1)              ‚àÇu(n‚àí1)   i=1
              |                                                                        ‚àÇu(n‚àí1)
blank         | 
text          |                                                    (n)             T                                     T
              |                                                s
              |                                                X ‚àÇRj                    (n)        ‚àÇF         ‚àÇ rÃÉ (n)
              |                                                           (n)
              |                                                                        Œ∫j     =     (n)
              |                                                                                           ‚àí       (n)
              |                                                                                                              Œª(n)
              |                                                j=i   ‚àÇki                          ‚àÇki         ‚àÇki
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s. These are the fully discrete adjoint equations corresponding to
              | the primal evolution equations in (D.18) and quantity of interest F . Defining the dual variables as
              |                                                                     dF
              | the solution of the adjoint equations in (D.23), the expression for    in (D.22) reduces to
              |                                                                     d¬µ
blank         | 
text          |                                                      N                              N     s                    (n)
              |                                                T ‚àÇ rÃÉ (n)
              |                                         t                     t X
              |                              dF   ‚àÇF   X                    X            T ‚àÇRp
              |                                 =    ‚àí    Œª(n)            ‚àí         Œ∫(n)
              |                                                                      p         ,                                            (D.24)
              |                              d¬µ   ‚àÇ¬µ n=0          ‚àÇ¬µ        n=1 p=1
              |                                                                             ‚àÇ¬µ
blank         | 
              | 
text          | which is independent of the state sensitivities. Finally, elimination of the auxiliary variables, rÃÉ (n)
              |       (n)
              | and Ri , in equations (D.23) and (D.24) through differentiation of their expressions in (D.18) gives
              | rise to the adjoint equations
blank         | 
text          |                            ‚àÇF T
              |              Œª(Nt ) =
              |                           ‚àÇu(Nt )
              |                                                           s
              |                                            ‚àÇF T X          ‚àÇr  (n)                      T
              |                                                                                              (n)
              |             Œª(n‚àí1) = Œª(n) +                      +     ‚àÜtn     u i  , ¬µ, t n‚àí1 + ci ‚àÜt n    Œ∫i                              (D.25)
              |                                          ‚àÇu(n‚àí1)   i=1
              |                                                            ‚àÇu
              |                                      T                   s
              |                 (n)        ‚àÇF                            X                    ‚àÇr  (n)                 T
              |                                                                                                           (n)
              |          M T Œ∫i       =     (n)
              |                                          + bi Œª(n) +           aji ‚àÜtn            uj , ¬µ, tn‚àí1 + cj ‚àÜtn Œ∫j
              |                           ‚àÇki                            j=i
              |                                                                               ‚àÇu
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          248
blank         | 
              | 
              | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s and the expression for gradient reconstruction, independent of
              | state sensitivities,
blank         | 
text          |                                        Nt      s
              |                dF   ‚àÇF        T ‚àÇu0   X       X    (n) T ‚àÇr   (n)
              |                   =    + Œª(0)       +     ‚àÜtn     Œ∫i        (ui , ¬µ, tn‚àí1 + ci ‚àÜtn ),             (D.26)
              |                d¬µ   ‚àÇ¬µ          ‚àÇ¬µ    n=1     i=1
              |                                                          ‚àÇ¬µ
blank         | 
text          | specialized to the case of a DIRK temporal discretization. From inspection of (D.26), it is clear
              |                                        ‚àÇu0                                                         dF
              | that the initial condition sensitivity      is the only sensitivity term required to reconstruct      .
              |                                        ‚àÇ¬µ                                                          d¬µ
              | The presence of this term does not destroy the efficiency of the adjoint method for two reasons:
              |                                         ‚àÇu0 T
              | (a) only matrix-vector products with           are required and (b) the parametrization of the initial
              |                                         ‚àÇ¬µ
              | condition is either known analytically (uniform flow, zero freestream, independent of ¬µ, etc) or is the
              | solution of some nonlinear system of equations (most likely the steady-state equations). In the first
              |            T ‚àÇu0
              | case, Œª(0)       can be computed analytically once Œª(0) is known. The next section details efficient
              |              ‚àÇ¬µ
              |                     T ‚àÇu0
              | computation of Œª(0)        using the adjoint method of the steady-state problem.
              |                       ‚àÇ¬µ
blank         | 
title         | D.2.2      Parametrization of Initial Condition
text          | Suppose the initial condition u0 (¬µ) is defined as the solution of the nonlinear system of equations‚Äî
              | whose Jacobian is invertible at u0 (¬µ)‚Äîwhich is most likely the fully discrete steady-state form of
              | the governing equations
              |                                                R(u0 (¬µ), ¬µ) = 0.                                  (D.27)
blank         | 
text          | Differentiating with respect to the parameter ¬µ leads to the expansion
blank         | 
text          |                                        dR   ‚àÇR   ‚àÇR ‚àÇu0
              |                                           =    +        = 0,                                      (D.28)
              |                                        d¬µ   ‚àÇ¬µ   ‚àÇu0 ‚àÇ¬µ
blank         | 
text          | where arguments have been dropped for brevity. Assuming the Jacobian matrix is invertible, mul-
              | tiply the preceding equation by the Œª(0) and rearrange to obtain
              |                                                     "           #T
              |                                       (0) T   ‚àÇu0     ‚àÇR ‚àíT (0)    ‚àÇR
              |                                  ‚àíŒª               =        Œª          .                           (D.29)
              |                                               ‚àÇ¬µ      ‚àÇu0          ‚àÇ¬µ
blank         | 
text          |                             ‚àÇu0
              |                              T
              | This reveals the term Œª(0)       can be computed at the cost of one linear system solve of the form
              |                             ‚àÇ¬µ
              |      T
              | ‚àÇR                                       ‚àÇR
              |        v = Œª(0) and an inner product v T     . The only operation whose cost scales with the size
              | ‚àÇu0                                      ‚àÇ¬µ
              |                           ‚àÇR
              | of ¬µ is the evaluation of       and subsequent inner product. Given this exposition on the fully
              |                            ‚àÇ¬µ
              |                                                                                              T ‚àÇu0
              | discrete, time-dependent adjoint method and the discrete adjoint method for computing Œª(0)         ,
              |                                                                                                ‚àÇ¬µ
              | a discussion is provided detailing the advantages of the fully discrete framework when computing
              | gradients of output quantities before discussing implementation details in Section D.2.4.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                   249
blank         | 
              | 
              | 
title         | D.2.3      Benefits of Fully Discrete Framework
text          | In the context of optimization, the fully discrete adjoint method is advantageous compared to the
              | continuous or semi-discrete version as it is guaranteed that the resulting derivatives will be consistent
              | with the quantity of interest, F . This emanates from the fact that in the fully discrete setting, the
              | discretization errors are also differentiated. This property is practically relevant as convergence
              | guarantees and convergence rates of many black-box optimizers are heavily dependent on consistent
              | gradients of optimization functionals.
              |    Additionally, when Runge-Kutta schemes are chosen for the temporal discretization, the fully
              | discrete framework is particularly advantageous since the stages are rarely invariant with respect to
              | the direction of time, that is to say,
blank         | 
text          |                        6 ‚àÉi, j ‚àà {1, . . . , s}   such that tn‚àí1 + ci ‚àÜtn = tn ‚àí cj ‚àÜtn ,                 (D.30)
blank         | 
text          | where c is from the Butcher tableau. Temporal invariance of an Runge-Kutta scheme, as defined in
              | (D.30) is significant when computing adjoint variables. During the primal solve, u will be computed
              | at tn for n = 1, . . . , N and its stage values at tn‚àí1 + ci ‚àÜtn for n = 1, . . . , N and i = 1, . . . , s. If the
              | same RK scheme is applied to integrate the semi-discrete adjoint equations backward in time, the
              | primal solution will be required at tn ‚àí ci ‚àÜtn for n = 1, . . . , N and i = 1, . . . , s. Due to condition
              | (D.30), the solution to the primal problem was not computed during the forward solve. Obtaining the
              | primal solution at this time requires interpolation, which complicates the implementation, degrades
              | the accuracy of the computed adjoint variables, and destroys discrete consistency of the computed
              | gradients. This issue does not arise in the fully discrete setting as only terms computed during the
              | primal solve appear in the adjoint equations, by construction.
              |    The next section is devoted to detailing an efficient and modular implementation of the fully
              | discrete adjoint method on deforming domains.
blank         | 
              | 
title         | D.2.4      Implementation
text          | Implementation of the fully discrete adjoint method introduced in Section D.2 relies on the compu-
              | tation of the following terms from the spatial discretization
blank         | 
text          |                                                   ‚àÇr ‚àÇr T ‚àÇr        ‚àÇfh ‚àÇfh
              |                                        M , r,       ,    ,   , fh ,    ,    .                             (D.31)
              |                                                   ‚àÇu ‚àÇu ‚àÇ¬µ          ‚àÇu ‚àÇ¬µ
blank         | 
text          | Here, M is the mass matrix of the semi-discrete conservation law, and r is the spatial residual
              |                          ‚àÇr                    ‚àÇr
              | vector with derivatives      (Jacobian) and       . As in the previous section, fh is the discretization
              |                          ‚àÇu                    ‚àÇ¬µ
              |                                                                              ‚àÇfh        ‚àÇfh
              | of the spatial integral of the output quantity of interest with derivatives       and       . The mass
              |                                                                               ‚àÇu        ‚àÇ¬µ
              | matrix, spatial flux, Jacobian of spatial flux, and output quantity are standard terms required by an
              | implicit solver and will not be considered further. The Jacobian transpose is explicitly mentioned as
              | additional implementational effort is required when performing parallel matrix transposition. The
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                        250
blank         | 
              | 
              | 
text          | derivatives with respect to ¬µ are rarely required outside adjoint method computations and will be
              | considered further in subsequent sections. As indicated in Section D.1.2, all relevant derivatives of
              | the mass matrix are zero since it is independent of time, parameter, and state variable, which is an
              | artifact of the transformation to a fixed reference domain.
              |       The parallel implementation of all semi-discrete quantities in (D.31) is performed using domain
              | decomposition, where each processor contains a subset of the elements in the mesh, including a halo
              | of elements to be communicated with neighbors [154]. Linear systems of the form
blank         | 
text          |                                           ‚àÇr                       ‚àÇr T
              |                                              x=b                        x=b
              |                                           ‚àÇu                       ‚àÇu
blank         | 
text          | are solved in parallel using a GMRES solver with a block Incomplete-LU (ILU) preconditioner.
              |       Given the availability of all terms in (D.31), the solution of the primal problem and integration
              | of the output quantity F is given in Algorithm 19. The solution of the corresponding fully discrete
              | adjoint equation, and reconstruction of the gradient of F , is given in Algorithm 20.
blank         | 
text          | Algorithm 19 Primal Solution: Functional Evaluation
              | Input: Initial condition, u(0) ; parameter configuration, ¬µ
              |                                                     (N )                              (n)
              | Output: Integrated output quantity, F = Fh t , and primal state quantities, u(n) and ki for
              |     n = 1, . . . , Nt and i = 1, . . . , s
              |                     (0)
              |  1: Initialize: Fh = 0
              |  2: for n = 1, . . . , Nt do
              |  3:   for i = 1, . . . , s do
              |                               (n)
              |  4:      Solve (D.14) for ki
blank         |                                                                          
text          |                                              (n)         (n)
              |                                           M ki = ‚àÜtr ui , ¬µ, tn‚àí1 + ci ‚àÜt
blank         | 
text          |               (n)                Pi          (n)
              |       where ui      = u(n‚àí1) +    j=1   aij kj
              |                     (n)
              |  5:       Write ki to disk
              |  6:     end for
              |  7:     Update u according to (D.14)
              |                                                                     s
              |                                                                                (n)
              |                                                                     X
              |                                                  u(n) = u(n‚àí1) +           bi ki
              |                                                                      i=1
blank         | 
text          |  8:     Update Fh according to (2.45)
              |                                                         s                               
              |                                   (n)       (n‚àí1)                   (n)
              |                                                         X
              |                                  Fh     = Fh        +         bi f ui , ¬µ, tn‚àí1 + ci ‚àÜtn
              |                                                         i=1
blank         | 
title         |  9:     Write u(n) to disk
              | 10:   end for
blank         | 
              | 
text          |       A well-documented implementational issue corresponding to the unsteady adjoint method per-
              | tains to storage and I/O demands. The adjoint equations are solved backward in time and require
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                 251
blank         | 
              | 
              | 
              | 
text          | Algorithm 20 Dual Solution: Gradient Evaluation
              |                                                      (n)
              | Input: Primal state quantities, u(n) and ki for n = 1, . . . , Nt and i = 1, . . . , s; initial condition
              |                     ‚àÇu(0)
              |     sensitivity,          ; parameter configuration, ¬µ
              |                      ‚àÇ¬µ
              |                                                        dF                                          (n)
              | Output: Gradient of integrated output quantity,           , and dual state quantities, Œª(n) and Œ∫i for
              |                                                        d¬µ
              |     n = 1, . . . , Nt and i = 1, . . . , s
              |  1: Read primal solution u(Nt ) from disk
              |       (N )        ‚àÇF T
              |  2: Œª t =
              |                ‚àÇu(Nt )
              |  3: Initial gradient of F with partial derivative and initial condition sensitivity
blank         | 
text          |                                                  dF   ‚àÇF        T ‚àÇu0
              |                                                     =    + Œª(0)
              |                                                  d¬µ   ‚àÇ¬µ          ‚àÇ¬µ
              |  4: for n = Nt , . . . , 1 do
              |  5:   Read primal solution u(n‚àí1) from disk
              |  6:   for i = s, . . . , 1 do
              |                                   (n)
              |  7:     Read primal solution ki from disk
              |                               (n)
              |  8:     Solve (D.25) for Œ∫i
              |                                         T                 s
              |                       (n)        ‚àÇF                       X               ‚àÇr (n)                      (n)
              |                 M T Œ∫i      =               + bi Œª(n) +         aji ‚àÜtn     (u , ¬µ, tn‚àí1 + cj ‚àÜtn )T Œ∫j
              |                                   (n)
              |                                 ‚àÇki                       j=i
              |                                                                           ‚àÇu j
blank         | 
text          |                   dF
              |  9:      Update      according to (D.26)
              |                   d¬µ
              |                                 dF   dF        (n) T ‚àÇr   (n)
              |                                    =    + ‚àÜtn Œ∫i        (u , ¬µ, tn‚àí1 + ci ‚àÜtn )
              |                                 d¬µ   d¬µ              ‚àÇ¬µ i
              | 10:    end for
              | 11:    Update Œª according to (D.25)
              |                                                                 s
              |                                               ‚àÇF T X          ‚àÇr (n)                      (n)
              |                    Œª(n‚àí1) = Œª(n) +                  +     ‚àÜtn    (ui , ¬µ, ti + ci ‚àÜtn )T Œ∫i
              |                                             ‚àÇu(n‚àí1)   i=1
              |                                                               ‚àÇu
blank         | 
meta          | 12:   end for
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          252
blank         | 
              | 
              | 
text          | the solution of the primal problem at each of the corresponding steps/stages. Therefore, the adjoint
              | computations cannot begin until all primal states have been computed. Additionally, this implies
              | all primal states must be stored since they will be required in reverse order during the adjoint
              | computation. For most problems, storing all primal states in memory will be infeasible, requiring
              | disk I/O, which must be performed in parallel to ensure parallel scaling is maintained. There have
              | been a number of strategies to minimize the required I/O operations, such as local-in-time adjoint
              | strategies [206] and checkpointing [40, 90, 95]. For the DG-ALE method in this work, the cost of I/O
              | was not significant compared to the cost of assembly and solving the linearized system of equations.
              |    In this work, the 3DG software [150] was used for the high-order DG-ALE scheme. The tem-
              | poral discretization and unsteady adjoint method were implemented in the Model Order Reduction
              | Testbed (MORTestbed) [209, 210] code-base, which was used to wrap 3DG such that all data struc-
              | tures, and thus all parallel capabilities, were inherited.
blank         | 
title         | Partial Derivatives of Residuals and Output Quantities
blank         | 
text          | This section details computation of partial derivatives of the residual, r, and the output quantity, fh ,
              | with respect to the parameter ¬µ. The DG-ALE discretizations of Section D.1.2, with and without
              | GCL augmentation, are considered separately as the implicit dependence of gÃÑ on ¬µ requires special
              | treatment.
blank         | 
title         | Without GCL Augmentation
text          | When the GCL augmentation is not considered, the dependence of r and fh on the parameter
              | ¬µ is solely due to the domain parametrization. Therefore, the following expansion of the partial
              | derivatives with respect to ¬µ is exploited
blank         | 
text          |                       ‚àÇr   ‚àÇr ‚àÇx   ‚àÇr ‚àÇ xÃá              ‚àÇfh   ‚àÇfh ‚àÇx ‚àÇfh ‚àÇ xÃá
              |                          =       +                          =       +                             (D.32)
              |                       ‚àÇ¬µ   ‚àÇx ‚àÇ¬µ ‚àÇ xÃá ‚àÇ¬µ                ‚àÇ¬µ    ‚àÇx ‚àÇ¬µ   ‚àÇ xÃá ‚àÇ¬µ
blank         | 
text          |         ‚àÇx     ‚àÇ xÃá
              | where      and      are determined solely from the domain parametrization and the terms
              |         ‚àÇ¬µ     ‚àÇ¬µ
blank         | 
text          |                                            ‚àÇr ‚àÇr ‚àÇfh ‚àÇfh
              |                                              ,    ,  ,                                            (D.33)
              |                                            ‚àÇx ‚àÇ xÃá ‚àÇx ‚àÇ xÃá
blank         | 
text          | are determined from the form of the governing equations and spatial discretization outlined in
              | Section D.1. From the expressions in (D.32), the terms in (D.33) are not explicitly required in
              |                                                 ‚àÇx     ‚àÇ xÃá
              | matrix form, rather matrix-vector products with    and      from Section D.2.4 are required.
              |                                                 ‚àÇ¬µ     ‚àÇ¬µ
blank         | 
text          | With GCL Augmentation
              | For the DG-ALE scheme with GCL augmentation, the dependence of r and f on the parameter
              | ¬µ arises from two sources, the domain parametrization and the implicit dependence of gÃÑ on ¬µ.
              | Therefore, the chain rule expansions in (D.32) must include an additional term to account for the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                           253
blank         | 
              | 
              | 
text          | dependence of gÃÑ on ¬µ
blank         | 
text          |          ‚àÇr   ‚àÇr ‚àÇx   ‚àÇr ‚àÇ xÃá   ‚àÇr ‚àÇ gÃÑ                        ‚àÇf   ‚àÇf ‚àÇx   ‚àÇf ‚àÇ xÃá ‚àÇf ‚àÇ gÃÑ
              |             =       +         +                                   =       +        +        .      (D.34)
              |          ‚àÇ¬µ   ‚àÇx ‚àÇ¬µ ‚àÇ xÃá ‚àÇ¬µ ‚àÇ gÃÑ ‚àÇ¬µ                            ‚àÇ¬µ   ‚àÇx ‚àÇ¬µ ‚àÇ xÃá ‚àÇ¬µ ‚àÇ gÃÑ ‚àÇ¬µ
blank         | 
text          |                                                        ‚àÇx     ‚àÇ xÃá
              | Similar to the previous section, the terms                and      are determined solely from the domain
              |                                                        ‚àÇ¬µ     ‚àÇ¬µ
              | parametrization and
              |                                         ‚àÇr ‚àÇr ‚àÇr ‚àÇf ‚àÇf ‚àÇf
              |                                           ,    ,    ,  ,    ,                                      (D.35)
              |                                         ‚àÇx ‚àÇ xÃá ‚àÇ gÃÑ ‚àÇx ‚àÇ xÃá ‚àÇ gÃÑ
              | are determined from the form of the governing equations and spatial discretization in Section D.1.
              |                          ‚àÇ gÃÑ
              | The only remaining term       is defined as the solution of the following ODE
              |                          ‚àÇ¬µ
blank         |                                                   
text          |                                    ‚àÇ        ‚àÇ gÃÑ           ‚àÇrgÃÑ   ‚àÇrgÃÑ ‚àÇ gÃÑ   ‚àÇrgÃÑ
              |                                MgÃÑ                     =        +           =      ,               (D.36)
              |                                    ‚àÇt       ‚àÇ¬µ             ‚àÇ¬µ     ‚àÇ gÃÑ ‚àÇ¬µ     ‚àÇ¬µ
blank         | 
text          | obtained by direct differentiation of (D.11). The last equality uses the fact that rgÃÑ is independent of
              | gÃÑ, which can be deduced from examination of the governing equation for gÃÑ (D.5). Equation (D.36)
              | is discretized with the same DIRK scheme used for the temporal discretization of the state equation.
blank         | 
text          | Remark. The special treatment of gÃÑ detailed in this section, including integration of the sensitivity
              | equations (D.36), can be avoided by considering the ODEs in (D.11) directly without leveraging the
              | fact that the gÃÑ equation is independent of uXÃÑ . This implies the state vector will contain an additional
              | unknown for gÃÑ for each DG node. This increases the cost of a primal and dual solve, but simplifies
              | the adjoint derivation and implementation.
blank         | 
title         | Time-Dependent, Parametrized Domain Deformation
blank         | 
text          | A crucial component of the fully discrete adjoint method on deforming domains is a time-dependent
              | parametrization of the domain, amenable to parallel implementation. A parallel implementation
              | is required as domain deformation will involve operations on the entire computational mesh and
              | will be queried at every stage of each time step of both the primal and dual solves, according to
              | Algorithms 19 and 20. In this work, the domain parametrization is required to be sufficiently general
              | to handle shape deformation, as well as kinematic motion. Additionally, the domain deformation
              | must be sufficiently smooth to ensure sufficient regularity of the transformed solution, and the
              | spatial and temporal derivatives must be analytically available for fast, accurate computation of the
              | deformation gradient, G, and velocity, vX , of the mapping, G.
              |    The domain deformation will be defined by the superposition of a rigid body motion and a
              | spatially varying deformation. To avoid large mesh velocities at the far-field, which could arise from
              | rigid rotations of the body, the blending maps of [152] are used. First, define a spatial configuration
              | consisting of a rigid body motion (Q(¬µ, t), v(¬µ, t)) and deformation (œï(X, ¬µ, t)) to the reference
              | domain
              |                                 X 0 = Q(¬µ, t)X + v(¬µ, t) + œï(X, ¬µ, t),                             (D.37)
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                       254
blank         | 
              | 
              | 
text          | which completely defines the physical motion of the body. This physical configuration is blended
              | with the reference configuration according to
blank         | 
text          |                                  x = (1 ‚àí b(d(X)))X 0 + b(d(X))X                               (D.38)
blank         | 
text          | where d(X) = kX ‚àí X0 k ‚àí R0 is the signed distance from the origin X0 to the circle of radius R0
              | centered at X0 and                       Ô£±
              |                                          Ô£¥
              |                                          Ô£¥
              |                                          Ô£¥ 0,      if s < 0
              |                                          Ô£≤
              |                                    b(s) = 1,       if s > R1                                   (D.39)
              |                                          Ô£¥
              |                                          Ô£¥
              |                                          Ô£¥
              |                                          Ô£≥r(s/R ), otherwise
              |                                                1
blank         | 
              | 
text          | where r(s) = 3s2 ‚àí 2x3 for a cubic blending and r(s) = 10s3 ‚àí 15s4 + 6s5 for a quintic blending.
              | Spatial blending of this form ensures the desired physical motion of the body, X 0 is exactly achieved
              | within a radius R0 of the origin. Further, there is no deformation outside a radius R0 + R1 of the
              | origin. In the annulus about the origin with inner radius R0 and outer radius R0 + R1 , the spatial
              | configuration is blended smoothly between these two spatial configurations.
              |    The specific form of Q(¬µ, t), v(¬µ, t), and œï(X, ¬µ, t) is problem-specific and will be deferred to
              | Sections D.2.5, D.2.6, D.4.4, D.4.5. Assuming these terms are known analytically, the specific form
              |         ‚àÇx               ‚àÇx ‚àÇx         ‚àÇ xÃá
              | of G =      , vX = xÃá =     ,    , and      can be easily computed.
              |         ‚àÇX               ‚àÇt ‚àÇ¬µ         ‚àÇ¬µ
              |    In the next two sections, the high-order numerical discretization of a system of conservation laws
              | and corresponding adjoint method is applied to the isentropic compressible Navier-Stokes equa-
              | tions (2.24)-(2.25) to solve optimal control and shape optimization problems using gradient-based
              | optimization techniques. The DG-ALE scheme introduced in Section D.1 is used for the spatial dis-
              | cretization of the system of conservation laws with polynomial order p = 3 and a diagonally implicit
              | Runge-Kutta scheme for the temporal discretization. The DG-ALE scheme uses the Roe flux [169]
              | for the inviscid numerical flux and the Compact DG flux [150] for the viscous numerical flux. The
              | Butcher tableau for the three-stage, third-order DIRK scheme considered in this work is given in
              | Table D.1. The instantaneous quantities of interest for a body, defined by the surface Œì, take the
blank         | 
text          |            Table D.1: Butcher Tableau for 3-stage, 3rd order DIRK scheme [3]
              |                                            2                2
              |            Œ± = 0.435866521508459, Œ≥ = ‚àí 6Œ± ‚àí16Œ±+1
              |                                               4    , œâ = 6Œ± ‚àí20Œ±+5
              |                                                               4    .
blank         | 
text          |                                       Œ±          Œ±
              |                                        1+Œ±    1+Œ±
              |                                         2      2  ‚àíŒ±     Œ±
              |                                       1          Œ≥       œâ   Œ±
              |                                                  Œ≥       œâ   Œ±
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                           255
blank         | 
              | 
              | 
text          | following form
              |                          Z                                                        Z
              |    Fx (U , ¬µ, t) =            f (U , ¬µ, t) ¬∑ e1 dS              Fy (U , ¬µ, t) =        f (U , ¬µ, t) ¬∑ e2 dS
              |                          ZŒì                                                       ZŒì
              |     P(U , ¬µ, t) =             f (U , ¬µ, t) ¬∑ xÃá dS              Px (U , ¬µ, t) =   xÃáf (U , ¬µ, t) ¬∑ e1 dS                           (D.40)
              |                          ZŒì                                                      Z Œì
blank         | 
text          |    Py (U , ¬µ, t) =            yÃáf (U , ¬µ, t) ¬∑ e2 dS            PŒ∏ (U , ¬µ, t) = ‚àí Œ∏Ãáf (U , ¬µ, t) √ó (x ‚àí x0 ) dS
              |                           Œì                                                            Œì
blank         | 
text          | where f ‚àà Rnsd is the force imparted by the fluid on the body, ei is the ith canonical basis vector
              | in Rnsd , x and xÃá are the position and velocity of a point on the surface Œì, and x, y, Œ∏, xÃá, yÃá, Œ∏Ãá define
              | the motion of the reference point, x0 (the 1/3-chord of the airfoil, in this case); see Figure D.2. The
              | Fx and Fy terms correspond to the total x- and y-directed forces on the body and P is the total
              | power exerted on the body by the fluid. The total power P is broken into its translational, Px and
              | Py , and rotational, PŒ∏ , components. For a 2D rigid body motion, an additive relationship among
              | these terms holds
              |                                        P(U , ¬µ, t) = Px (U , ¬µ, t) + Py (U , ¬µ, t) + PŒ∏ (U , ¬µ, t).                                (D.41)
blank         | 
text          | The negative sign is included in the definition of PŒ∏ due to the clockwise definition of Œ∏ in Fig-
              | ure D.2. In the remainder of this document, a superscript h will be used to denote the high-order
              | DG approximation to these spatial integrals that constitute the instantaneous quantities of interest,
              | e.g., P h (u, ¬µ, t) is the high-order approximation of P(U , ¬µ, t), where u is the semi-discrete approxi-
              | mation of U . Temporal integration of the instantaneous quantities of interest leads to the integrated
              | quantities of interest
              |                 Z    T   Z                                                            Z    T   Z
              |  Jx (U , ¬µ) =                    f (U , ¬µ, t) ¬∑ e1 dS dt            Jy (U , ¬µ) =                       f (U , ¬µ, t) ¬∑ e2 dS dt
              |                  0           Œì                                                         0           Œì
              |                 Z    T   Z                                                            Z    T   Z
              |  W(U , ¬µ) =                      f (U , ¬µ, t) ¬∑ xÃá dS dt           Wx (U , ¬µ) =                        xÃáf (U , ¬µ, t) ¬∑ e1 dS dt
              |                  0           Œì                                                         0            Œì
              |                 Z    T   Z                                                                Z        T Z
              | Wy (U , ¬µ) =                     yÃáf (U , ¬µ, t) ¬∑ e2 dS dt         WŒ∏ (U , ¬µ) = ‚àí                          Œ∏Ãáf (U , ¬µ, t) √ó (x ‚àí x0 ) dS dt
              |                  0           Œì                                                                 0       Œì
              |                                                                                                                                    (D.42)
              | which will be used as optimization functionals in subsequent sections. The terms Jx and Jy are
              | the x- and y-directed impulse the fluid exerts on the airfoil, respectively, W is the total work done
              | on the airfoil by the fluid, and Wx , Wy , and WŒ∏ are the translational and rotational components
              | of the total work. The fully discrete, high-order approximation of the integrated quantities of
              | interest (DG in space, DIRK in time) will be denoted with the corresponding Roman symbol, e.g.,
              |                                  (n)         (n)
              | W (u(0) , . . . , u(Nt ) , k1 , . . . , ks , ¬µ) is the fully discrete approximation of W(U , ¬µ).
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          256
blank         | 
              | 
              | 
title         | D.2.5     Numerical Experiment: Energetically Optimal Trajectory of 2D
              |           Airfoil in Compressible, Viscous Flow
text          | In this section, the high-order, time-dependent PDE-constrained optimization framework introduced
              | in this document is applied to find the energetically optimal trajectory of a 2D NACA0012 airfoil with
              | chord length l = 1 and zero-thickness trailing edge. The governing equations are the 2D compressible,
              | isentropic Navier-Stokes equations. The mission of the airfoil is to move a distance of ‚àí1.5 units
blank         | 
text          |                                                            l
              |                                                     l/3
blank         | 
text          |                                      Œ∏(t)
blank         | 
text          |                                             x(t)
              |                                                    y(t)
blank         | 
              | 
text          |                                      Figure D.2: Airfoil kinematics
blank         | 
text          | horizontally and 1.5 units vertically in T = 4 units of time, with the restriction that Œ∏(0) = Œ∏(T ) = 0,
              | i.e., the angle of attack at the initial and final time is zero. Additionally, to ensure smoothness of
              | the motion and avoid non-physical transients, xÃá(0) = xÃá(T ) = yÃá(0) = yÃá(T ) = Œ∏Ãá(0) = Œ∏Ãá(T ) = 0 are
              | enforced. The goal is to determine the trajectory x(t), y(t), Œ∏(t) of the airfoil that minimizes the
              | total energy required to complete the mission, i.e.,
blank         | 
text          |                           minimize     W(U , ¬µ)
              |                             U, ¬µ
blank         | 
text          |                           subject to x(0) = xÃá(0) = xÃá(T ) = 0, x(T ) = ‚àí1.5
              |                                        y(0) = yÃá(0) = yÃá(T ) = 0, y(T ) = 1.5                     (D.43)
              |                                        Œ∏(0) = Œ∏(T ) = Œ∏Ãá(0) = Œ∏Ãá(T ) = 0
              |                                        ‚àÇU
              |                                           + ‚àá ¬∑ F (U , ‚àáU ) = 0       in v(¬µ, t).
              |                                        ‚àÇt
blank         | 
text          | The trajectory of the airfoil‚Äîx(t), y(t), and Œ∏(t)‚Äîis discretized via clamped cubic splines with
              | mx + 1, my + 1, and mŒ∏ + 1 knots, respectively. The knots are uniformly spaced between 0 and
              | T in the t-dimension and the knot values are optimization parameters. Table D.2 summarizes
              | two parametrizations considered in this section: (PI) the translational degrees of freedom‚Äîx(t)
              | and y(t))‚Äîare frozen at their nominal value in Figure D.4 and the rotational degree of freedom‚Äî
              | Œ∏(t)‚Äîis parametrized with a mŒ∏ + 1-knot clamped cubic spline and (PII) all rigid body modes are
              | parametrized with clamped cubic splines. The 7 IDs in Table D.2 correspond to levels of refinement
              | of the given parametrization with ID = 1 being the coarsest parametrization and ID = 7 the finest.
              | With this parametrization of the airfoil kinematics, spatial and temporal discretization with the
              | high-order scheme of Section D.1 leads to the fully discrete version of the optimization problem in
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                              257
blank         | 
              | 
              | 
text          | Table D.2: Summary of parametrizations considered in Section D.2.5. The number of clamped cubic
              | spline knots used to discretize x(t), y(t), and Œ∏(t) are mx + 1, my + 1, and mŒ∏ , respectively. PI
              | freezes the rigid body translation (mx = my = 0) and optimizes over only the rotation (mŒ∏ 6= 0).
              | PII optimizes over all rigid body degrees of freedom (mx = my = mŒ∏ 6= 0).
blank         | 
text          |                                                 PI                             PII
              |                           ID    mx        my         mŒ∏   N¬µ     mx       my          mŒ∏    N¬µ
              |                           1      0         0      2        3       2       2           2     9
              |                           2      0         0      6        7       6       6           6     21
              |                           3      0         0     10       11      10      10          10     33
              |                           4      0         0     15       16      15      15          15     48
              |                           5      0         0     25       26      25      25          25     78
              |                           6      0         0     50       51      50      50          50    153
              |                           7      0         0     100      101    100      100         100   303
blank         | 
              | 
text          | (D.43)
              |                                                                              (1)
              |                           minimize              W (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ)
              |                     (0)        (Nt )
              |                    u , ..., u          ‚ààRNu ,
              |                     (1)
              |                    k1 , ..., ks(Nt ) ‚ààRNu ,
              |                           ¬µ‚ààRN¬µ
blank         | 
text          |                    subject to                   x(0) = xÃá(0) = xÃá(T ) = 0, x(T ) = ‚àí1.5
              |                                                 y(0) = yÃá(0) = yÃá(T ) = 0, y(T ) = 1.5
              |                                                 Œ∏(0) = Œ∏(T ) = Œ∏Ãá(0) = Œ∏Ãá(T ) = 0                      (D.44)
blank         | 
text          |                                                 u(0) = u0
              |                                                                     s
              |                                                                                 (n)
              |                                                                     X
              |                                                 u(n) = u(n‚àí1) +           bi ki
              |                                                                     i=1
blank         |                                                                                       
text          |                                                    (n)             (n)
              |                                                 M ki      = ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn .
blank         | 
text          |    Before considering the optimization problem (D.44), the proposed adjoint method for comput-
              | ing gradients of quantities of interest on the manifold of fully discrete, high-order solutions of the
              | conservation law (D.14) is verified against a fourth-order finite difference approximation. The finite
              | difference approximation to gradients on the aforementioned manifold requires finding the solution
              | of the fully-discretized governing equations at perturbations about the nominal parameter config-
              | uration in Figure D.4. To mitigate round-offs errors as much as possible in the finite difference
              | computation, the number of time steps was reduced to 10 and only half of a period was simulated.
              | Figure D.3 shows the relative error between the gradients computed via the adjoint method and this
              | finite difference approximation for a sweep of finite difference intervals, œÑ . A relative error on the
              | order of 10‚àí10 is observed for a finite difference step of œÑ = 10‚àí4 . As expected, the error starts to
              | increase after œÑ drops too small due to the trade-off between finite difference accuracy and roundoff
              | error.
              |    With this verification of the adjoint-based gradients, attention is turned to the optimization prob-
              | lem in (D.44). The optimization solver used in this section is L-BFGS-B [215], a bound-constrained,
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                    258
blank         | 
              | 
              | 
text          |                            10‚àí5
blank         | 
              | 
              | 
              | 
text          |                   ‚àÜW
              |                   ‚àÜ¬µ
              |                            10‚àí7
blank         | 
              | 
              | 
              | 
text          |                   /
              |                   ‚àÜW
              |                   ‚àÜ¬µ
              |                            10‚àí9
blank         | 
text          |                   ‚àí
              |                   dW
              |                   d¬µ
              |                        10‚àí11
              |                                      10‚àí9           10‚àí7              10‚àí5           10‚àí3           10‚àí1
              |                                                                        œÑ
blank         | 
text          | Figure D.3: Verification of adjoint-based gradient with fourth-order centered finite difference approx-
              | imation, for a range of finite intervals, œÑ , for the total work W ‚Äîthe objective function in (D.44)‚Äîfor
              | parametrization PII (Table D.2). The computed gradient match the finite difference approximation
              | to about 10 digits of accuracy before round-off errors degrade the accuracy.
blank         | 
              | 
text          | limited-memory BFGS algorithm. Figure D.4 contains the initial guess for the optimization prob-
              | lem in (D.44) as well as its solution under both parametrization, PI and PII, at the finest level of
              | refinement (ID = 7). The initial guess for the optimization problem is a pure translational motion
              | with Œ∏(t) = 0. The solution under parametrization PI freezes the translational motion at its nominal
              | value and incorporates rotational motion. The solution under parametrization PII increases the am-
              | plitude of the rotation, flattens the trajectory of x(t), and incorporates an overshoot in y(t) before
              | settling to the required location, as compared to the optimal solution corresponding to PI.
blank         | 
text          |           0                                                            1.5
blank         | 
text          |        ‚àí0.5                                                             1
              | x(t)
blank         | 
              | 
              | 
              | 
text          |                                                                y(t)
blank         | 
              | 
              | 
              | 
text          |         ‚àí1                                                             0.5
blank         | 
text          |        ‚àí1.5                                                             0
              |               0        1         2          3         4                      0              1         2    3   4
              |                                time                                                                 time
blank         | 
text          |                                         0
              |                               Œ∏(t)
blank         | 
              | 
              | 
              | 
text          |                                      ‚àí0.5
blank         | 
              | 
              | 
text          |                                                 0          1            2        3              4
              |                                                                       time
blank         | 
              | 
text          | Figure D.4: Trajectories of x(t), y(t), and Œ∏(t) at initial guess (   ), solution of (D.44) under
              | parametrization PI (    ), and solution of (D.44) under parametrization PII (    ) for ID = 7.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          259
blank         | 
              | 
              | 
text          |    The instantaneous quantities of interest for the nominal trajectory and solution of (D.44) under
              | parametrizations PI and PII are included in Figure D.5. It is clear that the optimal solution under
              | both parametrizations result in a time history of the total power that is uniformly closer to 0 than
              | that at the nominal trajectory, which is expected since W is the objective function. With the
              | exception of the edges of the time interval, the total power time history for the optimal solution
              | under parametrization PII is uniformly closer to 0 than that of PI. The same observation holds for
              | the power due to the translational motion, Pxh and Pyh . Whereas the total power corresponding
              | to the nominal trajectory is due solely to the translational motion (since there is no rotation), the
              | optimal solutions exchange large amounts of translational power for a small amount of rotational
              | power. These observations can also be verified in Table D.3 which summarizes the optimal values
              | of the integrated quantities of interest.
blank         | 
text          |        0.1
              |                                                                    1
              |          0
              |  Fxh
blank         | 
              | 
              | 
              | 
text          |                                                             Fyh
              |                                                                    0
              |       ‚àí0.1
              |                                                                   ‚àí1
blank         | 
              | 
              | 
              | 
text          |          0                                                      0.05
              |  Ph
blank         | 
              | 
              | 
              | 
text          |                                                          Pxh
blank         | 
              | 
              | 
              | 
text          |       ‚àí0.2
              |                                                                    0
              |       ‚àí0.4
              |                                                                ‚àí0.05
blank         | 
              | 
              | 
text          |          0                                                      0.05
blank         | 
text          |       ‚àí0.2                                                         0
              | Pyh
blank         | 
              | 
              | 
              | 
text          |                                                          PŒ∏h
blank         | 
              | 
              | 
              | 
text          |                                                                ‚àí0.05
              |       ‚àí0.4
              |                                                                ‚àí0.1
              |               0       1        2       3        4                      0       1        2       3           4
              |                              time                                                     time
blank         | 
              | 
text          | Figure D.5: Time history of instantaneous quantities of interest (x-directed force ‚Äì Fxh (u, ¬µ, t),
              | y-directed force ‚Äì Fyh (u, ¬µ, t), total power ‚Äì P h (u, ¬µ, t), x-translational power ‚Äì Pxh (u, ¬µ, t), y-
              | translational power ‚Äì Pyh (u, ¬µ, t), rotational power ‚Äì PŒ∏h (u, ¬µ, t)) at initial guess ( ), solution of
              | (D.44) under parametrization PI (        ), and solution of (D.44) under parametrization PII (      ) for
              | ID = 7.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                       260
blank         | 
              | 
              | 
text          |     The convergence of the total work, i.e., the objective function of the optimization problem, with
              | iterations of the optimization solver is summarized in Figure D.6 (left). Both parametrizations are
              | included and iterations are agglomerated over all IDs. The first iteration corresponds to a steepest
              | descent step, which causes an adverse jump in the objective value. The following iterations make
              | rapid progress toward the optimal solution, which is slowed as convergence is approached. The solver
              | requires additional iterations to converge the solution corresponding to parametrization PII, which
              | is expected due to the larger parameter space.
              |     Next, convergence of the total work as the parameter space is refined is considered in Figure D.6
              | (right) and Table D.3. This implies the optimal trajectory among all twice continuously differentiable
              | functions is being approached. For both parametrizations, the optimal value of the total work agrees
              | to 3 digits between IDs 6 and 7 (roughly a factor of 2 difference in dimension of parameter spaces)
              | and 2 digits between IDs 3 and 7 (roughly a factor of 10 difference in dimension of parameter spaces).
blank         | 
text          |         0
blank         | 
text          |     ‚àí0.5
              |                                                            ‚àí0.15
              | W
blank         | 
              | 
              | 
              | 
text          |                                                        W
              |       ‚àí1
              |                                                            ‚àí0.2
              |     ‚àí1.5
              |             0       20       40      60       80                           101              102
              |                          iteration                                                 N¬µ
blank         | 
              | 
text          | Figure D.6: Left: Convergence of total work W with optimization iteration for parametrization PI
              | (    ) and PII (   ) for ID = 7. Both optimization problems converge to a motion with significantly
              | lower required total work; PII finds a better motion than PI (in terms of total work) due to the
              | enlarged search space, at the cost of additional iterations. Each optimization iteration requires a
              | primal flow computation‚Äîto evaluate the quantities of interest‚Äîand its corresponding adjoint‚Äî
              | to evaluate the gradient of the quantity of interest. Right: Convergence of optimal value of total
              | work W as parameter space is refined for parametrization PI (       ) and PII (     ). This implies
              | convergence to an optimal, smooth trajectory that is not polluted by its discrete parametrization.
blank         | 
text          |     The motion of the airfoil and vorticity of the surrounding flow are shown in Figure D.7 (nominal
              | trajectory), Figure D.8 (optimal solution under parametrization PI), and Figure D.9 (optimal solu-
              | tion under parametrization PII). The flow corresponding to the nominal configuration experiences
              | flow separation and vortex shedding, which results in the relatively large amount of total energy to
              | complete the mission. Fixing the translational motion and optimizing over the rotation (PI) dra-
              | matically reduces the amount of shedding and consequently reduces the amount of work required.
              | Optimizing the entire rigid body motion (PII) further reduces the shedding and required work.
              | Table D.3: Table summarizing integrated quantities of interest at optimal solution of (D.44) for each parametrization (PI, PII) for each level
              | of refinement. The total work monotonically increases as N¬µ increases for a given parametrization, which is expected due to the nested search
              | spaces. For a fixed ID, the optimal total work for parametrization PII is larger than that for PI since the search space for PI is a subset of
              | that of PII. The other integrated quantities are included for completeness, but do not exhibit trends (except for converging to a fixed value
              | as N¬µ increases) since they were not included in the optimization problem.
blank         | 
text          |                                                                         PI
              |                    ID         1             2              3             4             5             6             7
              |                    W     -2.1951e-01   -1.5881e-01   -1.5358e-01    -1.5128e-01   -1.5026e-01   -1.4950e-01   -1.4924e-01
              |                    Wx    8.1329e-02     5.6090e-02   4.9543e-02     4.5924e-02    4.5085e-02    4.4712e-02    4.4707e-02
              |                    Wy    -2.3460e-01   -1.8153e-01   -1.7122e-01    -1.6544e-01   -1.6374e-01   -1.6298e-01   -1.6294e-01
              |                    WŒ∏    6.6234e-02     3.3370e-02   3.1906e-02     3.1768e-02    3.1604e-02    3.1223e-02    3.1010e-02
              |                    Fx    -1.9234e-01   -1.3123e-01   -1.1886e-01    -1.1136e-01   -1.0912e-01   -1.0810e-01   -1.0800e-01
              |                    Fy    -5.1539e-01   -3.1711e-01   -3.1816e-01    -3.1877e-01   -3.2551e-01   -3.2959e-01   -3.3063e-01
              |                                                                         PII
              |                    ID         1             2              3             4             5             6             7
              |                    W     -1.7357e-01   -1.2095e-01   -1.1733e-01    -1.1629e-01   -1.1603e-01   -1.1557e-01   -1.1502e-01
              |                    Wx    9.6487e-03    -1.4123e-02   -1.4328e-02    -1.4967e-02   -1.5021e-02   -1.5061e-02   -1.5027e-02
              |                    Wy    -1.1041e-01   -6.2238e-02   -6.1036e-02    -6.0425e-02   -6.0032e-02   -5.9489e-02   -5.9245e-02
              |                    WŒ∏    7.2807e-02     4.4585e-02   4.1963e-02     4.0895e-02    4.0980e-02    4.1023e-02    4.0749e-02
              |                    Fx    -4.1265e-02    2.8091e-02   2.7677e-02     2.9596e-02    2.9848e-02    3.0231e-02    3.0221e-02
              |                    Fy    -3.2231e-01    -1.064e-01   -1.0806e-01    -1.0890e-01   -1.1343e-01   -1.1626e-01   -1.1764e-01
meta          |                                                                                                                                                  APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION
              |                                                                                                                                                  261
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                     262
blank         | 
              | 
              | 
              | 
text          | Figure D.7: Flow vorticity around airfoil undergoing motion corresponding to initial guess for opti-
              | mization, i.e., pure heaving (  ). Flow separation off leading edge implies a large amount of work
              | required to complete mission. Snapshots taken at times t = 0.0, 0.8, 1.6, 2.4, 3.2, 4.0.
blank         | 
              | 
              | 
              | 
text          | Figure D.8: Flow vorticity around airfoil undergoing motion corresponding to optimal pitching
              | motion for fixed translational motion, i.e., solution of (D.44) under parametrization PI ( ). The
              | pitching motion greatly reduces the degree of flow separation and vortex shedding compared to
              | the initial guess, and requires less work to complete the mission. Snapshots taken at times t =
meta          | 0.0, 0.8, 1.6, 2.4, 3.2, 4.0.
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          263
blank         | 
              | 
              | 
              | 
text          | Figure D.9: Flow vorticity around airfoil undergoing motion corresponding to optimal rigid body mo-
              | tion, i.e., solution of (D.44) under parametrization PII (    ). This rigid body motion further reduces
              | the degree of flow separation and required work to complete the mission. This motion differs from
              | the solution of PI as it has a larger pitch amplitude and slightly overshoots the final vertical position
              | before settling to the required position. Snapshots taken at times t = 0.0, 0.8, 1.6, 2.4, 3.2, 4.0.
blank         | 
              | 
title         | D.2.6     Numerical Experiment: Energetically Optimal Shape and Flapping
text          |           Motion of 2D Airfoil at Constant Impulse
              | In this section, the high-order, time-dependent PDE-constrained optimization framework introduced
              | in this document is applied to find the energetically optimal flapping motion, under an impulse
              | constraint, of a 2D NACA0012 airfoil (Figure D.10) with chord length l = 1 and zero-thickness
              | trailing edge. The governing equations are the 2D compressible, isentropic Navier-Stokes equations.
blank         | 
              | 
              | 
text          |                                 l
              |                       l/3
blank         | 
text          |           Œ∏(t)                                                              c(t)
blank         | 
              | 
text          |                      y(t)
blank         | 
              | 
text          |                             Figure D.10: Airfoil kinematics and deformation
blank         | 
              | 
text          |    The goal is to determine the flapping motion‚Äîy(t) and Œ∏(t)‚Äîand shape‚Äîc(t)‚Äîof the airfoil
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                       264
blank         | 
              | 
              | 
text          | that minimizes the total energy such than a x-impulse of q is achieved, i.e.,
blank         | 
text          |                             minimize     W(U , ¬µ)
              |                               U, ¬µ
blank         | 
text          |                             subject to Jx (U , ¬µ) = q                                                           (D.45)
              |                                           ‚àÇU
              |                                              + ‚àá ¬∑ F (U , ‚àáU ) = 0                  in v(¬µ, t).
              |                                           ‚àÇt
blank         | 
text          | The flapping frequency is fixed at 0.2, which corresponds to a period of T = 5. Proper initialization
              | of the flow is the initial condition that results in a time-periodic flow [212] to completely avoid non-
              | physical transients and simulate representative, in-flight conditions; this experiment uses a crude
              | approximation that initializes the flow from the steady-state condition, simulates 3 periods of the
              | flapping motion, and integrates the quantities of interest over the last period only. The deformation
              | of the domain is determined from the value of c(t) using the spatial blending map of Section D.2.4
              | with                                                 "                              #
              |                                                                   0
              |                                       œï(X, ¬µ, t) =                              2                               (D.46)
              |                                                          2c(t)e‚àí[(X‚àíx0 )¬∑e1 ]
blank         | 
text          |    The trajectory of the airfoil‚Äîy(t), and Œ∏(t)‚Äîand its shape ‚Äì c(t)‚Äîare discretized via cubic
              | splines with my + 1, mŒ∏ + 1, and mc + 1 knots, respectively, with boundary conditions that enforce
blank         | 
text          |          y(t) = ‚àíy(t + T /2)              Œ∏(t) = ‚àíŒ∏(t + T /2)                   c(t) = ‚àíc(t + T /2).            (D.47)
blank         | 
text          | These boundary conditions1 for y(t), Œ∏(t), and c(t) correspond to a mirroring of the trajectory at
              | t = T /2 and implicitly enforces periodicity with period T . The knots are uniformly spaced between
              | 0 and T in the t-dimension and the knot values are optimization parameters. Since the unsteady
              | simulation is initialized from the steady-state flow, non-zero velocities of the airfoil at t = 0 will
              | result in non-physical transients. These transients are avoided by blending the periodic cubic spline
              | smoothly to the zero function at the beginning of the time interval [193]. Let sy (t; ¬µ), sŒ∏ (t; ¬µ), and
              | sc (t; ¬µ) denote the periodic cubic spline approximations. Then, the flapping and shape trajectories
              | are defined as
blank         | 
text          |                y(t) = b(t)sy (t; ¬µ)          Œ∏(t) = b(t)sŒ∏ (t; ¬µ)                       c(t) = b(t)sc (t; ¬µ),   (D.48)
blank         | 
text          |                         2
              | where b(t) = 1.0 ‚àí e‚àít . Table D.4 summarizes two parametrizations considered in this section:
              | (FI) rigid body motion parametrized via cubic splines and shape fixed at nominal value and (FII) rigid
              | body motion and shape of airfoil parametrized via cubic splines. With this parametrization of the
              | airfoil kinematics and shape, spatial and temporal discretization with the high-order scheme of
              |   1 Periodic and mirrored cubic splines of this form with m + 1 knots only have m degrees of freedom since the
blank         | 
text          | boundary condition prescribes the value of the m + 1 knot from the values of the others m.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                 265
blank         | 
              | 
              | 
text          | Table D.4: Summary of parametrizations considered in Section D.2.6. The number of periodic cubic
              | spline knots used to discretize y(t), Œ∏(t), and ¬∏(t) are my + 1, mŒ∏ + 1, and mc + 1, respectively.
              | FI freezes the airfoil shape and considers only rigid body motions (my = mŒ∏ 6= 0, mc = 0). FII
              | parametrizes both shape and kinematic motion (my = mŒ∏ = mc 6= 0).
blank         | 
text          |                                               FI                              FII
              |                                my      mŒ∏          mc   N¬µ     my     mŒ∏            mc   N¬µ
              |                                 4         4        0     6      4         4         4    9
blank         | 
              | 
text          | Section D.1 leads to the fully discrete version of the optimization problem in (D.45)
blank         | 
text          |                                                                               (1)
              |                         minimize              W (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ)
              |                   (0)        (Nt )   Nu
              |                  u , ..., u        ‚ààR   ,
              |                   (1)
              |                  k1 , ..., ks(Nt ) ‚ààRNu ,
              |                         ¬µ‚ààRN¬µ
              |                                                                               (1)
              |                  subject to                   Jx (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ) = 0
              |                                               u(0) = u0                                                   (D.49)
              |                                                                     s
              |                                                                               (n)
              |                                                                     X
              |                                               u(n) = u(n‚àí1) +             bi ki
              |                                                                     i=1
blank         |                                                                                     
text          |                                                  (n)             (n)
              |                                               M ki      = ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn .
blank         | 
text          |    Given the gradient verification from the previous section, attention is turned directly to the
              | optimization problem in (D.49) for various values of the impulse constraint, q. The optimization
              | solver used in this section is SNOPT [70], a nonlinearly constrained SQP method. Figure D.11
              | contains the initial guess for the optimization problem in (D.44) as well as its solution under both
              | parametrization, FI and FII. The initial guess for the optimization problem is a pure heaving motion
              | at a fixed shape, i.e., c(t) = Œ∏(t) = 0. The solution under parametrization PI freezes the shape at its
              | nominal configuration (NACA0012) and modifies the rigid body motion. Pitch is introduced for all
              | values of the impulse constraint and the amplitude of the heaving motion is decreased for q = 0.0, 1.0
              | and increased for q = 2.5. The solution under parametrization PII reduces the heaving amplitude
              | and slightly increases the pitch amplitude as compared to PI. It also introduces non-trivial camber.
              |    The instantaneous quantities of interest‚ÄîW and Jx in this case‚Äîfor the nominal motion and
              | shape and solution of (D.49) under parametrizations PI and PII are included in Figure D.12. It
              | is clear that the optimal solution under both parametrizations result in a time history of the total
              | power that is uniformly closer to 0 than that at the nominal trajectory, which is expected since W
              | is the objective function. It is also clear that larger values of the impulse constraint require more
              | power to complete the flapping motion. While it may not be clear from Figure D.12, the integration
              | of Fxh leads to an impulse that exactly conforms to the specified value of q. This can be seen more
              | clearly in Figure D.13. These observations can also be verified in Figure D.13 and Table D.5 that
              | summarizes the optimal values of the integrated quantities of interest.
              |    Figure D.13 shows the convergence of the integrated quantities of interest with iterations in
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                   266
blank         | 
              | 
              | 
              | 
text          |                                                                        1
              |         1
              |                                                                       0.5
              | y(t)
blank         | 
              | 
              | 
              | 
text          |                                                            Œ∏(t)
              |         0                                                              0
blank         | 
text          |                                                                   ‚àí0.5
              |        ‚àí1
              |             10   11    12          13       14        15              ‚àí1
              |                                                                         10         11         12          13        14         15
              |                               t                                                                     t
blank         | 
text          |                                     0.4
blank         | 
text          |                                     0.2
              |                             c(t)
blank         | 
              | 
              | 
              | 
text          |                                         0
              |                                    ‚àí0.2
blank         | 
text          |                                    ‚àí0.4
              |                                        10        11        12          13         14         15
              |                                                                   t
blank         | 
              | 
text          | Figure D.11: Trajectories of y(t), Œ∏(t), and c(t) at initial guess (     ), solution of (D.49) under
              | parametrization FI (q = 0.0:       , q = 1.0:      , q = 2.5:       ), and solution of (D.49) under
              | parametrization FII (q = 0.0:     , q = 1.0:    , q = 2.5:     ) from Table D.4.
blank         | 
              | 
              | 
              | 
text          |         0
              |                                                                             0
              |        ‚àí2
              |                                                                 Fxh
              | Ph
blank         | 
              | 
              | 
              | 
text          |                                                                       ‚àí0.5
blank         | 
text          |        ‚àí4
              |                                                                         ‚àí1
              |         10       11   12           13       14        15                     10         11         12          13        14         15
              |                            time                                                                         time
blank         | 
              | 
text          | Figure D.12: Time history of total power, P h (u, ¬µ, t), and x-directed force, Fxh (u, ¬µ, t), imparted
              | onto foil by fluid at initial guess (    ), solution of (D.49) under parametrization FI (q = 0.0:    ,
              | q = 1.0:        , q = 2.5:       ), and solution of (D.49) under parametrization FII (q = 0.0:       ,
              | q = 1.0:      , q = 2.5:      ) from Table D.4.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                           267
blank         | 
              | 
              | 
text          | the optimization solver. The aforementioned observations can be verified by inspection of the final
              | iteration: all impulse constraints are satisfied, larger values of q require more work to achieve,
              | and morphing the shape of the airfoil allows for a slight reduction in the required work. After 20
              | iterations, the impulse constraint is satisfied for q = 0.0, 1.0 and reduction of the work has essentially
              | ceased, implying the optimization could have been terminated at that point. The case with q = 2.5
              | requires an additional 15 - 20 iterations to settle to a converged solution.
blank         | 
text          |        0                                                         1
              |                                                                  0
blank         | 
              | 
              | 
              | 
text          |                                                            Jx
              | W
blank         | 
              | 
              | 
              | 
text          |      ‚àí5                                                         ‚àí1
              |                                                                 ‚àí2
              |     ‚àí10
              |            0         20               40         60                  0         20               40           60
              |                           iteration                                                 iteration
blank         | 
              | 
text          | Figure D.13: Convergence of quantities of interest, W and Jx , with optimization iteration for
              | parametrization FI (q = 0.0:       , q = 1.0:    , q = 2.5:     ) and FII (q = 0.0:     , q = 1.0:  ,
              | q = 2.5:       ) from Table D.4. Each optimization iteration requires the a primal flow computation‚Äî
              | to evaluate quantities of interest‚Äîand its corresponding adjoint‚Äîto evaluate the gradient of quan-
              | tities of interest.
blank         | 
text          |     The shape and motion of the airfoil and vorticity of the surrounding flow are shown in Figure D.14
              | (nominal), Figure D.15 (optimal solution under parametrization FI for q = 2.5), and Figure D.16
              | (optimal solution under parametrization FII for q = 2.5). The flow corresponding to the nominal
              | configuration experiences flow separation and vortex shedding, which results in the relatively large
              | amount of total energy to complete the flapping motion and does not satisfy the impulse constraint.
              | Fixing the shape and optimizing over the heaving and pitching motion (FI) dramatically reduces the
              | amount of shedding and consequently reduces the amount of work required. Optimizing the shape in
              | addition to the pitching and heaving motion (FII) further reduces the shedding and required work.
              | The solution of FI and FII both satisfy the impulse constraint to greater than 8 digits of accuracy.
              |     To conclude this section, a brief comparison of the optimal flapping motions found in this work are
              | compared to those found in the literature. From Figure D.11, the pitch of the foil leads its plunge
              | by approximately 90‚ó¶ in all optimal flapping motions, a result that was found in several works
              | that range from experimental and computational [191, 162, 158, 148]. The improved efficiency is
              | largely due to a dramatic reduction in leading edge vortex shedding characteristic of pure heaving
              | motions (Figure D.14) [191, 158]. The specific pitching and heaving amplitudes were determined by
              | the optimizer such that the thrust constraint is satisfied; as the thrust requirement is increased, the
              | magnitude of the pitch and plunge increase and leading edge shedding off the leading edge is induced
              | (Figure D.15) [148]. The time-dependent shape deformation slightly reduces the magnitude of the
              | vortices shedding off the leading edge, which can be seen by comparing Figures D.15 and D.16.
              | Table D.5: Table summarizing integrated quantities of interest at optimal solution of each optimization problem for each impulse level. In all
              | cases, the desired value of Jx is achieved to greater than 4 digits of accuracy. The optimal solution for larger values of the impulse constraint
              | require more total work to complete flapping motion, i.e., work monotonically increases in magnitude as value of impulse constraint increases.
              | Smaller values of total work are achievable if airfoil is allowed to morph its shape in addition its rigid body motion. The other integrated
              | quantities are included for completeness, but do not exhibit trends since they were not in the optimization problem.
blank         | 
text          |                           Initial                          FI                                           FII
              |                  q                         0.0            1.0            2.5            0.0            1.0             2.5
              |                  W     -9.4096e+00    -4.5695e-01    -2.1419e+00    -4.9476e+00     -4.3252e-01    -2.0271e+00    -4.6110e+00
              |                  Wx    0.0000e+00     0.0000e+00      0.0000e+00    0.0000e+00      0.0000e+00     0.0000e+00      0.0000e+00
              |                  Wy    -9.4096e+00    -4.2807e-01    -2.0642e+00    -4.7967e+00     -3.7363e-01    -1.5413e+00    -3.3712e+00
              |                  WŒ∏    0.0000e+00      2.8883e-02     7.7694e-02     1.5083e-01     1.7101e-02      6.7900e-03     1.8744e-01
              |                  Fx     -1.7660e-01   -4.0490e-11    -1.0000e+00    -2.5000e+00     1.6937e-10     -1.0000e+00    -2.5000e+00
              |                  Fy     3.5413e-01     1.5989e-02     5.0480e-02     9.7240e-02     -1.5292e-02     4.8657e-02     9.6440e-02
meta          |                                                                                                                                                     APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION
              |                                                                                                                                                     268
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                               269
blank         | 
              | 
              | 
              | 
text          | Figure D.14: Flow vorticity around flapping airfoil undergoing motion corresponding to initial
              | guess for optimization problem (D.49), i.e., pure heaving (    ). Flow separation off leading
              | edge implies a large amount of work required for flapping motion. Snapshots taken at times
              | t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                  270
blank         | 
              | 
              | 
              | 
text          | Figure D.15: Flow vorticity around flapping airfoil undergoing optimal rigid body motion corre-
              | sponding to the solution of (D.49) under parametrization FI. The x-directed impulse is Jx = 2.5.
              | The pitching motion greatly reduces the degree of flow separation and vortex shedding compared
              | to the initial guess, and requires less work to complete the flapping motion and generate desired
              | impulse. Snapshots taken at times t = 9.75, 10.8, 11.85, 12.9, 13.95, 15.0.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                271
blank         | 
              | 
              | 
              | 
text          | Figure D.16: Flow vorticity around flapping airfoil undergoing optimal deformation and kine-
              | matic motion, corresponding to the solution of (D.49) under parametrization FII. The x-directed
              | impulse is Jx = 2.5. The morphing further reduces the flow separation and work required
              | to complete the flapping motion and generate desired impulse. Snapshots taken at times t =
meta          | 9.75, 10.8, 11.85, 12.9, 13.95, 15.0.
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                            272
blank         | 
              | 
              | 
title         | D.3        Computing Time-Periodic Solutions of Partial Differen-
              |            tial Equations
text          | This section is devoted to the solution of partial differential equations with time-periodicity con-
              | straints. This will largely be a review of existing work on the topic [131, 196, 7, 8, 201, 77], although
              | emphasis will be placed on equations that are parametrized and fully discretized. This will lead
              | to the main contribution of this work, the fully discrete adjoint equations corresponding to time-
              | periodic solutions of partial differential equations and their use in computing gradients of quantities
              | of interest along the manifold of time-periodic solutions.
              |    Consider the general, nonlinear, time-periodically constrained system of partial differential equa-
              | tions, parametrized by the vector ¬µ ‚àà RN¬µ ,
blank         | 
text          |                                  ‚àÇU
              |                                       = L(U , ¬µ, t)                   in         ‚Ñ¶(¬µ, t) √ó (0, T ]
              |                                  ‚àÇt                                                                  (D.50)
              |                              U (x, 0) = U (x, T ),
blank         | 
text          | where L(¬∑, ¬µ, t) is a spatial differential operator on the parametrized, time-dependent domain ‚Ñ¶(¬µ, t) ‚äÇ
              | Rnsd . The boundary conditions have not been explicitly stated for brevity. This work will only con-
              | sider temporally first-order partial differential equations, or those that have been recast as such.
              | Without loss of generality, consider a quantity of interest of the form
              |                                                  Z     T   Z
              |                                  F(U , ¬µ) =                          f (U , ¬µ, t) dS dt,             (D.51)
              |                                                    0       Œì(¬µ,t)
blank         | 
              | 
text          | where Œì(¬µ, t) ‚äÜ ‚àÇ‚Ñ¶(¬µ, t). The generalization to other types of quantities of interest, such as volu-
              | metric integrals and instantaneous or pointwise quantities of interest, is immediate as the specific
              | form of the quantity of interest will be abstracted away at the fully discrete level. The form in
              | (D.51) will be used in the physical setup of the applications in subsequent sections. In subsequent
              | sections, this quantity of interest will correspond to either the objective function or a constraint of
              | an optimization problem governed by a partial differential equation and subject to a time-periodicity
              | requirement. After space-time discretization of (D.50) via the DG-ALE-DIRK scheme discussed in
              | Section D.1, the fully discrete equations are
              |                                                                s
              |                                                                            (n)
              |                                                                X
              |                                    u(n) = u(n‚àí1) +                   bi ki
              |                                                                i=1                                   (D.52)
blank         |                                                                                            
text          |                                    (n)                      (n)
              |                                 M ki     = ‚àÜtn r           ui ,      ¬µ, tn‚àí1 + ci ‚àÜtn ,
blank         | 
text          |         (n)
              | where ui      is defined in (D.15) and the fully discrete quantity of interest is
blank         | 
text          |                                                                      (1)
              |                                     F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) ).               (D.53)
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          273
blank         | 
              | 
              | 
text          | Time-periodicity may then be expressed as the constraint
blank         | 
text          |                                                 u(0) = u(Nt ) ,                                   (D.54)
blank         | 
text          | where Nt is the time index of the cycle period.
              |    The next section discusses methods for solving the fully discrete, time-periodically constrained
              | partial differential equations. The periodicity constraint, i.e., u(0) = u(Nt ) , turns the problem into
              | a nonlinear two-point boundary value problem, which eliminates the possibility of using traditional
              | evolution methods (since the initial conditions is unknown).
blank         | 
              | 
title         | D.3.1      Numerical Solvers: Shooting Methods
text          | This section provides a brief, non-exhaustive review of methods which have been introduced for
              | solving time-periodic partial differential equations. A distinguishing feature of this work is that
              | we directly consider the fully discrete form of the governing equations, whereas previous work has
              | focused on the continuous [194] or semi-discrete [185] levels. The section will conclude with a
              | discussion of a Newton-Krylov shooting method using a purely matrix-free Krylov solver to solve
              | the linear systems of equations that arise, which extends the work in [77].
              |    Define u(Nt ) (u0 ; ¬µ) as the solution of the following initial-value problem
blank         | 
text          |                                    u(0) = u0
              |                                                       s
              |                                                                  (n)
              |                                                       X
              |                                   u(n) = u(n‚àí1) +            bi ki                                (D.55)
              |                                                        i=1
blank         |                                                                        
text          |                                     (n)               (n)
              |                                M ki       = ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn ,
blank         | 
text          | which can be solved using a traditional evolution algorithm that advances the solution from timestep
              | n to n + 1. Notice that this overloads the notation introduced in Section 2.1.3, which defines u(Nt )
              | as the discrete approximation of the time-periodic solution of the system of partial differential
              | equations at the final time. Here, it is a nonlinear function that maps a state u0 ‚àà RNu to the state
              | u(Nt ) (u0 ; ¬µ). It is clear that u0 is the time-periodic initial condition of the fully discrete partial
              | differential equation if it is a fixed point of u(Nt ) (¬∑; ¬µ), namely
blank         | 
text          |                                              u(Nt ) (u0 ; ¬µ) = u0 .                               (D.56)
blank         | 
text          | Then, provided the mapping u0 ‚Üí u(Nt ) (u0 ; ¬µ) is a contraction mapping, the Banach Fixed Point
              | Theorem implies the existence of the fixed point and provides a convergent algorithm for finding
              | it, see Algorithm 21. This is a convenient algorithm as it only relies on solution of the nonlinear
              | evolution equation (D.55), but is known to suffer from poor convergence rates and lack of convergence
              | if the mapping under consideration is not a contraction.
              |    Another class of solvers for time-periodically constrained partial differential equations rely on
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                               274
blank         | 
              | 
              | 
text          | Algorithm 21 Fixed Point Iteration Time-Periodic Solutions of PDE
              | Input: Initial guess for periodic initial condition, u0 ; parameter configuration, ¬µ
              | Output: Periodic initial condition, u(0)
              |  1: while u(Nt ) (u0 ; ¬µ) ‚àí u0 2 >  do
              |  2:  Update
              |                                             u0 ‚Üê u(Nt ) (u0 ; ¬µ)
              |  3:   end while
              |  4:   Define periodic initial condition
              |                                                              u(0) = u0
blank         | 
              | 
text          | unconstrained, gradient-based optimization techniques. Define the function
blank         | 
text          |                                                         1                        2
              |                                             j(u0 ) =      u(Nt ) (u0 ; ¬µ) ‚àí u0                        (D.57)
              |                                                         2                        2
blank         | 
              | 
text          | and consider the unconstrained optimization problem
blank         | 
text          |                                                        minimize j(u0 ),                               (D.58)
              |                                                        u0 ‚ààRNu
blank         | 
              | 
text          | which can be solved using gradient-based optimization techniques such as steepest descent, the
              | Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm, or the limited-memory version of BFGS (L-
              |                                                     dj
              | BFGS) [71, 215, 143]. The gradient of (D.57),          , is usually computed using the adjoint method
              |                                                    du0
              | since the large number of optimization variables, Nu , renders the finite differences method or the
              |                                                                                    d(¬∑)
              | linearized forward method impractical [78]. Throughout this work, the notation          will be used to
              |                                                                                     d¬µ
              | denote the total derivative of a quantity of interest with respect to parameters‚Äîincluding the explicit
              | dependence as well as the implicit dependence through the solution of the governing equation‚Äîand
              |                                 ‚àÇ(¬∑)
              | the partial derivative notation      will be used elsewhere. The adjoint equations for the fully discrete
              |                                 ‚àÇ¬µ
              | evolution equations in (D.55) corresponding to the quantity of interest, j(u0 ), with parameter u0
              | are
              |                       Œª(Nt ) = u(Nt ) (u0 ; ¬µ) ‚àí u0
              |                                                 s
              |                          (n‚àí1)        (n)
              |                                                 X         ‚àÇr  (n)                 T
              |                                                                                       (n)
              |                      Œª           =Œª         +       ‚àÜtn       ui , ¬µ, tn‚àí1 + ci ‚àÜtn Œ∫i
              |                                                 i=1
              |                                                           ‚àÇu                                          (D.59)
              |                                                    s
              |                            (n)
              |                                                   X      ‚àÇr  (n)                 T
              |                                                                                      (n)
              |                    M T Œ∫i        = bi Œª(n) +     aji ‚àÜtn     uj , ¬µ, tn‚àí1 + cj ‚àÜtn Œ∫j
              |                                              j=i
              |                                                          ‚àÇu
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s. The gradient of j(u0 ) is reconstructed from the dual variables as
blank         | 
text          |                                             dF       T
              |                                                = Œª(0) + u0 ‚àí u(Nt ) (u0 ; ¬µ).                         (D.60)
              |                                             d¬µ
              | See [211] for the derivation. These methods have been used with considerable success to solve a
              | variety of time-periodic partial differential equations, including the Benjamin-Ono equation [7], a
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                  275
blank         | 
              | 
              | 
text          | wave-guide array mode-locked laser system [202], and the vortex sheet with surface tension [8]. Un-
              | fortunately, the underlying optimization algorithms suffer from relatively slow convergence, requiring
              | many line-searches before becoming superlinear, and never achieve quadratic convergence.
              |    An attractive alternative is to recast the fixed point iteration as a nonlinear system of equations
              | and use the Newton-Raphson method to reap the benefits of quadratic convergence. To this end,
              | define the nonlinear system of equations
blank         | 
text          |                                      R(u0 ) = u(Nt ) (u0 ; ¬µ) ‚àí u0 = 0                                    (D.61)
blank         | 
text          | with Jacobian matrix
              |                                              ‚àÇR          ‚àÇu(Nt )
              |                                 J (u0 ) =        (u0 ) =         (u0 ; ¬µ) ‚àí I                             (D.62)
              |                                              ‚àÇu0          ‚àÇu0
              | where I is the Nu √ó Nu identity matrix. The crucial component of the Newton-Raphson method is
              | the solution of a linear system of equations with the Jacobian (D.62), i.e., the solution of J (u0 )x = b,
              |                                                                            ‚àÇu(Nt )
              | given u0 ‚àà RNu and b ‚àà RNu . A linear evolution equation defining                  , i.e., the sensitivity of
              |                                                                             ‚àÇu0
              | the final state with respect to perturbations in the initial state, is introduced by linearizing the fully
              |                                                                                 (n)
              | discrete evolution equation in (D.55) about the primal state u(n) , ki                with respect to the initial
              | state u0 . Direct differentiation of (D.55) with respect to u0 leads to the forward sensitivity equations
blank         | 
text          |                   ‚àÇu(0)
              |                         =I
              |                    ‚àÇu0
              |                                          s         (n)
              |                   ‚àÇu(n)   ‚àÇu(n‚àí1) X ‚àÇki
              |                         =        +     bi
              |                    ‚àÇu0     ‚àÇu0            ‚àÇu0                                                             (D.63)
              |                                    i=1
              |                                                                  Ô£Æ                                  Ô£π
              |                      (n)                               ‚àÇu(n‚àí1) X                     i       (n)
              |                   ‚àÇki        ‚àÇr  (n)                                    ‚àÇkj
              |               M        = ‚àÜtn     ui , ¬µ, tn‚àí1 + ci ‚àÜtn Ô£∞       +     aij      Ô£ª.
              |                    ‚àÇu0       ‚àÇu                          ‚àÇu0     j=1
              |                                                                           ‚àÇu0
blank         | 
text          |             ‚àÇu(Nt )
              | In general,         is a large (Nu √ó Nu ), dense matrix that requires the solution of Nu linear
              |              ‚àÇu0
              | evolution equations to form. While it is true that the columns of the matrix can be solved in
              | parallel, formation and storage of this matrix may be impractical, particularly for the large-scale
              | computational fluid dynamics problems that motivate this work. For non-dissipative problems such
              | as standing waves in the free-surface Euler equations [201, 176], this is worth the expense since all
              | perturbation directions have to be explored (as opposed to letting the evolution over a cycle damp
              | out high frequency transients). But for viscous problems such as those studied in the numerical
              | experiments, solving the Newton-Raphson equations by Krylov subspace methods requires many
              | fewer iterations than there are columns of the Jacobian.
              |                                ‚àÇu(Nt )
              |     Formation and storage of           can be completely avoided if a matrix-free Krylov method [106]
              |                                  ‚àÇu0
              | is used to solve the linear systems arising in the Newton-Raphson method, i.e., J (u0 )x = b. In this
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                               276
blank         | 
              | 
              | 
text          | case, only matrix-vector products of the form
blank         | 
text          |                                            ‚àÇR           ‚àÇu(Nt )
              |                               J (u0 )v =       (u0 )v =         (u0 ; ¬µ)v ‚àí v                         (D.64)
              |                                            ‚àÇu0           ‚àÇu0
blank         | 
text          | for any v ‚àà RNu , are required. For efficiency, these must be computed without explicitly forming the
              |         ‚àÇu(Nt )
              | matrix          . This is accomplished by considering the forward sensitivity equations in (D.63) in
              |           ‚àÇu0
              | the direction defined by v. Multiplying (D.63) by the vector v leads to the system of linear evolution
              | equations
blank         | 
text          |             ‚àÇu(0)
              |                   v=v
              |              ‚àÇu0
              |                                  s       (n)
              |             ‚àÇu(n)    ‚àÇu(n‚àí1)    X     ‚àÇk
              |                   v=         v+     bi i v
              |              ‚àÇu0      ‚àÇu0              ‚àÇu0                                                            (D.65)
              |                                 i=1
              |                                                             Ô£Æ                              Ô£π
              |               (n)                                   ‚àÇu(n‚àí1)     i        (n)
              |             ‚àÇki           ‚àÇr  (n)                              X       ‚àÇkj
              |         M         v = ‚àÜtn     ui , ¬µ, tn‚àí1 + ci ‚àÜtn Ô£∞        v+     aij       vÔ£ª .
              |              ‚àÇu0          ‚àÇu                          ‚àÇu0       j=1
              |                                                                          ‚àÇu0
blank         | 
text          |                                              (n)
              |                         ‚àÇu(n)          ‚àÇki
              | These can be solved for       ¬∑ v and        ¬∑ v directly, only requiring one linear evolution for each
              |                          ‚àÇu0            ‚àÇu0
              | v. Since the equations in (D.65) are linear, the underlying linear solver must be converged to high
              | accuracy if accurate sensitivities are to be obtained. This mitigates the speedup with respect to the
              | nonlinear, primal solves whose linear systems are usually solved to low precision. For the problems
              | considered in Section D.4.4‚ÄìD.4.5, the primal equations were, on average, 2 times more expensive
              | than the sensitivity equations, even though 5 nonlinear iterations were required for convergence.
              | This implies the cost of evaluating R(u0 ) is approximately 2 times as expensive as a Jacobian-
              | vector product J (u0 )v. The Newton-Krylov method, with Jacobian-vector products computed as
              | the solution of (D.65), is summarized in Algorithm 22. If the linear system of equations arising
              | at each iteration is solved to sufficient accuracy, this algorithm will converge quadratically. The
              | starting guess can be obtained by fixed point iteration (Algorithm 21), or, if solutions come in
              | families parametrized by an amplitude, by numerical continuation [76, 103, 53, 7, 8, 200, 176]. The
              | latter approach is particularly useful if the system is not dissipative and externally driven, as fixed
              | point iteration relies on transient modes being damped by the evolution equations, i.e., on the
              | periodic solution being stable and attracting.
              |    Given this exposition on methods for computing time-periodic solutions of partial differential
              | equations, we turn our attention to determining the stability of the corresponding periodic orbit.
blank         | 
              | 
text          | D.3.2       Stability of Periodic Orbits of Fully Discrete Partial Differential
              |             Equations
              | In this section, the concept of stability of a periodic orbit of fully discrete partial differential equations
              | is introduced and a method for determining the stability of a periodic solution presented. Recall
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                      277
blank         | 
              | 
              | 
text          | Algorithm 22 Newton-Krylov Shooting Method for Time-Periodic Solutions of PDE
              | Input: Initial guess for periodic initial condition, u0 ; parameter configuration, ¬µ
              | Output: Periodic initial condition, u(0)
              |  1: while u(Nt ) (u0 ; ¬µ) ‚àí u0 2 >  do
              |  2:  Solve unsymmetric linear system of equations
blank         | 
text          |                                      ‚àÇu(Nt )
              |                                              (u0 ; ¬µ) ¬∑ ‚àÜu = u(Nt ) (u0 ; ¬µ) ‚àí u0
              |                                       ‚àÇu0
              |       using a matrix-free Krylov method with matrix-vector products
blank         | 
text          |                                                   ‚àÇu(Nt )
              |                                                           (u0 ; ¬µ) ¬∑ v
              |                                                    ‚àÇu0
              |       computed as the solution of the directional sensitivity equations (D.65)
              |  3:     Update solution
              |                                               u0 ‚Üê u0 ‚àí ‚àÜu
              |  4:   end while
              |  5:   Define periodic initial condition
              |                                                        u(0) = u0
blank         | 
              | 
text          | the interpretation of u(Nt ) as a function that propagates an initial condition u0 to its final state
              | u(Nt ) (u0 ; ¬µ). Let u‚àó0 (¬µ) be the time-periodic solution of the fully discrete partial differential
              | equation in (D.52), (D.54) at parameter configuration ¬µ, i.e., u‚àó0 (¬µ) = u(Nt ) (u‚àó0 ; ¬µ). A periodic
              | orbit is stable if there is a Œ¥ > 0 such that
blank         | 
text          |                                lim    u(n¬∑Nt ) (u‚àó0 (¬µ) + ‚àÜu; ¬µ) ‚àí u‚àó0 (¬µ) = 0                (D.66)
              |                               n‚Üí‚àû
blank         | 
              | 
text          | if k‚àÜuk < Œ¥, where
              |                            u(n¬∑Nt ) (u0 ; ¬µ) = u(Nt ) (¬∑; ¬µ) ‚ó¶ ¬∑ ¬∑ ¬∑ ‚ó¶ u(Nt ) (u0 ; ¬µ).       (D.67)
blank         | 
text          | A Taylor expansion of u(Nt ) about the periodic solution leads to
blank         | 
text          |                                                       ‚àÇu(Nt ) ‚àó                       2
              |                    u(Nt ) (u‚àó0 (¬µ); ¬µ) = u‚àó0 (¬µ) +           (u0 (¬µ); ¬µ) ¬∑ ‚àÜu + O(k‚àÜuk )      (D.68)
              |                                                        ‚àÇu0
blank         | 
text          | where time-periodicity of u‚àó0 (¬µ) was used. Repeated application of (D.68) leads to
              |                                                                         n
              |                                                       ‚àÇu(Nt ) ‚àó
blank         |                                                   
text          |                                                                                     n+1
              |          u(n¬∑Nt ) (u‚àó0 (¬µ) + ‚àÜu; ¬µ) = u‚àó0 (¬µ) +              (u0 (¬µ); ¬µ) ‚àÜu + O(k‚àÜuk    ).    (D.69)
              |                                                        ‚àÇu0
blank         | 
text          |                                                                              ‚àÇu(Nt ) ‚àó
              | Taking Œ¥ < 1, the stability criteria in (D.66) is satisfied if all eigenvalues of   (u0 (¬µ); ¬µ) have
              |                                                                                ‚àÇu0
              | modulus strictly less than 1. In Section D.4.4, the stability of the periodic flow around a flapping
              | airfoil is verified using this method.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                      278
blank         | 
              | 
              | 
title         | D.4         Fully Discrete, Time-Periodic Adjoint Method
text          | In this section, the adjoint equations corresponding to the fully discrete time-periodically constrained
              |                                                                                                                          (1)
              | partial differential equations (D.52), (D.54) and quantity of interest F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ),
              |                                                                                                 (1)
              | will be derived. For the remainder of this section, u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) will be taken as
              | the time-periodic solution of the fully discrete partial differential equations (D.52), (D.54) at pa-
              | rameter ¬µ. The adjoint equations will be derived by linearizing the fully discrete equations about
              | this periodic solution. This highlights the importance of an efficient periodic solver‚Äîthe subject of
              | Section D.3.1‚Äîas it is a prerequisite for the adjoint method.
              |     Before proceeding to the derivation of the adjoint equations, the following definitions are in-
              | troduced for the fully discrete time-periodic constraint and Runge-Kutta stage equations and state
              | updates
blank         | 
text          |                                rÃÉ (0) (u(0) , u(Nt ) ) = u(0) ‚àí u(Nt ) = 0
              |                                                                                      s
              |                                 (n)                                                           (i)
              |                                                                                      X
              |      rÃÉ (n) (u(n‚àí1) , u(n) , k1 , . . . , ks(n) , ¬µ) = u(n) ‚àí u(n‚àí1) ‚àí                     bi ki = 0                           (D.70)
              |                                                                                      i=1
blank         |                                                                                                  
text          |                 (n)              (n)         (n)               (n)            (n)
              |               Ri (u(n‚àí1) , k1 , . . . , ki , ¬µ) = M ki               ‚àí ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn = 0
blank         | 
text          | for n = 1, . . . , n and i = 1, . . . , s.
blank         | 
              | 
title         | D.4.1        Derivation
text          | The derivation of the fully discrete adjoint equations corresponding to the output functional, F ,
              | begins with the introduction of test variables
blank         | 
text          |                                                                      (n)
              |                                                    Œª(0) , Œª(n) , Œ∫i        ‚àà RNu                                               (D.71)
blank         | 
text          |                                                                                (1)
              | for n = 1, . . . , Nt and i = 1, . . . , s. Since u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) are taken as the solution of
              | the fully discrete time-periodic problem in (D.70), the following identity holds, for any ¬µ ‚àà RN¬µ ,
blank         | 
text          |                                                               Nt                           Nt X
              |                                                                                               s
              |                                                     T                      T                          (n) T    (n)
              |                                                               X                            X
              |                       F = F + 0 = F ‚àí Œª(0) rÃÉ (0) ‚àí                  Œª(n) rÃÉ (n) ‚àí                   Œ∫i       Ri               (D.72)
              |                                                               n=1                          n=1 i=1
blank         | 
text          |                                                               (n)
              | for any value of the test functions Œª(n) and Œ∫i . In (D.72), arguments have been dropped for
              | brevity; it is understood that all terms are evaluated at the periodic solution of (D.52), (D.54) at
              |                                                                                                                    (1)
              | parameter ¬µ. Since (D.70) holds for any ¬µ ‚àà RN¬µ , provided u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) is the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                         279
blank         | 
              | 
              | 
text          | corresponding periodic solution, differentiation with respect to ¬µ leads to
blank         | 
text          |            Nt                     Nt X   s           (n)
              |                ‚àÇF ‚àÇu(n) X
              |                                                                       (0)
              |                                                                                  ‚àÇ rÃÉ (0) ‚àÇu(0)     ‚àÇ rÃÉ (0) ‚àÇu(Nt )
blank         |                                                                                                                      
text          | dF   ‚àÇF   X                                  ‚àÇF ‚àÇki            (0) T ‚àÇ rÃÉ
              |    =    +                     +                           ‚àí  Œª                +                 +
              | d¬µ   ‚àÇ¬µ n=0 ‚àÇu(n) ‚àÇ¬µ             n=1 i=1 ‚àÇki
              |                                                (n) ‚àÇ¬µ                 ‚àÇ¬µ         ‚àÇu(0) ‚àÇ¬µ          ‚àÇu(Nt ) ‚àÇ¬µ
              |            Nt
              |                       "                                                        s
              |                                                                                                   #
              |                            (n)                                                                (n)
              |           X
              |                 (n) T ‚àÇ rÃÉ         ‚àÇ rÃÉ (n) ‚àÇu(n)     ‚àÇ rÃÉ (n) ‚àÇu(n‚àí1) X ‚àÇ rÃÉ (n) ‚àÇkp
              |         ‚àí     Œª                 +                 +                        +
              |           n=1
              |                           ‚àÇ¬µ       ‚àÇu(n) ‚àÇ¬µ         ‚àÇu(n‚àí1) ‚àÇ¬µ                p=1 ‚àÇkp
              |                                                                                         (n) ‚àÇ¬µ
blank         | 
text          |                            Ô£Æ                                                           Ô£π
              |            Nt Xs                 (n)          (n)                  i      (n)    (n)
              |           X          (n) T Ô£∞ ‚àÇRi           ‚àÇRi     ‚àÇu(n‚àí1) X ‚àÇRi ‚àÇkj Ô£ª
              |         ‚àí         Œ∫i                  +                       +                          .
              |           n=1 i=1
              |                                ‚àÇ¬µ         ‚àÇu(n‚àí1) ‚àÇ¬µ             j=1 ‚àÇk
              |                                                                           (n) ‚àÇ¬µ
              |                                                                                         j
              |                                                                                                                   (D.73)
              | Re-arrangement of terms in (D.73) such that the state variable sensitivities are isolated leads to the
              |                          dF
              | following expression for
              |                          d¬µ
blank         | 
text          |                                      (Nt )                  (0)                 Nt                      Nt Xs           (n)
              |                                                                     ‚àÇu(Nt ) X                    (n)
blank         |                                                                 
text          | dF   ‚àÇF       ‚àÇF        (Nt ) T ‚àÇ rÃÉ             (0) T ‚àÇ rÃÉ                           (n) T ‚àÇ rÃÉ
              |                                                                                                        X
              |                                                                                                                 (n) T ‚àÇRp
              |    =    +           ‚àí Œª                     ‚àí  Œª                             ‚àí      Œª                ‚àí         Œ∫p
              | d¬µ   ‚àÇ¬µ     ‚àÇu(Nt )             ‚àÇu(Nt )               ‚àÇu(Nt )        ‚àÇ¬µ        n=0
              |                                                                                              ‚àÇ¬µ        n=1 p=1
              |                                                                                                                        ‚àÇ¬µ
              |            Nt
              |               "                                                                  s
              |                                                                                                        #
              |                                               (n‚àí1)                    (n)                         (n)
              |           X        ‚àÇF           (n‚àí1) T ‚àÇ rÃÉ                (n) T ‚àÇ rÃÉ
              |                                                                                X      (n) T ‚àÇRi          ‚àÇu(n‚àí1)
              |         +                ‚àí   Œª                        ‚àí  Œª                   ‚àí      Œ∫
              |           n=1
              |                 ‚àÇu(n‚àí1)                    ‚àÇu(n‚àí1)                 ‚àÇu(n‚àí1) i=1 i            ‚àÇu(n‚àí1)         ‚àÇ¬µ
              |                   Ô£Æ                                                        Ô£π
              |            Nt Xs                            (n)       s               (n)       (n)
              |                   Ô£∞ ‚àÇF ‚àí Œª(n) ‚àÇ rÃÉ                           (n) T ‚àÇRi Ô£ª ‚àÇkp
              |           X                           T              X
              |         +              (n)                  (n)
              |                                                   ‚àí      Œ∫   i        (n)
              |                                                                                     .
              |           n=1 p=1 ‚àÇki                   ‚àÇkp          i=p           ‚àÇkp        ‚àÇ¬µ
              |                                                                                                                 (D.74)
              |                                     (n)
              | The dual variables, Œª(n) and Œ∫i , which have remained arbitrary to this point, are chosen such that
              | the bracketed terms in (D.74) vanish
blank         | 
text          |                                 T                    T
              |                         ‚àÇ rÃÉ (0)   (0)   ‚àÇ rÃÉ (Nt )   (Nt )    ‚àÇF T
              |                                  Œª     +            Œª       =
              |                        ‚àÇu(Nt )           ‚àÇu(Nt )              ‚àÇu(Nt )
              |                             T                    T                                          s             (n) T
              |                     ‚àÇ rÃÉ (n)   (n)   ‚àÇ rÃÉ (n‚àí1)   (n‚àí1)     ‚àÇF T X ‚àÇRi           (n)
              |                              Œª     +            Œª       =         ‚àí             Œ∫i                                (D.75)
              |                    ‚àÇu(n‚àí1)           ‚àÇu(n‚àí1)              ‚àÇu(n‚àí1)   i=1
              |                                                                         ‚àÇu(n‚àí1)
blank         | 
text          |                                               (n)        T                                     T
              |                                           s
              |                                           X ‚àÇRj               (n)        ‚àÇF         ‚àÇ rÃÉ (n)
              |                                                 (n)
              |                                                              Œ∫j     =     (n)
              |                                                                                 ‚àí     (n)
              |                                                                                                    Œª(n)
              |                                           j=i ‚àÇki                       ‚àÇki         ‚àÇki
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s. These are the fully discrete adjoint equations corresponding to the
              | time-periodic primal evolution equations in (D.70), discrete quantity of interest F , and parameter
              | ¬µ. Defining the dual variables as the solution of the adjoint equations in (D.75), the expression for
              | dF
              |     in (D.74) reduces to
              | d¬µ
blank         | 
text          |                                              N                            N     s                    (n)
              |                                               T ‚àÇ rÃÉ (n)
              |                                        t                     t X
              |                             dF   ‚àÇF   X                    X            T ‚àÇRp
              |                                =    ‚àí    Œª(n)            ‚àí         Œ∫(n)
              |                                                                     p         .                                   (D.76)
              |                             d¬µ   ‚àÇ¬µ n=0          ‚àÇ¬µ        n=1 p=1
              |                                                                            ‚àÇ¬µ
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                280
blank         | 
              | 
              | 
text          |                                                             dF
              | This provides a means of computing the total derivative         without explicitly computing the large,
              |                                                             d¬µ
              | dense state sensitivities since the expression in (D.76) is independent of them. Direct differentiation
              |                (n)
              | of rÃÉ (n) and Ri         from their definitions in (D.70) leads to the final form of the adjoint equations of
              | the fully discrete, time-periodically constrained partial differential equations in (D.52), (D.54)
blank         | 
text          |                                    ‚àÇF T
              |             Œª(Nt ) = Œª(0) +
              |                                   ‚àÇu(Nt )
              |                                               s
              |                                     ‚àÇF T X           ‚àÇr  (n)                 T
              |                                                                                  (n)
              |            Œª(n‚àí1)        = Œª(n) +    (n‚àí1)
              |                                            +     ‚àÜtn     ui , ¬µ, tn‚àí1 + ci ‚àÜtn Œ∫i                        (D.77)
              |                                   ‚àÇu         i=1
              |                                                      ‚àÇu
              |                                      T                 s
              |                (n)            ‚àÇF                       X               ‚àÇr  (n)                 T
              |                                                                                                    (n)
              |          M T Œ∫i          =     (n)
              |                                          + bi Œª(n) +         aji ‚àÜtn       uj , ¬µ, tn‚àí1 + cj ‚àÜtn Œ∫j
              |                              ‚àÇki                       j=i
              |                                                                        ‚àÇu
blank         | 
text          | for n = 1, . . . , Nt and i = 1, . . . , s. Similarly, the total derivative of F , independent of state sensi-
              | tivities, takes the form
blank         | 
text          |                                        Nt      s
              |                              dF   ‚àÇF   X      X    (n) T ‚àÇr   (n)
              |                                 =    +    ‚àÜtn     Œ∫i        (ui , ¬µ, tn‚àí1 + ci ‚àÜtn ).                    (D.78)
              |                              d¬µ   ‚àÇ¬µ n=1      i=1
              |                                                          ‚àÇ¬µ
blank         | 
text          | From (D.77), it can be seen that the fully discrete adjoint equations take the form of a linear, two-
              | point boundary-value problem and cannot be solved directly as an evolution equation. D.6 proves
              | existence and uniqueness of solutions to (D.77). The next section will discuss solvers for the discrete
              | time-periodic adjoint equations in (D.77).
blank         | 
              | 
title         | D.4.2      Numerical Solver: Matrix-Free Krylov Method
text          | As the adjoint equations corresponding to the fully discrete time-periodic partial differential equation
              | are linear, this section will consider matrix-free Krylov methods to solve them. Alternatively, any
              | of the methods discussed in Section D.3.1 could be used.
              |    Define Œª(0) (ŒªNt ; ¬µ, t) as the solution of the linear, backward evolution equations
blank         | 
text          |              Œª(Nt ) = ŒªNt
              |                                                          s
              |                                            ‚àÇF T X           ‚àÇr  (n)                 T
              |                                                                                         (n)
              |             Œª(n‚àí1) = Œª(n) +                 (n‚àí1)
              |                                                   +     ‚àÜtn     ui , ¬µ, tn‚àí1 + ci ‚àÜtn Œ∫i
              |                                          ‚àÇu         i=1
              |                                                             ‚àÇu                                           (D.79)
              |                                                    s
              |                    (n)        ‚àÇF                   X               ‚àÇr  (n)                 T
              |                                                                                                (n)
              |          M T Œ∫i          =     (n)
              |                                      + bi Œª(n) +         aji ‚àÜtn       uj , ¬µ, tn‚àí1 + cj ‚àÜtn Œ∫j ,
              |                              ‚àÇki                   j=i
              |                                                                    ‚àÇu
blank         | 
text          | which can be directly evolved, backward-in-time. Similar to Section D.3.1 this constitutes a notation
              | overload since Œª(0) ‚àà RNu is the initial solution of the adjoint equations corresponding to the fully
              | discrete periodic partial differential equations, as well as the linear function that takes a state ŒªNt
              | to Œª(0) (ŒªNt ; ¬µ). Then, Œª(0) (ŒªNt ; ¬µ) is the initial solution of (D.77) if the following linear equation
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                          281
blank         | 
              | 
              | 
text          | is satisfied
              |                                                                     ‚àÇF T
              |                                        Œª(0) (ŒªNt ; ¬µ, t) = ŒªNt ‚àí           .                      (D.80)
              |                                                                    ‚àÇu(Nt )
              | This is a linear system of equations of the form, Ax = b where
blank         | 
text          |                                                       ‚àÇŒª(0)
              |                                                 A=          ‚àí I.                                  (D.81)
              |                                                       ‚àÇŒªNt
blank         | 
text          | The columns of the linear operator A can be formed by considering perturbations of (D.79) with
              | respect to the final state ŒªNt . Differentiation of (D.79) with respect to ŒªNt leads to the adjoint
              | sensitivity equations
blank         | 
text          |                       ‚àÇŒª(Nt )
              |                               =I
              |                        ‚àÇŒªNt
              |                                            s                                       (n)
              |                      ‚àÇŒª(n‚àí1)   ‚àÇŒª(n) X        ‚àÇr  (n)                  T ‚àÇŒ∫
              |                                                                              i
              |                              =      +     ‚àÜtn     ui , ¬µ, tn‚àí1 + ci ‚àÜtn                           (D.82)
              |                       ‚àÇŒªNt     ‚àÇŒªNt   i=1
              |                                               ‚àÇu                           ‚àÇŒª Nt
blank         | 
text          |                        (n)              s                                                (n)
              |                      ‚àÇŒ∫i
              |                      T          ‚àÇŒª(n) X            ‚àÇr  (n)                  T ‚àÇŒ∫
              |                                                                                   j
              |                    M       = bi      +     aji ‚àÜtn     uj , ¬µ, tn‚àí1 + cj ‚àÜtn          .
              |                      ‚àÇŒªNt       ‚àÇŒªNt   j=i
              |                                                    ‚àÇu                           ‚àÇŒª Nt
blank         | 
              | 
              | 
text          |                                                                     ‚àÇŒª(0)
              | Similar to the situation for the primal problem, the matrix                is an Nu √ó Nu dense matrix
              |                                                                      ‚àÇŒªNt
              | that requires Nu         linear evolution equations to form. As this is impractical for large problems, a
              | matrix-free Krylov method is used to solve (D.80), which only requires matrix-vector products of
              | the form
              |                                                       ‚àÇŒª(0)
              |                                                Av =         v ‚àí v.                                (D.83)
              |                                                       ‚àÇŒªNt
              | The first term in this matrix-vector product can be computed directly by considering the adjoint
              | sensitivity equations in a given direction v
blank         | 
text          |                 ‚àÇŒª(Nt )
              |                         v=v
              |                  ‚àÇŒªNt
              |                                     s                                T ‚àÇŒ∫(n)
              |                ‚àÇŒª(n‚àí1)    ‚àÇŒª(n)    X       ‚àÇr  (n)                       i
              |                        v=       v+     ‚àÜtn     ui , ¬µ, tn‚àí1 + ci ‚àÜtn          v                   (D.84)
              |                 ‚àÇŒªNt      ‚àÇŒªNt     i=1
              |                                            ‚àÇu                           ‚àÇŒª Nt
blank         | 
text          |              (n)                  s                                                  (n)
              |            ‚àÇŒ∫i T        ‚àÇŒª(n)    X           ‚àÇr  (n)                  T ‚àÇŒ∫
              |                                                                             j
              |          M       v = bi       v+     aji ‚àÜtn     uj , ¬µ, tn‚àí1 + cj ‚àÜtn          v.
              |            ‚àÇŒªNt         ‚àÇŒªNt     j=i
              |                                              ‚àÇu                           ‚àÇŒª Nt
blank         | 
              | 
              | 
text          |                                             ‚àÇŒª(0)
              | The equations in (D.84) can be solved for         ¬∑ v at the cost of one linear evolution solution for
              |                                             ‚àÇŒªNt
              | each v. The adjoint sensitivity equations in (D.84) are independent of the quantity of interest, F . If
              | there are multiple quantities of interest, fast multiple right-hand side solvers [182, 38, 79] could be
              | used to solve Ax = b as the matrix A will be fixed and only the right-hand side varied. Furthermore,
              | the adjoint sensitivity equations in (D.84) and the adjoint equations in (D.79) are identical, with the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                         282
blank         | 
              | 
              | 
text          |                          ‚àÇF           ‚àÇF
              | exception of the terms          and         . Therefore, the adjoint sensitivities are less expensive to
              |                        ‚àÇu(n‚àí1)       ‚àÇki
              |                                         (n)
blank         | 
text          | compute than the adjoint states and the savings becomes substantial when the number of parameters
              |                       ‚àÇF          ‚àÇF
              | in ¬µ is large since         and         become expensive to compute. Algorithm 23 below details
              |                     ‚àÇu(n‚àí1)      ‚àÇki
              |                                     (n)
blank         | 
text          | the use of a matrix-free GMRES method to solve (D.80) with matrix-vector products defined by
              | (D.84).
blank         | 
text          | Algorithm 23 GMRES for Solution of Fully Discrete, Time-Periodic Adjoint PDE
              | Input: Initial guess for periodic adjoint final condition, ŒªNt ,0 ; parameter configuration, ¬µ; periodic
              |                                               (1)          (N )
              |     primal solution, u(0) , . . . , u(Nt ) , k1 , . . . , ks t
              |                                                          (Nt )
              | Output: Periodic adjoint final condition, Œª
              |  1: Compute
              |                                                                       ‚àÇF T
              |                                        r0 = Œª(0) (ŒªNt , ¬µ) +                 ‚àí ŒªNt ,0
              |                                                                      ‚àÇu(Nt )
              |  2: Set Œ≤ = kr0 k2 , v1 = r0 /Œ≤, and ŒªNt = ŒªNt ,0
blank         | 
text          |              (0)                   ‚àÇF T
              |  3: while Œª (ŒªNt , ¬µ) +                      ‚àí ŒªNt >  do
              |                                ‚àÇu(Nt )
              |                                                        2
              |  4:   for j = 1, 2, . . . , m do
              |  5:     Compute
              |                                                                ‚àÇŒª(0)
              |                                                     wj =             vj ‚àí vj
              |                                                                ‚àÇŒªNt
              |       as the solution of the adjoint sensitivity equations (D.84)
              |  6:        for i = 1, . . . , j do
              |  7:          hij = (wj , vi )
              |  8:          wj = wj ‚àí hij vi
              |  9:        end for
              | 10:        hj+1,j = kwj k2
              | 11:        vj+1 = wj /hj+1,j
              | 12:     end for
              | 13:     Compute
              |                                         ym = arg min kŒ≤e1 ‚àí Hm yk2 ,
              |     where e1 is the first canonical until vector in RNu and H = {hij }1‚â§i‚â§m+1,1‚â§j‚â§m
              | 14:   Update solution
              |                                            ŒªNt = ŒªNt ,0 + Vm ym
              |       where                                                        
              |                                             Vm = v1    ¬∑¬∑¬∑     vm
              | 15:   end while
              | 16:   Define adjoint periodic final condition
blank         | 
text          |                                                 Œª(Nt ) = ŒªNt
blank         | 
              | 
              | 
text          |       With the solution of the fully discrete primal and dual time-periodic problems fully specified,
              | from numerical discretization to solution algorithms, we close this section with an algorithm that
              | uses the fully discrete adjoint method to compute the gradient of the quantity of interest on the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                        283
blank         | 
              | 
              | 
text          | manifold of periodic solutions. First, the fully discrete time-periodic solution (D.52), (D.54) must be
              |                                                                                                            (1)
              | computed, e.g., using a matrix-free Newton-Krylov method, to yield u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) .
              | Next, the corresponding fully discrete adjoint equations are defined about this periodic solution and
              |                                                                                                (1)
              | solved, e.g., using a matrix-free Krylov method, for Œª(0) , . . . , Œª(Nt ) , Œ∫1 , . . . , Œ∫(N
              |                                                                                            s
              |                                                                                               t)
              |                                                                                                  . Finally, (D.78)
              |                                             dF
              | is used to reconstruct the desired gradient    . This procedure is summarized in Algorithm 24.
              |                                             d¬µ
blank         | 
text          | Algorithm 24 Gradients on Manifold of Time-Periodic Solutions of PDEs
              | Input: Parameter               configuration,      ¬µ,   and    fully    discrete quantity of interest,
              |                                 (1)          (N )
              |     F (u(0) , . . . , u(Nt ) , k1 , . . . , ks t )
              |                             dF
              | Output: Gradient,                , on manifold of time-periodic solutions
              |                             d¬µ
              |  1: For parameter ¬µ, compute time-periodic solution of fully discrete PDE in (D.52), (D.54), e.g.,
              |     using the Newton-Krylov shooting method in Algorithm 22
              |                                                                          (1)
              |                                                 u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt )
              |                                                                       (1)              (N )
              |  2:   For fully discrete functional F (u(0) , . . . , u(Nt ) , k1 , . . . , ks t ), compute adjoint solution of fully
              |       discrete time-periodic PDE in (D.77), e.g., using GMRES shooting method in Algorithm 23
              |       with matrix-vector products computed from the backward evolution of the adjoint sensitivity
              |       equations in (D.84)
              |                                                                   (1)
              |                                         Œª(0) , . . . , Œª(Nt ) , Œ∫1 , . . . , Œ∫s(Nt )
              |                      dF
              |  3:   Reconstruct       using dual variables according to (D.78)
              |                      d¬µ
blank         | 
              | 
              | 
title         | D.4.3        Generalized Reduced-Gradient Method for PDE Optimization with
text          |              Time-Periodicity Constraints
              | Consider the fully discrete time-dependent PDE-constrained optimization problem
blank         | 
text          |                                                                                      (1)
              |                              minimize            F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ)
              |                        (0)
              |                      u , ...,  u(Nt ) ‚ààRNu ,
              |                       (1)
              |                      k1 , ..., ks(Nt ) ‚ààRNu ,
              |                             ¬µ‚ààRN¬µ
              |                                                                                      (1)
              |                      subject to                  c(u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ) ‚â• 0
              |                                                  u(0) = u0                                                       (D.85)
              |                                                                             s
              |                                                                                        (n)
              |                                                                             X
              |                                                  u(n) = u(n‚àí1) +                  bi ki
              |                                                                             i=1
blank         |                                                                                                     
text          |                                                     (n)                        (n)
              |                                                  M ki       = ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn
blank         | 
text          | where F is a fully discrete output functional of the partial differential equation and c is a vector
              | of such output functionals. The nested or Generalized Reduced-Gradient (GRG) approach to solve
              | (D.85) explicitly enforces the PDE constraint at each optimization iteration. The implicit function
              | theorem states that the solution of the discretized PDE, can be considered an implicit function of the
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                           284
blank         | 
              | 
              | 
text          |                                                   (n)      (n)
              | parameter ¬µ, i.e., u(n) = u(n) (¬µ) and ki               = ki (¬µ). Strict enforcement of the discretized partial
              | differential equation allows the PDE variables and equations to be removed from the optimization
              | problem
              |                                                                       (1)
              |                  minimize       F (u(0) (¬µ), . . . , u(Nt ) (¬µ), k1 (¬µ), . . . , ks(Nt ) (¬µ), ¬µ)
              |                   ¬µ‚ààRN¬µ
              |                                                                                                                  (D.86)
              |                                                                       (1)
              |                  subject to c(u(0) (¬µ), . . . , u(Nt ) (¬µ), k1 (¬µ), . . . , ks(Nt ) (¬µ), ¬µ) ‚â• 0.
blank         | 
text          |                                                                                       dF       dc
              | To solve this optimization problem using gradient-based techniques, the terms              and    ‚Äî
              |                                                                                       d¬µ       d¬µ
              | gradients of quantities of interest along the manifold of solutions of the PDE‚Äîare required. Depend-
              | ing on the relative number of variables in ¬µ to the number of constraints in c, the direct or adjoint
              | method can be efficiently used to compute these gradients without relying on finite differences.
              |    Now consider the optimization problem in (D.85) with the time-periodicity constraint added
blank         | 
text          |                                                                               (1)
              |                             minimize           F (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ)
              |                       (0)
              |                     u , ...,  u(Nt ) ‚ààRNu ,
              |                      (1)
              |                     k1 , ..., ks(Nt ) ‚ààRNu ,
              |                            ¬µ‚ààRN¬µ
              |                                                                              (1)
              |                     subject to                 c(u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ) ‚â• 0
              |                                                u(0) = u(Nt )                                                     (D.87)
              |                                                                     s
              |                                                                                 (n)
              |                                                                     X
              |                                                u(n) = u(n‚àí1) +              bi ki
              |                                                                     i=1
blank         |                                                                                      
text          |                                                   (n)             (n)
              |                                                M ki      = ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn .
blank         | 
text          | Strict enforcement of the time-periodic partial differential equations leads to an application of the
              |                                                                                              (n)      (n)
              | implicit function theorem, similar to that above, i.e., u(n) = u(n) (¬µ) and ki                     = ki (¬µ), where u(n)
              |       (n)
              | and ki      are the time-periodic solution of the discrete partial differential equations. This results in
              |                                                                                                                 (n)
              | an optimization problem identical to that in (D.86) with this new definition of u(n) (¬µ) and ki (¬µ).
              | The novel periodic adjoint method, derived in Section D.4.1, can be used to compute gradients along
              |                                                                         dF       dc
              | the manifold of time-periodic solutions of the fully discrete PDE, i.e.      and    , for the use in
              |                                                                         d¬µ       d¬µ
              | gradient-based optimization.
blank         | 
              | 
title         | D.4.4       Numerical Experiment: Time-Periodic Solutions of the Compress-
              |             ible Navier-Stokes Equations
text          | In this section, the various solvers discussed in this document for determining primal and dual time-
              | periodic solutions of partial differential equations are compared for a flapping airfoil in an isentropic,
              | viscous flow. The stability of the periodic orbit is verified by performing an eigenvalue analysis of
              | ‚àÇu(Nt )
              |         . The section closes with validation of the adjoint method, introduced for efficient gradient
              |   ‚àÇu0
              | computation of quantities of interest, against a second-order finite difference approximation.
              |    Consider the NACA0012 airfoil in Figure D.27 immersed in an isentropic, viscous flow with
              | Reynolds and Mach number set to 1000 and 0.2, respectively. The kinematic motion of the foil is
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                           285
blank         | 
              | 
              | 
text          |                 1                                                                     0.2
blank         | 
              | 
text          |      h(¬µ, t)
blank         | 
              | 
              | 
              | 
text          |                                                                            Œ∏(¬µ, t)
              |                 0                                                                       0
blank         | 
              | 
text          |                ‚àí1                                                                    ‚àí0.2
              |                      0                2.5                 5                                 0                    2.5              5
              |                                      time                                                                       time
blank         | 
text          | Figure D.17: Trajectories of h(¬µ, t) and Œ∏(¬µ, t) that define the motion of the airfoil in Figure D.27
              | and will be used to study primal and dual time-periodic solvers.
blank         | 
              | 
text          | parametrized with a single Fourier mode, i.e.,
blank         | 
text          |                                                h(¬µ, t) = Ah sin(œâh t + œÜh ) + ch
              |                                                                                                                                  (D.88)
              |                                                 Œ∏(¬µ, t) = AŒ∏ sin(œâŒ∏ t + œÜŒ∏ ) + cŒ∏ .
blank         | 
text          | The vector of parameters is fixed for the remainder of this section
              |     h                                                      i h                                                                i
              |  ¬µ = Ah                                                                                           œÄ                œÄ
              |                     œâh    œÜh   ch    AŒ∏      œâŒ∏   œÜŒ∏     cŒ∏ = 1.0          0.4œÄ       0.0   0.0   15       0.4œÄ    2       0.0 , (D.89)
blank         | 
text          | and corresponds to the motion in Figure D.17 with period T = 5. The mapping G(X, t) from the
              | fixed reference domain V to the physical domain ‚Ñ¶(¬µ, t) takes the form of a parametrized rigid body
              | motion
              |                                          G(X, t) = v(¬µ, t) + Q(¬µ, t)(X ‚àí x0 ) + x0 ,                                             (D.90)
blank         | 
text          | where x0 is the location of pitching axis in the reference configuration (the 1/3 chord) and
              |                                      "                                 #                               "          #
              |                                           cos Œ∏(¬µ, t)    sin Œ∏(¬µ, t)                                        0
              |                          Q(¬µ, t) =                                                      v(¬µ, t) =                      .
              |                                          ‚àí sin Œ∏(¬µ, t)   cos Œ∏(¬µ, t)                                   h(¬µ, t)
blank         | 
text          | The isentropic Navier-Stokes equations are discretized with the discontinuous Galerkin scheme of
              | Section D.1.2 using 978 triangular p = 3 elements. No-slip boundary conditions are imposed on
              | the airfoil wall and characteristic free-stream boundary conditions at the far-field. The temporal
              | discretization uses a third-order diagonally implicit Runge-Kutta solver with 100 equally spaced
              | steps to discretize a single period of the motion. The airfoil and surrounding fluid vorticity field
              | are shown in Figures D.18 and D.19 with the flow field initialized from steady-state flow and the
              | time-periodic initial condition, respectively. It is clear that the flow in Figure D.19 will seamlessly
              | transition between periods. The initialization from the steady-state solution in Figure D.18 will
              | introduce non-physcial transients into the flow as discussed in the next section.
              |    First, the solvers introduced in Section D.3.1 are compared for different initial guesses for the
              | time-periodic initial condition. In the absence of any a priori information regarding the time-periodic
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                      286
blank         | 
              | 
              | 
              | 
text          | Figure D.18: Flow vorticity around heaving/pitching airfoil for simulation initialized from steady
              | state flow. Non-physical transients are introduced at the beginning of the time interval that re-
              | sult in non-trivial errors in integrated quantities of interests. Snapshots taken at times t =
              | 0.0, 1.0, 2.0, 3.0, 4.0, 5.0.
blank         | 
              | 
              | 
              | 
text          | Figure D.19: Time-periodic flow vorticity around heaving/pitching airfoil, i.e., initialized from pe-
              | riodic initial condition. The time-periodic initial condition ensures transients are not introduced
              | at the beginning of the simulation; the result is a seamless transition between periods, as would
              | be experienced in-flight, and trusted integrated quantities of interest. Snapshots taken at times
              | t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                              287
blank         | 
              | 
              | 
text          |                 102
blank         | 
text          |     2
              | u(Nt ) ‚àí u0
blank         | 
text          |               10‚àí2
blank         | 
              | 
text          |               10‚àí6
blank         | 
              | 
text          |               10‚àí10
              |                        100         101        102    100         101        102    100         101        102
              |                       iterations (primal solves)    iterations (primal solves)    iterations (primal solves)
blank         | 
              | 
text          | Figure D.20: Convergence comparison for numerical solvers for fully discrete time-periodically con-
              | strained partial differential equations (D.52), (D.54), nonlinearly preconditioned with m fixed point
              | iterations. Left: m = 0, middle: m = 1, right: m = 5. Solvers: fixed point iteration (       ), steep-
              | est decent (     ), L-BFGS (      ), Newton-GMRES: ‚àÜ = 10‚àí2 (          ), ‚àÜ = 10‚àí3 (     ), ‚àÜ = 10‚àí4
              | (    ), where ‚àÜ is the GMRES convergence tolerance. The optimization algorithms (steepest decent
              | and L-BFGS) were not included in the m = 0 study due to lack of convergence issues.
blank         | 
              | 
text          | solution, a reasonable initial guess is the steady-state flow. Since the problem under consideration
              | is being forced by an input‚Äîthe periodic motion of the foil‚Äîa mechanism for improving the initial
              | guess is to simulate the flow field for m periods of the foil motion and use the final state of the
              | final period as the initial guess. This corresponds to using m iterations of fixed point iteration
              | (Algorithm 21) as a nonlinear preconditioner for the nonlinear system of equations (D.56) that
              | enforces time-periodicity of the flow.
              |       Figure D.20 and Table D.6 compare the solvers under consideration for different levels of nonlinear
              | preconditioning. Regardless of nonlinear preconditioning, the Newton-GMRES solver converges most
              | rapidly for a range of linear system tolerances from 10‚àí2 to 10‚àí4 and the optimization algorithms
              | (steepest decent and L-BFGS) converge most slowly. In fact, without any nonlinear preconditioning
              | the optimization algorithms fail to make progress toward the optimal solution and were not included
              | in the figure. Nonlinear preconditioning helps the Newton-GMRES algorithm most substantially,
              | particularly with m = 5, as this appears to place the initial guess close enough to the solution that
              | quadratic convergence is obtained from the outset. This causes the number of Newton iterations to
              | be reduced from 8 or 9 to 3 or 4. From Table D.6, this does not save many primal solvers‚Äîsince
              | the nonlinear preconditioning requires primal solves‚Äîbut requires far fewer linear system solves and
              | therefore fewer sensitivity solutions. Figure D.21 isolates the Newton-GMRES solver (for m = 0, i.e.,
              | the case without preconditioning) to highlight convergence rates for different GMRES tolerances. It
              | also shows the convergence of GMRES for each nonlinear iteration and each tolerance considered.
              | As expected, more GMRES iterations are required near convergence as it becomes more difficult to
              | reduce the linear residual the prescribed orders of magnitude.
              | Table D.6: Table summarizing performance of numerical solvers for fully discrete time-periodic partial differential equations, considering
              | nonlinear preconditioning via m fixed point iterations.
blank         | 
text          |             m=0                       u(Nt ) ‚àí u0   2
              |                                                         Primal Solves (D.55)   Sensitivity Solves (D.63)   Adjoint Solves (D.25)
              |             Fixed Point Iteration       8.10e-07                90                        0                         0
              |             Newton-Krylov (10‚àí2 )       4.41e-08                9                        128                        0
              |             Newton-Krylov (10‚àí3 )       1.60e-08                8                        156                        0
              |             Newton-Krylov (10‚àí4 )       4.85e-10                8                        220                        0
              |             m=1                       u(Nt ) ‚àí u0   2
              |                                                         Primal Solves (D.55)   Sensitivity Solves (D.63)   Adjoint Solves (D.25)
              |             Fixed Point Iteration      8.10e-07                  90                       0                         0
              |             Steepest Decent            6.09e+00                 121                       0                        121
              |             L-BFGS                     1.36e+00                 121                       0                        121
              |             Newton-Krylov (10‚àí2 )      1.96e-08                  8                       104                        0
              |             Newton-Krylov (10‚àí3 )      2.69e-08                  7                       116                        0
              |             Newton-Krylov (10‚àí4 )      1.77e-09                  7                       149                        0
              |             m=5                       u(Nt ) ‚àí u0   2
              |                                                         Primal Solves (D.55)   Sensitivity Solves (D.63)   Adjoint Solves (D.25)
              |             Fixed Point Iteration       8.10e-07                 90                       0                         0
              |             Steepest Decent             4.65e-01                125                       0                        125
              |             L-BFGS                      7.40e-02                125                       0                        125
meta          |             Newton-Krylov (10‚àí2 )       3.50e-08                 10                      92                         0
              |             Newton-Krylov (10‚àí3 )       7.18e-08                 9                       88                         0
              |             Newton-Krylov (10‚àí4 )       5.61e-09                 9                       121                        0
              |                                                                                                                                              APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION
              |                                                                                                                                              288
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                             289
blank         | 
              | 
              | 
              | 
text          |                 102                                                                 102
blank         | 
              | 
text          |               10‚àí1                                                                10‚àí1
              |     2
blank         | 
              | 
              | 
              | 
text          |                                                                      kJ x ‚àí rk2
              | u(Nt ) ‚àí u0
blank         | 
              | 
              | 
              | 
text          |               10‚àí4                                                                10‚àí4
blank         | 
              | 
text          |               10‚àí7                                                                10‚àí7
blank         | 
              | 
text          |               10‚àí10                                                               10‚àí10
              |                           0        2       4      6      8                                0         20          40
              |                               iterations (primal solves)                                  iterations (linearized solves)
blank         | 
              | 
text          | Figure D.21: Linear and nonlinear convergence of Newton-GMRES method for determining fully
              | discrete time-periodic solutions with various linear system tolerances, ‚àÜ, i.e., kJ x ‚àí Rk < ‚àÜ, where
              | r and J are defined in (D.61) and (D.62). Tolerances considered: ‚àÜ = 10‚àí2 (         ), ‚àÜ = 10‚àí3 (  ),
              |         ‚àí4
              | ‚àÜ = 10 (        ).
blank         | 
              | 
text          |       The time history of the instantaneous quantities of interest in Figure D.22 illustrate the non-
              | physical transients that result from initializing the flow with the steady-state solution. While the
              | transients mostly vanish after a single Newton iteration, the trajectories of these quantities of inter-
              | est do not coincide with those of the true time-periodic solution. The error between the integrated
              | quantities of interest‚ÄîW and Jx ‚Äîat the time-periodic flow versus intermediate iterations is shown
              | in Figure D.23. Comparing Figures D.20 and D.23, it can be seen that a tolerance of 10‚àí8 on
              |  u(Nt ) ‚àí u(0)            2
              |                               leads to an accuracy of 10‚àí6 in the integrated quantities of the time-periodic solu-
              | tion.
              |                                                                                                                     ‚àÇu(Nt )
              |       Next, the stability of the periodic orbit is verified by considering the eigenvalues of                               ,
              |                                                                                                                      ‚àÇu0
blank         | 
              | 
text          |                0                                                                     0
blank         | 
text          |               ‚àí2                                                                  ‚àí0.5
              |                                                                      Fxh
              |    Ph
blank         | 
              | 
              | 
              | 
text          |                                                                                    ‚àí1
              |               ‚àí4
              |                                                                                   ‚àí1.5
              |                       0             2           4                                         0         2           4
              |                                      time                                                            time
blank         | 
text          | Figure D.22: Time history of power, Fxh (u, ¬µ, t), and x-directed force, P h (u, ¬µ, t), after k Newton-
              | GMRES iterations (linear system convergence tolerance ‚àÜ = 10‚àí2 ) starting from steady-state.
              | Values of k: 0 (  ), 1 (    ), and 8 (    ).
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                             290
blank         | 
              | 
              | 
              | 
text          |                                                                        10‚àí1
              |          10‚àí1
              | |W ‚àí W ‚àó |
blank         | 
              | 
              | 
              | 
text          |                                                              |Jx ‚àí Jx‚àó |
              |          10‚àí3                                                          10‚àí3
blank         | 
              | 
text          |          10‚àí5                                                          10‚àí5
blank         | 
              | 
text          |          10‚àí7                                                          10‚àí7
              |                 0      2         4        6                                     0   2         4     6
              |                            iteration                                                    iteration
blank         | 
text          | Figure D.23: Convergence of fully discrete quantities of interest to their values at the time-periodic
              | solution, W ‚àó and Jx‚àó , for various solvers, without nonlinear preconditioning. Solvers: Newton-
              | GMRES: ‚àÜ = 10‚àí2 (         ), ‚àÜ = 10‚àí3 (     ), ‚àÜ = 10‚àí4 (     ), where ‚àÜ is the GMRES convergence
              | tolerance.
blank         | 
              | 
text          | evaluated at the time-periodic solution. As discussed in Section D.3.2 and many prior works [47, 112],
              | the periodic orbit is stable if all eigenvalues of this matrix have modulus less than unity. Figure D.24
              | shows that the 200 eigenvalues of largest modulus lie within the unit circle in the complex plane;
              | thus, the periodic orbit is stable for this problem.
blank         | 
text          |                                           1
blank         | 
              | 
text          |                                         0.5
              |                                 =(Œª)
blank         | 
              | 
              | 
              | 
text          |                                           0
blank         | 
              | 
text          |                                        ‚àí0.5
blank         | 
              | 
text          |                                         ‚àí1
              |                                               ‚àí1      ‚àí0.5       0            0.5   1
              |                                                              <(Œª)
blank         | 
              | 
text          |                                                    (Nt )
              | Figure D.24: First 200 eigenvalues ( ) of ‚àÇu‚àÇu0 ‚Äîevaluated at periodic solution‚Äîwith largest mag-
              | nitude. All eigenvalues lie in unit circle, thus the periodic orbit is stable.
blank         | 
text          |       This completes the discussion of the primal time-periodic problem and attention is turned to the
              | dual, or adjoint, problem. First, a brief comparison of two potential solvers‚Äîfixed point iteration
              | and GMRES‚Äîfor the periodic adjoint equation is provided. In contrast to the primal problem, there
              | is a less pronounced difference between the convergence of fixed point iteration and the Krylov solver
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                       291
blank         | 
              | 
              | 
              | 
text          |                     100                                                       100
              |      kAx ‚àí b1 k2
blank         | 
              | 
              | 
              | 
text          |                                                                kAx ‚àí b2 k2
              |                    10‚àí3                                                      10‚àí3
blank         | 
text          |                    10‚àí6                                                      10‚àí6
blank         | 
text          |                    10‚àí9                                                      10‚àí9
              |                           0        50        100        150                         0        50         100       150
              |                       iterations (adjoint linearized solves)                    iterations (adjoint linearized solves)
blank         | 
              | 
text          | Figure D.25: GMRES convergence for determining solution of adjoint equations corresponding to
              | fully discrete time-periodic partial differential equation, i.e., a linear two-point boundary value prob-
              |                                      ‚àÇW                  ‚àÇJx
              | lem. A defined in (D.81), b1 =              , and b2 =            from (D.80), where W is fully discrete
              |                                    ‚àÇu(Nt )              ‚àÇu(Nt )
              | approximation of the total work done by fluid on airfoil and Jx is the x-directed impulse. Solvers:
              | fixed point iteration (     ) and GMRES (           ). The linearization is performed about the time-
              | periodic solution obtained with Newton-Krylov (‚àÜ = 10‚àí4 ) method.
blank         | 
              | 
text          | in the dual problem. Figure D.25 shows the convergence history for two different right-hand sides
              | of Ax = b, each corresponding to the adjoint method for a different quantity of interest. However,
              | it should be noted that the iterations for the GMRES solver are cheaper than those of the fixed
              |                           ‚àÇF
              | point solver as the terms    ‚Äîwhich may be expensive if ¬µ is a large vector‚Äîare not computed.
              |                           ‚àÇ¬µ
              | Therefore, the GMRES algorithm is superior to fixed point iterations as there are fewer required
              | iterations, each of which is cheaper.
              |    Finally, the adjoint method for computing gradients of quantities of interest on the manifold of
              | time-periodic solutions of the partial differential equations is verified against a second-order finite
              | difference approximations. The finite difference approximation to gradients on the aforementioned
              | manifold requires finding the time-periodic solution of the governing equations at perturbations about
              | the nominal parameter configuration in (D.89). Figure D.26 shows the relative error between the
              | gradients computed via the adjoint method in Algorithm 24 to this finite difference approximation
              | for a sweep of finite difference intervals, œÑ . To realize the sub-10‚àí6 finite difference errors in the
              | time-periodic gradient, tolerances of 10‚àí12 were used for the primal and dual time-periodic solutions.
              | As expected, the error starts to increase after œÑ drops too small due to the trade-off between finite
              | difference accuracy and round-off error.
              |    Given this exposition on solvers for time-periodically constrained partial differential equations,
              | we turn our attention to deriving the corresponding fully discrete adjoint equations.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                      292
blank         | 
              | 
              | 
              | 
text          |        10‚àí3
blank         | 
              | 
              | 
              | 
text          |                                                              2
              |    2
blank         | 
              | 
              | 
              | 
text          |                                                         ‚àÜJx
              |  ‚àÜW
blank         | 
              | 
              | 
              | 
text          |                                                         ‚àÜ¬µ
              |  ‚àÜ¬µ
blank         | 
              | 
              | 
              | 
text          |                                                                  10‚àí4
              |             ‚àí4
              |        10
blank         | 
              | 
              | 
              | 
text          |                                                         /
              |  /
blank         | 
              | 
              | 
              | 
text          |                                                              2
              |    2
blank         | 
              | 
              | 
              | 
text          |                                                         ‚àÜJx
              |  ‚àÜW
              |  ‚àÜ¬µ
blank         | 
              | 
              | 
              | 
text          |                                                         ‚àÜ¬µ
              |        10‚àí5                                                      10‚àí5
              |  ‚àí
blank         | 
              | 
              | 
              | 
text          |                                                         ‚àí
              |                                                         dJx
              |  dW
              |  d¬µ
blank         | 
              | 
              | 
              | 
text          |                                                         d¬µ
              |        10‚àí6
              |                                                                  10‚àí6
              |                  10‚àí810‚àí710‚àí610‚àí510‚àí410‚àí310‚àí2                           10‚àí810‚àí710‚àí610‚àí510‚àí410‚àí310‚àí2
              |                                œÑ                                                      œÑ
blank         | 
              | 
text          | Figure D.26: Verification of periodic adjoint-based gradient with second-order centered finite dif-
              | ference approximation, for a range of finite intervals, œÑ . The computed gradient match the finite
              | difference approximation to nearly 7 digits before round-off errors degrade the accuracy.
blank         | 
              | 
              | 
              | 
text          | Table D.7: Comparison of non-zero derivatives of total energy, W , and x-impulse, Jx , computed
              | with the adjoint method and a second-order finite difference approximation with step size œÑ = 10‚àí6 .
blank         | 
text          |                              ‚àÇW                 ‚àÇW                      ‚àÇW               ‚àÇW
              |                              ‚àÇAh                ‚àÇœâh                     ‚àÇAŒ∏              ‚àÇcŒ∏
blank         | 
text          |  Adjoint                -2.30919016e+01   -2.593579090e+01       -7.99568107e+00   5.881595017e-01
              |  Finite difference      -2.30919013e+01   -2.593579395e+01       -7.99568151e+00   5.881594917e-01
              |                              ‚àÇJx                ‚àÇJx                     ‚àÇJx              ‚àÇJx
              |                              ‚àÇAh                ‚àÇœâh                     ‚àÇAŒ∏              ‚àÇcŒ∏
blank         | 
meta          |  Adjoint                -1.85436790e-01   -1.029830753e-01       6.72970822e+00    1.270106907e-02
              |  Finite difference      -1.85436774e-01   -1.029834126e-01       6.72970891e+00    1.270112956e-02
              | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                                     293
blank         | 
              | 
              | 
title         | D.4.5       Numerical Experiment: Energetically Optimal Flapping with Thrust
              |             and Time-Periodicity Constraints
text          | This section will apply the novel, fully discrete, periodic adjoint method to solve an optimal con-
              | trol problem governed by the time-periodically constrained isentropic compressible Naiver-Stokes
              | equations. The system of PDEs is discretized using a nodal discontinuous Galerkin (DG) method
              | on unstructured meshes of triangles, with polynomial degrees 3 within each element. The viscous
              | fluxes are chosen according to the compact DG method [150] method, and our implementation
              | is fully implicit with exact Jacobian matrices and a range of parallel iterative solvers [153]. The
              | resulting semi-discrete system has the form of our general system of ODEs (D.13). All partial
              | derivatives of the semi-discrete governing equations and corresponding quantities of interest, namely
              | ‚àÇr ‚àÇr ‚àÇfh ‚àÇfh
              |     ,    ,      ,     are computed via automatic symbolic differentiation at the element-level with
              | ‚àÇu ‚àÇ¬µ ‚àÇu ‚àÇ¬µ
              | the MAPLE software [126] and subsequent
              |                                    Z           assembly. The semi-discrete quantity of interest fh is
              | defined as the approximation of           f (U , ¬µ, t) dS in (D.51) using the DG shape functions and
              |                                              Œì(¬µ, t)
              | required, along with the temporal discretization scheme, to compute the discrete output functional
              | F in (D.53). Additional details regarding computation of the partial derivatives with respect to ¬µ
              | in the case of a parametrized, deforming domain are provided in Section D.2.4 and [211].
blank         | 
              | 
text          |                                                                            l
              |                                                                l/3
blank         | 
text          |                                                Œ∏(t)
blank         | 
text          |                                                       x(t)
              |                                                               y(t)
blank         | 
              | 
text          |     Figure D.27: Kinematic description of body under consideration, NACA0012 airfoil (right).
blank         | 
text          |     The remainder of this section will consider the time-periodic solution and optimization of a
              | flapping NACA0012 airfoil, shown in Figure D.27. Two quantities of interest that will be considered
              | are the total work exerted by the fluid on the airfoil, W, and the impulse in the x-direction imparted
              | on the airfoil by the fluid, Jx , which take the form
              |                Z     T   Z                                                          Z       T   Z
              |  W(U , ¬µ) =                  f (U , ¬µ, t)¬∑ xÃá dS dt          and     Jx (U , ¬µ) =                   f (U , ¬µ, t)¬∑e1 dS dt (D.91)
              |                  0       Œì                                                              0       Œì
blank         | 
              | 
text          | In this case, Œì is the surface of the airfoil, e1 ‚àà Rnsd is the 1st canonical unit vector, f (U , ¬µ, t) ‚àà Rnsd
              | is the instantaneous force that the fluid exerts on the airfoil, and xÃá is the pointwise velocity of airfoil.
              | The solver-consistent discretization, discussed in Section 2.1.4 and [211], of these quantities results in
              |                                                                      (1)       (Nt )                                         (1)         (Nt )
              | the fully discrete approximations W (u(0) , . . . , u(Nt ) , k1 , . . . , ks           , ¬µ) and Jx (u(0) , . . . , u(Nt ) , k1 , . . . , ks      , ¬µ).
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                 294
blank         | 
              | 
              | 
text          | The instantaneous quantities of interest corresponding to those in (D.91) are the power and x-
              | directed force the fluid exerts on the airfoil, which take the form
              |                        Z                                                   Z
              |          P(U , ¬µ, t) =    f (U , ¬µ, t) ¬∑ xÃá dS   and       Fx (U , ¬µ, t) =   f (U , ¬µ, t) ¬∑ e1 dS.
              |                           Œì                                                             Œì
blank         | 
              | 
text          | Define P h (u, ¬µ, t) and Fxh (u, ¬µ, t) as the solver-consistent semi-discretization of these instantaneous
              | quantities of interest.
              |    In this section, the periodic adjoint method is used to solve an optimal control problem with time-
              | periodicity constraints using gradient-based optimization techniques. The optimization problem is
              | to determine the energetically optimal flapping motion of the NACA0012 airfoil in isentropic, viscous
              | flow‚Äîover a single representative, in-flight period‚Äîsuch that the x-directed impulse on the body is
              | identically 0. The continuous form of the optimal control problem is given as
blank         | 
text          |                             minimize          W(U , ¬µ)
              |                                U, ¬µ
blank         | 
text          |                             subject to Jx (U , ¬µ) = 0
              |                                                                                                           (D.92)
              |                                               U (x, 0) = U (x, T )
              |                                               ‚àÇU
              |                                                  + ‚àá ¬∑ F (U , ‚àáU ) = 0            in ‚Ñ¶(¬µ, t).
              |                                               ‚àÇt
blank         | 
text          | After spatial and temporal discretization via the high-order discontinuous Galerkin and diagonally
              | implicit Runge-Kutta schemes in Section 2.1.3, the continuous optimization problem in (D.92) is
              | replaced with its fully discrete counterpart
blank         | 
text          |                                                                            (1)
              |                         minimize              W (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ)
              |                   u(0) , ..., u(Nt ) ‚ààRNu ,
              |                    (1)
              |                   k1 , ..., ks(Nt ) ‚ààRNu ,
              |                            ¬µ‚ààRN¬µ
              |                                                                            (1)
              |                   subject to                  Jx (u(0) , . . . , u(Nt ) , k1 , . . . , ks(Nt ) , ¬µ) = 0
              |                                               u(0) = u(Nt )                                               (D.93)
              |                                                                   s
              |                                                                             (n)
              |                                                                   X
              |                                               u(n) = u(n‚àí1) +           bi ki
              |                                                                   i=1
blank         |                                                                                    
text          |                                                  (n)            (n)
              |                                               M ki     = ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn .
blank         | 
text          | The physical and numerical setup are identical to that in Section D.4.4 with the exception of the
              | kinematic parametrization. Instead of a single Fourier mode, the kinematic motion is parametrized
              | by cubic splines with 5 equally spaced knots and boundary conditions that enforce
blank         | 
text          |                                               h(¬µ, t) = ‚àíh(¬µ, t + T /2)
              |                                                                                                           (D.94)
              |                                                Œ∏(¬µ, t) = ‚àíŒ∏(¬µ, t + T /2)
blank         | 
text          | where t is time and T = 5 is the fixed period of the flapping motion. The vector of parameters,
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                     295
blank         | 
              | 
              | 
text          | ¬µ‚Äîused as optimization parameters‚Äîare the knots of the cubic splines. This leads to N¬µ = 8 pa-
              | rameters; 4 knots for the motion of h(¬µ, t) and Œ∏(¬µ, t)2 . Notice that (D.94) enforces the trajectories
              | of h(¬µ, t) and Œ∏(¬µ, t) in [T /2, T ] to be the mirror of those in [0, T /2], which implicitly enforces
              | periodicity with period T . The mapping G from the reference to physical domain required for the
              | DG-ALE formulation is defined in (D.90) with the new definition of h(¬µ, t) and Œ∏(¬µ, t) with periodic
              | cubic splines.
              |     The optimization problem in (D.93) is solved using the extension of the nested framework for
              | PDE-constrained optimization, or generalized reduced-gradient method, introduced in Section D.4.3.
              | The solvers introduced in Section D.3.1 will be used to determine the time-periodic flow around the
              | airfoil. Given the results in the previous section, the Newton-GMRES method with a tolerance of
              | ‚àÜ = 10‚àí3 , warm-started from m = 5 fixed-point iterations is employed. The flow is deemed to be
              | periodic if
              |                                              u(0) ‚àí u(Nt )       ‚â§ 10‚àí10 .                                   (D.95)
              |                                                              2
blank         | 
text          | The periodic flow is used to compute quantities of interest‚Äîthe total work and x-impulse. Then,
              | the periodic adjoint method will be used to compute gradients of the quantities of interest along
              | the manifold of time-periodic solutions of the governing equation. GMRES is used to solve the dual
              | linear, periodic adjoint equations with a tolerance of ‚àÜ = 10‚àí4 . Since there are two quantities of
              | interest, two periodic adjoint solves must be performed at each optimization iteration. Finally, the
              | quantities of interest and their gradients are passed to an optimization solver‚ÄîSNOPT [70] is used
              | in this work‚Äîand progress is made toward a local minimum.
              |     The initial condition for the optimization solver is shown in Figure D.28; the heaving motion is
              | a sinusoid with amplitude 1 and there is no pitch‚Äîpure heaving motion. The vorticity snapshots in
              | Figure D.31 show this motion induces a fairly violent flow with shedding vortices. The corresponding
              | time history of the power, P h (u, ¬µ, t), and x-directed force, Fxh (u, ¬µ, t), imparted onto the airfoil
              | by the fluid are shown in Figure D.29. After 16 periodic optimization iterations, the first-order
              | optimality conditions have been reduced by two orders of magnitude. From Figure D.28, the optimal
              | airfoil motion is a combination of heaving and pitching. From the initial guess, the amplitude of
              | the heaving motion has been reduced by more than a factor of two and the pitching amplitude
              | increased to 18.7‚ó¶ . The convergence history for the optimization solver is given in Figure D.30. At
              | the optimal solution, the total work required to perform the flapping motion is more than an order of
              | magnitude smaller than at the initial guess (pure heaving). Figures D.31 and D.32 show snapshots
              | of the flow in time at the initial, purely heaving motion and the optimal flapping motion. From
              | these figures, it is clear that the flow corresponding to the optimal motion is relatively benign with
              | no shedding vortices, which explains the reduction in required work. The efficiency of combined
              | pitching and heaving has been repeatedly observed experimentally [191, 162, 158] and the phase
              | angle of approximately 90‚ó¶ between pitching and heaving, as observed in Figure D.28, has also been
              | observed in experiments [191, 162, 158, 148]. The specific pitching and heaving amplitudes were
              |    2 There are only 4 degrees of freedom since the mirror boundary condition in (D.94) prescribes the value of one of
blank         | 
text          | the knots given the other four.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                     296
blank         | 
              | 
              | 
text          |            1                                                         0.4
              | h(¬µ, t)                                                              0.2
blank         | 
              | 
              | 
              | 
text          |                                                           Œ∏(¬µ, t)
              |            0                                                             0
blank         | 
text          |                                                                     ‚àí0.2
              |           ‚àí1
              |                                                                     ‚àí0.4
              |                    0       2        4                                            0           2            4
              |                             time                                                              time
blank         | 
text          | Figure D.28: Trajectories of h(¬µ, t) and Œ∏(¬µ, t) at initial guess (                  ) and optimal solution (     )
              | for optimization problem in (D.93).
blank         | 
text          |            0.1                                                       0
blank         | 
text          |                0
              |                                                                     ‚àí2
blank         | 
              | 
              | 
              | 
text          |                                                           Fxh
              | Ph
blank         | 
              | 
              | 
              | 
text          |           ‚àí0.1
blank         | 
text          |           ‚àí0.2                                                      ‚àí4
blank         | 
text          |                        0    2           4                                    0             2          4
              |                              time                                                           time
blank         | 
text          | Figure D.29: Time history of the power, P h (u, ¬µ, t), and x-directed force, Fxh (u, ¬µ, t), imparted onto
              | foil by fluid at initial guess ( ) and optimal solution (       ) for optimization problem in (D.93).
blank         | 
              | 
text          | determined by the optimizer such that the thrust constraint is satisfied; if the thrust requirement
              | was increased, these magnitudes would increase and result in a more violent flow field, eventually
              | leading to vortex shedding [191, 211].
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                       297
blank         | 
              | 
              | 
              | 
text          |       0                                                       0.2
blank         | 
text          |                                                                 0
blank         | 
              | 
              | 
              | 
text          |                                                         Jx
              | W
blank         | 
              | 
              | 
              | 
text          |      ‚àí5                                                      ‚àí0.2
blank         | 
text          |                                                              ‚àí0.4
blank         | 
text          |     ‚àí10                                                      ‚àí0.6
              |            0       5       10       15                              0       5       10       15
              |                optimization iteration                                   optimization iteration
blank         | 
text          | Figure D.30: Convergence of quantities of interest, W and Jx , with optimization iteration. Each
              | optimization iteration requires a periodic flow computation and its corresponding adjoint to evaluate
              | the quantities of interest and their gradients.
blank         | 
              | 
              | 
              | 
text          | Figure D.31: Trajectory of airfoil and flow vorticity at initial guess for optimization (pure heaving
              | motion, see Figure D.28). Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0.
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                        298
blank         | 
              | 
              | 
              | 
text          | Figure D.32: Trajectory of airfoil and flow vorticity at energetically optimal, zero-impulse flapping
              | motion (see Figure D.28). Snapshots taken at times t = 0.0, 1.0, 2.0, 3.0, 4.0, 5.0.
blank         | 
              | 
title         | D.5      Conclusion
text          | This appendix discussed a fully discrete framework for computing time-periodic solutions of partial
              | differential equations. The discussion included the spatio-temporal discretization of the governing
              | equations and a slew of time-periodic shooting solvers, including optimization-based and Newton-
              | Krylov methods. These shooting methods consider the state at the final time to be a nonlinear
              | function of the initial condition and solve u(Nt ) (u0 ) = u0 using Newton-Raphson iterations or
              | optimization techniques to minimize its norm. The linear system of equations, arising in the Newton-
              | Raphson iterations, were solved using matrix-free GMRES with matrix-vector products computed
              | as the solution of the linearized, sensitivity equations (with appropriate initial condition). The
              | adjoint method was used to compute the gradients in the gradient-based optimization solvers. These
              | periodic solvers were used to compute the time-periodic flow around a flapping airfoil in isentropic,
              | compressible, viscous flow, and their performance compared. The Newton-Krylov solver exhibits
              | superior convergence to the optimization-based shooting methods, even when inexact tolerances
              | were used on the linear system solves, and fully leverages quality starting guesses. An eigenvalue
              | analysis is provided to show the periodic orbit of the flapping problem is stable.
              |    The main contribution of the document is the derivation of the adjoint equations corresponding
              | to the fully discrete time-periodically constraint partial differential equations. As opposed to the
              | backward-in-time evolution equations, these equations constitute a linear, two-point boundary value
              | problem that is provably solvable. The corresponding adjoint method was introduced for computing
              | exact gradients of quantities of interest along the manifold of time-periodic solutions of the discrete
              | conservation law. The gradients were verified against a second-order finite difference approxima-
              | tion. These quantities of interest and their gradients were used in the context of gradient-based
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                         299
blank         | 
              | 
              | 
text          | optimization to solve an optimal control problem with time-periodicity constraints, among others.
              | In particular, the energetically optimal flapping motion of a 2D airfoil in time-periodic, isentropic,
              | compressible, viscous flow that generates a prescribed time-averaged thrust is sought. The proposed
              | framework improves the nominal flapping motion by reducing the flapping energy nearly an order
              | of magnitude and exactly satisfies the thrust constraint.
              |    While this work is an initial step toward problems of engineering and scientific relevance, addi-
              | tional development will be required to solve truly impactful problems. One extension of this work is
              | the development of robust solvers for determining nearly time-periodic solutions of problems where
              | a time-periodic solution does not exist, but exhibits quasi-cyclic behavior. An example of such a
              | problem is the 3D turbulent flow around periodically driven bodies such as helicopter and windmill
              | blades. Another extension will be the development of faster numerical solvers to reduce the cost
              | of computing time-periodic solutions or solving optimization problems with time-periodicity con-
              | straints. For example, economical, matrix-free preconditioners could result in non-trivial speedups
              | for the Newton-Krylov time-periodicity solver and Krylov solver for the periodic adjoint equations.
              | Model order reduction techniques could dramatically reduce the cost of computing the solution of
              | the primal partial differential equations, and consequently the entire time-periodic solver.
blank         | 
              | 
title         | D.6      Existence and Uniqueness of Solutions of the Adjoint
text          |          Equations of the Fully Discrete, Time-Periodically Con-
              |          strained Partial Differential Equations
              | This section proves existence and uniqueness of solutions of the adjoint equations of the fully dis-
              | crete, time-periodically constrained partial differential equation. The strategy is to show the linear
              | operator that encapsulates them is the transpose of the linear operator that defines the fully discrete,
              | sensitivity equations, which is assumed non-singular at a time-periodic solution.
              |    Consider the initial-value problem (D.55), with the initial condition parametrized by ¬µ,
blank         | 
text          |                                  u(0) = u0 (¬µ)
              |                                                    s
              |                                                              (n)
              |                                                    X
              |                                  u(n) = u(n‚àí1) +         bi ki                                   (D.96)
              |                                                    i=1
blank         |                                                                     
text          |                                   (n)            (n)
              |                               M ki      = ‚àÜtn r ui , ¬µ, tn‚àí1 + ci ‚àÜtn .
blank         | 
text          | The fully discrete adjoint equations corresponding to the primal equation in (D.96) and the discrete
meta          | APPENDIX D. UNSTEADY, PERIODIC PDE-CONSTRAINED OPTIMIZATION                                                           300
blank         | 
              | 
              | 
text          |                                                             (1)            (Nt )
              | quantity of interest, F (u(0) , . . . , u(Nt ) , k1 , . . . , ks                   , ¬µ) are
blank         | 
text          |                                 ‚àÇF T
              |               ŒΩ (Nt ) =
              |                                ‚àÇu(Nt )
              |                                                              s
              |                  (n‚àí1)          (n)       ‚àÇF T X           ‚àÇr  (n)                     T
              |                                                                                             (n)
              |              ŒΩ           =ŒΩ           +         +     ‚àÜt n     ui   , ¬µ, tn‚àí1 + ci ‚àÜt n    œÑi                      (D.97)
              |                                         ‚àÇu(n‚àí1)   i=1
              |                                                            ‚àÇu
              |                                         T                  s
              |                    (n)          ‚àÇF                         X               ‚àÇr  (n)                 T
              |                                                                                                        (n)
              |           MT œÑi          =        (n)
              |                                             + bi ŒΩ (n) +         aji ‚àÜtn       uj , ¬µ, tn‚àí1 + cj ‚àÜtn œÑ j ,
              |                                ‚àÇki                         j=i
              |                                                                            ‚àÇu
blank         | 
text          | and the gradient of the quantity of interest can be reconstructed as
blank         | 
text          |                                             Nt      s
              |                    dF   ‚àÇF         T ‚àÇu0   X       X    (n) T ‚àÇr   (n)
              |                       =    + ŒΩ (0)       +     ‚àÜtn     œÑi        (ui , ¬µ, tn‚àí1 + ci ‚àÜtn ),                         (D.98)
              |                    d¬µ   ‚àÇ¬µ           ‚àÇ¬µ    n=1     i=1
              |                                                               ‚àÇ¬µ
blank         | 
text          |                          (n)
              | where ŒΩ (n) and œÑ i            are the Lagrange multipliers. These equations can be obtained using an identical
              | derivation to that in Section D.4.1; see [211]. At this point, take F = v T u(Nt ) and ¬µ = u0 for a
              | fixed, arbitrary vector v ‚àà RNu . For this selection of F and ¬µ, the above equations reduce to
blank         | 
text          |                                ŒΩ (Nt ) = v
              |                                                     s
              |                                                     X             ‚àÇr  (n)                 T
              |                                                                                               (n)
              |                            ŒΩ (n‚àí1) = ŒΩ (n) +               ‚àÜtn        ui , ¬µ, tn‚àí1 + ci ‚àÜtn œÑ i
              |                                                     i=1
              |                                                                   ‚àÇu                                               (D.99)
              |                                                        s
              |                                  (n)
              |                                                       X                  ‚àÇr  (n)                 T
              |                                                                                                      (n)
              |                          MT œÑi          = bi ŒΩ (n) +         aji ‚àÜtn         uj , ¬µ, tn‚àí1 + cj ‚àÜtn œÑ j
              |                                                        j=i
              |                                                                          ‚àÇu
blank         | 
text          | and
              |                                                                                T
              |                                                        dF T   ‚àÇu(Nt )
              |                                                             =         v = ŒΩ (0) .                                 (D.100)
              |                                                        d¬µ      ‚àÇu0
blank         | 
text          |                                                                                                     ‚àÇŒª(0)
              | The equations in (D.99) defining ŒΩ (0) are identical to those in (D.79) defining                          , which leads to
              |                                                                                                     ‚àÇŒªNt
              | the relation
              |                                                                      T
              |                                                            ‚àÇu(Nt )    ‚àÇŒª(0)
              |                                                                    v=       v                                     (D.101)
              |                                                             ‚àÇu0       ‚àÇŒªNt
              | for any v. Thus, it can be concluded that
blank         | 
text          |                                                                                        T
              |                                                              ‚àÇŒª(0)   ‚àÇu(Nt )
              |                                                                    =         .                                    (D.102)
              |                                                              ‚àÇŒªNt     ‚àÇu0
blank         | 
text          |                                                     ‚àÇu(Nt )
              | Since the Jacobian of the time-periodic residual,           ‚àí I, is non-singular at a time-periodic
              |                                                      ‚àÇu0
              |                                                                                ‚àÇŒª(0)
              | solution, the matrix defining the linear, two-point boundary value problem,          ‚àí I must also
              |                                                                                ‚àÇŒªNt
              | be non-singular. Thus, a solution of the linear, two-point boundary value problem exists and is
              | unique.
title         | Bibliography
blank         | 
ref           |  [1] Anshul Agarwal and Lorenz T Biegler. A trust-region framework for constrained optimization
              |     using reduced order modeling. Optimization and Engineering, 14(1):3‚Äì35, 2013.
blank         | 
ref           |  [2] Volkan Akcelik, George Biros, Omar Ghattas, Judith Hill, David Keyes, and Bart van Bloe-
              |     men Waanders. Parallel algorithms for PDE-constrained optimization. Parallel Processing for
              |     Scientific Computing, 20:291, 2006.
blank         | 
ref           |  [3] Roger Alexander. Diagonally implicit Runge-Kutta methods for stiff ODE‚Äôs. SIAM Journal
              |     on Numerical Analysis, 14(6):1006‚Äì1021, 1977.
blank         | 
ref           |  [4] Natalia M Alexandrov, John E Dennis Jr, Robert Michael Lewis, and Virginia Torczon. A
              |     trust-region framework for managing the use of approximation models in optimization. Struc-
              |     tural Optimization, 15(1):16‚Äì23, 1998.
blank         | 
ref           |  [5] Natalia M Alexandrov and Robert Michael Lewis. An overview of first-order model manage-
              |     ment for engineering optimization. Optimization and Engineering, 2(4):413‚Äì430, 2001.
blank         | 
ref           |  [6] Natalia M Alexandrov, Robert Michael Lewis, Clyde R Gumbert, Lawrence L Green, and
              |     Perry A Newman. Approximation and model management in aerodynamic optimization with
              |     variable-fidelity models. Journal of Aircraft, 38(6):1093‚Äì1101, 2001.
blank         | 
ref           |  [7] David M Ambrose and Jon Wilkening.          Computation of time-periodic solutions of the
              |     Benjamin-Ono equation. arXiv preprint arXiv:0804.3623, 2008.
blank         | 
ref           |  [8] David M Ambrose and Jon Wilkening. Computation of symmetric, time-periodic solutions
              |     of the vortex sheet with surface tension. Proceedings of the National Academy of Sciences,
              |     107(8):3361‚Äì3366, 2010.
blank         | 
ref           |  [9] George R Anderson, Michael J Aftosmis, and Marian Nemec. Parametric deformation of
              |     discrete geometry for aerodynamic shape design. AIAA Paper, 965, 2012.
blank         | 
ref           | [10] Eyal Arian, Marco Fahl, and Ekkehard W Sachs. Trust-region proper orthogonal decomposi-
              |     tion for flow control. Technical report, DTIC Document, 2000.
blank         | 
              | 
              | 
              | 
meta          |                                               301
              | BIBLIOGRAPHY                                                                                       302
blank         | 
              | 
              | 
ref           | [11] Douglas N Arnold, Franco Brezzi, Bernardo Cockburn, and L Donatella Marini. Unified
              |     analysis of discontinuous Galerkin methods for elliptic problems. SIAM Journal on Numerical
              |     Analysis, 39(5):1749‚Äì1779, 2002.
blank         | 
ref           | [12] Ivo BabusÃåka, Fabio Nobile, and RauÃÅl Tempone. A stochastic collocation method for elliptic
              |     partial differential equations with random input data. SIAM Review, 52(2):317‚Äì355, 2010.
blank         | 
ref           | [13] Ivo Babuska, RauÃÅl Tempone, and Georgios E Zouraris. Galerkin finite element approximations
              |     of stochastic elliptic partial differential equations. SIAM Journal on Numerical Analysis,
              |     42(2):800‚Äì825, 2004.
blank         | 
ref           | [14] Maciej Balajewicz and Earl H Dowell. Stabilization of projection-based reduced order models
              |     of the navier‚Äìstokes. Nonlinear Dynamics, 70(2):1619‚Äì1632, 2012.
blank         | 
ref           | [15] Afonso S Bandeira, Katya Scheinberg, and Luƒ±ÃÅs N Vicente. Convergence of trust-region meth-
              |     ods based on probabilistic models. SIAM Journal on Optimization, 24(3):1238‚Äì1264, 2014.
blank         | 
ref           | [16] Jernej BarbicÃå and Doug L James. Real-time subspace integration for St. Venant-Kirchhoff
              |     deformable models. In ACM transactions on graphics (TOG), volume 24, pages 982‚Äì990.
              |     ACM, 2005.
blank         | 
ref           | [17] Maxime Barrault, Yvon Maday, Ngoc Cuong Nguyen, and Anthony T Patera. An empirical
              |     interpolation method: application to efficient reduced-basis discretization of partial differential
              |     equations. Comptes Rendus Mathematique, 339(9):667‚Äì672, 2004.
blank         | 
ref           | [18] Volker Barthelmann, Erich Novak, and Klaus Ritter. High dimensional polynomial interpola-
              |     tion on sparse grids. Advances in Computational Mathematics, 12(4):273‚Äì288, 2000.
blank         | 
ref           | [19] Ted Belytschko, Wing Kam Liu, Brian Moran, and Khalil Elkhodary. Nonlinear Finite Ele-
              |     ments for Continua and Structures. John wiley & sons, 2013.
blank         | 
ref           | [20] Martin Philip Bendsoe and Ole Sigmund. Topology Optimization: Theory, Methods, and
              |     Applications. Springer Science & Business Media, 2013.
blank         | 
ref           | [21] Gal Berkooz, Philip Holmes, and John L Lumley. The proper orthogonal decomposition in the
              |     analysis of turbulent flows. Annual Review of Fluid Mechanics, 25(1):539‚Äì575, 1993.
blank         | 
ref           | [22] A Borzƒ±ÃÄ, V Schulz, C Schillings, and G Von Winckel. On the treatment of distributed uncer-
              |     tainties in PDE-constrained optimization. GAMM-Mitteilungen, 33(2):230‚Äì246, 2010.
blank         | 
ref           | [23] Alfio Borzƒ±ÃÄ. Multigrid and sparse-grid schemes for elliptic control problems with random
              |     coefficients. Computing and Visualization in Science, 13(4):153‚Äì160, 2010.
blank         | 
ref           | [24] Alfio Borzƒ±ÃÄ and G von Winckel. Multigrid methods and sparse-grid collocation techniques
              |     for parabolic optimal control problems with random coefficients. SIAM Journal on Scientific
              |     Computing, 31(3):2172‚Äì2192, 2009.
meta          | BIBLIOGRAPHY                                                                                   303
blank         | 
              | 
              | 
ref           | [25] Matthew Brand. Incremental singular value decomposition of uncertain data with missing
              |     values. In European Conference on Computer Vision, pages 707‚Äì720. Springer, 2002.
blank         | 
ref           | [26] Matthew Brand. Fast low-rank modifications of the thin singular value decomposition. Linear
              |     Algebra and its Applications, 415(1):20‚Äì30, 2006.
blank         | 
ref           | [27] Martin Dietrich Buhmann. Radial basis functions. Acta Numerica 2000, 9:1‚Äì38, 2000.
blank         | 
ref           | [28] T. Bui-Thanh, K. Willcox, and O. Ghattas. Model reduction for large-scale systems with high-
              |     dimensional parametric input space. SIAM Journal on Scientific Computing, 30(6):3270‚Äì3288,
              |     2008.
blank         | 
ref           | [29] Hans-Joachim Bungartz and Michael Griebel. Sparse grids. Acta Numerica, 13:147‚Äì269, 2004.
blank         | 
ref           | [30] Yanzhao Cao, MY Hussaini, and HONGTAO Yang. Numerical optimization of radiated engine
              |     noise with uncertain wavenumbers. International Journal of Numerical Analysis and Modeling,
              |     4(3-4):392‚Äì401, 2007.
blank         | 
ref           | [31] Kevin Carlberg, Charbel Bou-Mosleh, and Charbel Farhat. Efficient non-linear model reduc-
              |     tion via a least-squares petrov‚Äìgalerkin projection and compressive tensor approximations.
              |     International Journal for Numerical Methods in Engineering, 86(2):155‚Äì181, 2011.
blank         | 
ref           | [32] Kevin Carlberg and Charbel Farhat. A low-cost, goal-oriented compact proper orthogonal
              |     decompositionbasis for model reduction of static systems. International Journal for Numerical
              |     Methods in Engineering, 86(3):381‚Äì402, 2011.
blank         | 
ref           | [33] Kevin Thomas Carlberg. Model reduction of nonlinear mechanical systems via optimal projec-
              |     tion and tensor approximation. PhD thesis, Stanford University, 2011.
blank         | 
ref           | [34] Richard G Carter. Numerical optimization in Hilbert space using inexact function and gradient
              |     evaluations. 1989.
blank         | 
ref           | [35] Richard G Carter. On the global convergence of trust region algorithms using inexact gradient
              |     information. SIAM Journal on Numerical Analysis, 28(1):251‚Äì265, 1991.
blank         | 
ref           | [36] Richard G Carter. Numerical experience with a class of algorithms for nonlinear optimization
              |     using inexact function and gradient information. SIAM Journal on Scientific Computing,
              |     14(2):368‚Äì388, 1993.
blank         | 
ref           | [37] ASL Chan. The design of Michell optimum structures. Technical report, College of Aeronautics
              |     Cranfield, 1960.
blank         | 
ref           | [38] Tony F Chan and Wing Lok Wan. Analysis of projection methods for solving linear systems
              |     with multiple right-hand sides. SIAM Journal on Scientific Computing, 18(6):1698‚Äì1721, 1997.
blank         | 
ref           | [39] Peter C Chang and S Chi Liu. Recent research in nondestructive evaluation of civil infrastruc-
              |     tures. Journal of Materials in Civil Engineering, 15(3):298‚Äì304, 2003.
meta          | BIBLIOGRAPHY                                                                                 304
blank         | 
              | 
              | 
ref           | [40] I Charpentier. Checkpointing schemes for adjoint codes: Application to the meteorological
              |     model Meso-NH. SIAM Journal on Scientific Computing, 22(6):2135‚Äì2151, 2001.
blank         | 
ref           | [41] Saifon Chaturantabut and Danny C Sorensen. Nonlinear model reduction via discrete empirical
              |     interpolation. SIAM Journal on Scientific Computing, 32(5):2737‚Äì2764, 2010.
blank         | 
ref           | [42] Peng Chen and Alfio Quarteroni. Weighted reduced basis method for stochastic optimal control
              |     problems with elliptic PDE constraint. SIAM/ASA Journal on Uncertainty Quantification,
              |     2(1):364‚Äì396, 2014.
blank         | 
ref           | [43] Peng Chen and Alfio Quarteroni. A new algorithm for high-dimensional uncertainty quan-
              |     tification based on dimension-adaptive sparse grid approximation and reduced basis methods.
              |     Journal of Computational Physics, 298:176‚Äì193, 2015.
blank         | 
ref           | [44] Peng Chen, Alfio Quarteroni, and Gianluigi Rozza. Multilevel and weighted reduced basis
              |     method for stochastic optimal control problems constrained by Stokes equations. Numerische
              |     Mathematik, pages 1‚Äì36, 2013.
blank         | 
ref           | [45] Jintai Chung and GM Hulbert. A time integration algorithm for structural dynamics with
              |     improved numerical dissipation: the generalized-Œ± method. Journal of applied mechanics,
              |     60(2):371‚Äì375, 1993.
blank         | 
ref           | [46] Bernardo Cockburn and Chi-Wang Shu. Runge‚ÄìKutta discontinuous Galerkin methods for
              |     convection-dominated problems. Journal of Scientific Computing, 16(3):173‚Äì261, 2001.
blank         | 
ref           | [47] Earl A Coddington and Norman Levinson. Theory of Ordinary Differential Equations. Tata
              |     McGraw-Hill Education, 1955.
blank         | 
ref           | [48] Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust Region Methods, volume 1. SIAM,
              |     2000.
blank         | 
ref           | [49] Arnaud Debussche, Marco Fuhrman, and Gianmario Tessitore. Optimal control of a stochastic
              |     heat equation with boundary-noise and boundary-control. ESAIM: Control, Optimisation and
              |     Calculus of Variations, 13(01):178‚Äì205, 2007.
blank         | 
ref           | [50] Jean-Antoine DeÃÅsideÃÅri and Ales Janka. Multilevel shape parameterization for aerodynamic
              |     optimization: Application to drag and noise reduction of transonic/supersonic business jet.
              |     In Proceedings of the European Congress on Computational Methods in Applied Sciences and
              |     Engineering, ECCOMAS 2004, 2004.
blank         | 
ref           | [51] Benoƒ±ÃÇt Desjardins, Emmanuel Grenier, P-L Lions, and Nader Masmoudi. Incompressible limit
              |     for solutions of the isentropic Navier‚ÄìStokes equations with dirichlet boundary conditions.
              |     Journal de MatheÃÅmatiques Pures et AppliqueÃÅes, 78(5):461‚Äì471, 1999.
meta          | BIBLIOGRAPHY                                                                                   305
blank         | 
              | 
              | 
ref           | [52] Markus A Dihlmann and Bernard Haasdonk. Certified PDE-constrained parameter optimiza-
              |     tion using reduced basis surrogate models for evolution problems. Computational Optimization
              |     and Applications, 60(3):753‚Äì787, 2015.
blank         | 
ref           | [53] Eusebius Doedel, Herbert B Keller, and Jean Pierre Kernevez. Numerical analysis and con-
              |     trol of bifurcation problems (II): Bifurcation in infinite dimensions. International Journal of
              |     Bifurcation and Chaos, 1(04):745‚Äì772, 1991.
blank         | 
ref           | [54] Arne Drud. CONOPT: A GRG code for large sparse dynamic nonlinear optimization problems.
              |     Mathematical Programming, 31(2):153‚Äì191, 1985.
blank         | 
ref           | [55] Thomas D Economon, Francisco Palacios, and Juan J Alonso. Unsteady continuous adjoint
              |     approach for aerodynamic design on dynamic meshes. AIAA Journal, 53(9):2437‚Äì2453, 2015.
blank         | 
ref           | [56] R. Everson and L. Sirovich. Karhunen‚ÄìLoeÃÄve procedure for gappy data. JOSA A, 12(8):1657‚Äì
              |     1664, 1995.
blank         | 
ref           | [57] Marco Fahl and Ekkehard W Sachs. Reduced order modelling approaches to PDE-constrained
              |     optimization based on proper orthogonal decomposition. In Large-scale PDE-constrained op-
              |     timization, pages 268‚Äì280. Springer, 2003.
blank         | 
ref           | [58] C. Farhat, C. Degand, B. Koobus, and M. Lesoinne. Torsional springs for two-dimensional
              |     dynamic unstructured fluid meshes. Computer Methods in Applied Mechanics and Engineering,
              |     163(1‚Äì4):231‚Äì245, 1998.
blank         | 
ref           | [59] Charbel Farhat, Philip Avery, Todd Chapman, and Julien Cortial. Dimensional reduction of
              |     nonlinear finite element dynamic models with finite rotations and energy-based mesh sampling
              |     and weighting for computational efficiency. International Journal for Numerical Methods in
              |     Engineering, 98(9):625‚Äì662, 2014.
blank         | 
ref           | [60] Charbel Farhat, Philippe Geuzaine, and CeÃÅline Grandmont. The discrete geometric conser-
              |     vation law and the nonlinear stability of ALE schemes for the solution of flow problems on
              |     moving grids. Journal of Computational Physics, 174(2):669‚Äì694, 2001.
blank         | 
ref           | [61] Gerald Farin. Curves and Surfaces for Computer-Aided Geometric Design: A Practical Guide.
              |     Elsevier, 2014.
blank         | 
ref           | [62] Alexander IJ Forrester, Neil W Bressloff, and Andy J Keane. Optimization using surrogate
              |     models and partially converged computational fluid dynamics simulations. In Proceedings of
              |     the Royal Society of London A: Mathematical, Physical and Engineering Sciences, volume 462,
              |     pages 2177‚Äì2204. The Royal Society, 2006.
blank         | 
ref           | [63] Alexander IJ Forrester and Andy J Keane. Recent advances in surrogate-based optimization.
              |     Progress in Aerospace Sciences, 45(1):50‚Äì79, 2009.
meta          | BIBLIOGRAPHY                                                                                  306
blank         | 
              | 
              | 
ref           | [64] Bradley M Froehle. High-order discontinuous Galerkin fluid-structure interaction methods.
              |     PhD thesis, University of California, Berkeley, 2013.
blank         | 
ref           | [65] Michel GeÃÅradin and Daniel J Rixen. Mechanical Vibrations: Rheory and Application to Struc-
              |     tural Dynamics. John Wiley & Sons, 2014.
blank         | 
ref           | [66] Thomas Gerstner and Michael Griebel. Numerical integration using sparse grids. Numerical
              |     Algorithms, 18(3-4):209‚Äì232, 1998.
blank         | 
ref           | [67] Thomas Gerstner and Michael Griebel. Dimension‚Äìadaptive tensor‚Äìproduct quadrature. Com-
              |     puting, 71(1):65‚Äì87, 2003.
blank         | 
ref           | [68] P. Geuzaine, G. Brown, C. Harris, and C. Farhat. Aeroelastic dynamic analysis of a full F-16
              |     configuration for various flight conditions. AIAA Journal, 41:363‚Äì371, 2003.
blank         | 
ref           | [69] Omar Ghattas and Jai-Hyeong Bark. Optimal control of two-and three-dimensional incom-
              |     pressible Navier‚ÄìStokes flows. Journal of Computational Physics, 136(2):231‚Äì244, 1997.
blank         | 
ref           | [70] Philip E Gill, Walter Murray, and Michael A Saunders. SNOPT: An SQP algorithm for
              |     large-scale constrained optimization. SIAM Review, 47(1):99‚Äì131, 2005.
blank         | 
ref           | [71] Philip E Gill, Walter Murray, and Margaret H Wright. Practical optimization. 1981.
blank         | 
ref           | [72] Victor Giurgiutiu and Adrian Cuc. Embedded non-destructive evaluation for structural health
              |     monitoring, damage detection, and failure prevention. Shock and Vibration Digest, 37(2):83,
              |     2005.
blank         | 
ref           | [73] Tuhfe GoÃàcÃßmen and Barƒ±sÃß OÃàzerdem. Airfoil optimization for noise emission problem and aero-
              |     dynamic performance criterion on small scale wind turbines. Energy, 46(1):62‚Äì71, 2012.
blank         | 
ref           | [74] Jedidiah Gohlke. Reduced Order Modeling for Optimization of Large Scale Dynamical Systems.
              |     PhD thesis, Rice University, 2013.
blank         | 
ref           | [75] Gene H Golub and Charles F Van Loan. Matrix Computations, volume 3. JHU Press, 2012.
blank         | 
ref           | [76] Willy JF Govaerts. Numerical Methods for Bifurcations of Dynamical Equilibria, volume 66.
              |     Siam, 2000.
blank         | 
ref           | [77] Sanjay Govindjee, Trevor Potter, and Jon Wilkening. Cyclic steady states of treaded rolling
              |     bodies. International Journal for Numerical Methods in Engineering, 99(3):203‚Äì220, 2014.
blank         | 
ref           | [78] Max D Gunzburger. Perspectives in Flow Control and Optimization, volume 5. SIAM, 2003.
blank         | 
ref           | [79] Martin H Gutknecht. Block Krylov space methods for linear systems with multiple right-hand
              |     sides: an introduction. 2006.
blank         | 
ref           | [80] Raphael T Haftka and Z Mroz. First-and second-order sensitivity analysis of linear and non-
              |     linear structures. AIAA Journal, 24(7):1187‚Äì1192, 1986.
meta          | BIBLIOGRAPHY                                                                                   307
blank         | 
              | 
              | 
ref           | [81] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness:
              |     Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
              |     53(2):217‚Äì288, 2011.
blank         | 
ref           | [82] Kathryn Harriman, DJ Gavaghan, and Endre Suli. The importance of adjoint consistency in
              |     the approximation of linear functionals using the discontinuous Galerkin finite element method.
              |     Technical report, 2004.
blank         | 
ref           | [83] Kathryn Harriman, Paul Houston, Bill Senior, and Endre Suli. hp-version discontinuous
              |     Galerkin methods with interior penalty for partial differential equations with nonnegative
              |     characteristic form. Technical report, 2002.
blank         | 
ref           | [84] Ralf Hartmann. Adjoint consistency analysis of discontinuous Galerkin discretizations. SIAM
              |     Journal on Numerical Analysis, 45(6):2671‚Äì2696, 2007.
blank         | 
ref           | [85] Alexander Hay, Imran Akhtar, and Jeff T Borggaard. On the use of sensitivity analysis
              |     in model reduction to predict flows for varying inflow conditions. International Journal for
              |     Numerical Methods in Fluids, 68(1):122‚Äì134, 2012.
blank         | 
ref           | [86] Alexander Hay, Jeff Borggaard, Imran Akhtar, and Dominique Pelletier. Reduced-order models
              |     for parameter dependent geometries based on shape sensitivity analysis. Journal of Compu-
              |     tational Physics, 229(4):1327‚Äì1352, 2010.
blank         | 
ref           | [87] Alexander Hay, Jeffrey T Borggaard, and Dominique Pelletier. Local improvements to reduced-
              |     order models using sensitivity analysis of the proper orthogonal decomposition. Journal of
              |     Fluid Mechanics, 629:41‚Äì72, 2009.
blank         | 
ref           | [88] Beichang He, Omar Ghattas, and James F Antaki. Computational strategies for shape opti-
              |     mization of time‚Äìdependent Navier‚ÄìStokes flows. Technical report, Carnegie Mellon University,
              |     1997.
blank         | 
ref           | [89] J He and LJ Durlofsky. Constraint reduction procedures for reduced-order subsurface flow
              |     models based on pod‚Äìtpwl. International Journal for Numerical Methods in Engineering,
              |     103(1):1‚Äì30, 2015.
blank         | 
ref           | [90] Patrick Heimbach, Chris Hill, and Ralf Giering. An efficient exact adjoint of the parallel
              |     MIT general circulation model, generated via automatic differentiation. Future Generation
              |     Computer Systems, 21(8):1356‚Äì1371, 2005.
blank         | 
ref           | [91] Matthias Heinkenschloss. Formulation and analysis of a sequential quadratic programming
              |     method for the optimal dirichlet boundary control of Navier-Stokes flow. In Optimal Control,
              |     pages 178‚Äì203. Springer, 1998.
blank         | 
ref           | [92] Matthias Heinkenschloss and Denis Ridzal. A matrix-free trust-region SQP method for equality
              |     constrained optimization. SIAM Journal on Optimization, 24(3):1507‚Äì1541, 2014.
meta          | BIBLIOGRAPHY                                                                                  308
blank         | 
              | 
              | 
ref           |  [93] Matthias Heinkenschloss and Luis N Vicente. Analysis of inexact trust-region SQP algorithms.
              |      SIAM Journal on Optimization, 12(2):283‚Äì302, 2002.
blank         | 
ref           |  [94] William S Hemp. Optimum Structures. Clarendon Press, 1973.
blank         | 
ref           |  [95] Vincent Heuveline and Andrea Walther. Online checkpointing for parallel adjoint computation
              |      in PDEs: Application to goal-oriented adaptivity and flow control. In Euro-Par 2006 Parallel
              |      Processing, pages 689‚Äì699. Springer, 2006.
blank         | 
ref           |  [96] Michael Hinze, ReneÃÅ Pinnau, Michael Ulbrich, and Stefan Ulbrich. Optimization with PDE
              |      Constraints, volume 23. Springer Science & Business Media, 2008.
blank         | 
ref           |  [97] Paul Houston and Endre SuÃàli. hp-adaptive discontinuous galerkin finite element methods
              |      for first-order hyperbolic problems. SIAM Journal on Scientific Computing, 23(4):1226‚Äì1252,
              |      2001.
blank         | 
ref           |  [98] Sergio R Idelsohn and Alberto Cardona. A reduction method for nonlinear structural dynamic
              |      analysis. Computer Methods in Applied Mechanics and Engineering, 49(3):253‚Äì279, 1985.
blank         | 
ref           |  [99] M Hasan Imam. Three-dimensional shape optimization. International Journal for Numerical
              |      Methods in Engineering, 18(5):661‚Äì673, 1982.
blank         | 
ref           | [100] Antony Jameson. Aerodynamic design via control theory. Journal of Scientific Computing,
              |      3(3):233‚Äì260, 1988.
blank         | 
ref           | [101] Ian Jolliffe. Principal Component Analysis. Wiley Online Library, 2002.
blank         | 
ref           | [102] Martin Jones and Nail K Yamaleev. Adjoint based shape and kinematics optimization of
              |      flapping wing propulsive efficiency. In 43rd AIAA Fluid Dynamics Conference. San Diego,
              |      CA, 2013.
blank         | 
ref           | [103] HB KELLER. Numerical Methods in Bifurcation Problems. Springer-Verlag, 1987.
blank         | 
ref           | [104] CT Kelley and David E Keyes. Convergence analysis of pseudo-transient continuation. SIAM
              |      Journal on Numerical Analysis, 35(2):508‚Äì523, 1998.
blank         | 
ref           | [105] CT Kelley, Li-Zhi Liao, Liqun Qi, Moody T Chu, JP Reese, and C Winton. Projected pseudo-
              |      transient continuation. 2007.
blank         | 
ref           | [106] Dana A Knoll and David E Keyes. Jacobian-free Newton‚ÄìKrylov methods: a survey of ap-
              |      proaches and applications. Journal of Computational Physics, 193(2):357‚Äì397, 2004.
blank         | 
ref           | [107] Drew P Kouri. An approach for the adaptive solution of optimization problems governed by
              |      partial differential equations with uncertain coefficients. Technical report, DTIC Document,
              |      2012.
meta          | BIBLIOGRAPHY                                                                                  309
blank         | 
              | 
              | 
ref           | [108] Drew P Kouri, Matthias Heinkenschloss, Denis Ridzal, and Bart G van Bloemen Waanders.
              |      A trust-region algorithm with adaptive stochastic collocation for PDE optimization under
              |      uncertainty. SIAM Journal on Scientific Computing, 35(4):A1847‚ÄìA1879, 2013.
blank         | 
ref           | [109] Drew P Kouri, Matthias Heinkenschloss, Denis Ridzal, and Bart G van Bloemen Waanders.
              |      Inexact objective function evaluations in a trust-region algorithm for PDE-constrained op-
              |      timization under uncertainty. SIAM Journal on Scientific Computing, 36(6):A3011‚ÄìA3029,
              |      2014.
blank         | 
ref           | [110] Drew Philip Kouri and Thomas M Surowiec. Risk-averse PDE-constrained optimization us-
              |      ing the conditional value-at-risk. Technical report, Sandia National Laboratories (SNL-NM),
              |      Albuquerque, NM (United States), 2014.
blank         | 
ref           | [111] J-P Kruth, Ming-Chuan Leu, and T Nakagawa. Progress in additive manufacturing and rapid
              |      prototyping. CIRP Annals-Manufacturing Technology, 47(2):525‚Äì540, 1998.
blank         | 
ref           | [112] Peter A Kuchment. Floquet Theory for Partial Differential Equations, volume 60. BirkhaÃàuser,
              |      2012.
blank         | 
ref           | [113] Karl Kunisch and Stefan Volkwein. Proper orthogonal decomposition for optimality systems.
              |      ESAIM: Mathematical Modelling and Numerical Analysis, 42(01):1‚Äì23, 2008.
blank         | 
ref           | [114] Toni Lassila and Gianluigi Rozza. Parametric free-form shape design with PDE models and re-
              |      duced basis method. Computer Methods in Applied Mechanics and Engineering, 199(23):1583‚Äì
              |      1592, 2010.
blank         | 
ref           | [115] Patrick Allen LeGresley. Application of proper orthogonal decomposition (POD) to design
              |      decomposition methods. PhD thesis, Citeseer, 2005.
blank         | 
ref           | [116] Friedemann Leibfritz and Ekkehard W Sachs. Inexact SQP interior point methods and large
              |      scale optimal control problems. SIAM Journal on Control and Optimization, 38(1):272‚Äì293,
              |      1999.
blank         | 
ref           | [117] Chad Lieberman, Karen Willcox, and Omar Ghattas. Parameter and state model reduction for
              |      large-scale statistical inverse problems. SIAM Journal on Scientific Computing, 32(5):2523‚Äì
              |      2542, 2010.
blank         | 
ref           | [118] Chi-Kun Lin. On the incompressible limit of the compressible Navier-Stokes equations. Com-
              |      munications in Partial Differential Equations, 20(3-4):677‚Äì707, 1995.
blank         | 
ref           | [119] Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale opti-
              |      mization. Mathematical Programming, 45(1-3):503‚Äì528, 1989.
blank         | 
ref           | [120] Trent Lukaczyk, Francisco Palacios, Juan J Alonso, and P Constantine. Active subspaces for
              |      shape optimization. In Proceedings of the 10th AIAA Multidisciplinary Design Optimization
              |      Conference, pages 1‚Äì18, 2014.
meta          | BIBLIOGRAPHY                                                                                  310
blank         | 
              | 
              | 
ref           | [121] L Machiels, Y Maday, and AT Patera. Output bounds for reduced-order approximations of el-
              |      liptic partial differential equations. Computer Methods in Applied Mechanics and Engineering,
              |      190(26):3413‚Äì3426, 2001.
blank         | 
ref           | [122] Yvon Maday and Einar M R√∏nquist. A reduced-basis element method. Journal of scientific
              |      computing, 17(1-4):447‚Äì459, 2002.
blank         | 
ref           | [123] Charles A Mader, JR RA Martins, Juan J Alonso, and E Van Der Weide. ADjoint: An
              |      approach for the rapid development of discrete adjoint solvers. AIAA Journal, 46(4):863‚Äì873,
              |      2008.
blank         | 
ref           | [124] Karthik Mani and Dimitri J Mavriplis.      Unsteady discrete adjoint formulation for two-
              |      dimensional flow problems with deforming meshes. AIAA Journal, 46(6):1351‚Äì1364, 2008.
blank         | 
ref           | [125] Andrea Manzoni, Alfio Quarteroni, and Gianluigi Rozza. Shape optimization for viscous flows
              |      by reduced basis methods and free-form deformation. International Journal for Numerical
              |      Methods in Fluids, 70(5):646‚Äì670, 2012.
blank         | 
ref           | [126] V Maple. Waterloo MAPLE software. University of Waterloo, Version, 5, 1994.
blank         | 
ref           | [127] K Maute, M Nikbay, and C Farhat. Sensitivity analysis and design optimization of three-
              |      dimensional non-linear aeroelastic systems by the adjoint method. International Journal for
              |      Numerical Methods in Engineering, 56(6):911‚Äì933, 2003.
blank         | 
ref           | [128] K Maute and M Raulli. FEM‚Äîoptimization module and SDESIGN user guides, 2006.
blank         | 
ref           | [129] Kurt Maute, Melike Nikbay, and Charbel Farhat. Coupled analytical sensitivity analysis and
              |      optimization of three-dimensional nonlinear aeroelastic systems. AIAA Journal, 39(11):2051‚Äì
              |      2061, 2001.
blank         | 
ref           | [130] Dimitri J Mavriplis. Discrete adjoint-based approach for optimization problems on three-
              |      dimensional unstructured meshes. AIAA Journal, 45(4):741‚Äì750, 2007.
blank         | 
ref           | [131] GN Mercer and AJ Roberts. Standing waves in deep water: Their stability and extreme form.
              |      Physics of Fluids A: Fluid Dynamics (1989-1993), 4(2):259‚Äì269, 1992.
blank         | 
ref           | [132] Asitav Mishra, Karthik Mani, Dimitri Mavriplis, and Jay Sitaraman. Time dependent adjoint-
              |      based optimization for coupled fluid‚Äìstructure problems. Journal of Computational Physics,
              |      292:253‚Äì271, 2015.
blank         | 
ref           | [133] Jorge J MoreÃÅ. Recent developments in algorithms and software for trust region methods.
              |      Springer, 1983.
blank         | 
ref           | [134] Matthias Morzfeld, Xuemin Tu, Jon Wilkening, and Alexandre Chorin. Parameter estimation
              |      by implicit sampling. Communications in Applied Mathematics and Computational Science,
              |      10(2):205‚Äì225, 2015.
meta          | BIBLIOGRAPHY                                                                                     311
blank         | 
              | 
              | 
ref           | [135] Siva Nadarajah and Antony Jameson. A comparison of the continuous and discrete adjoint
              |      approach to automatic aerodynamic optimization. AIAA Paper, 667:2000, 2000.
blank         | 
ref           | [136] Siva K Nadarajah and Antony Jameson. Optimum shape design for unsteady flows with
              |      time-accurate continuous and discrete adjoint method. AIAA Journal, 45(7):1478‚Äì1491, 2007.
blank         | 
ref           | [137] Guy Narkiss and Michael Zibulevsky. Sequential Subspace Optimization Method for Large-Scale
              |      Unconstrained Problems. Technion-IIT, Department of Electrical Engineering, 2005.
blank         | 
ref           | [138] James C Newman III, Arthur C Taylor III, Richard W Barnwell, Perry A Newman, and Gene
              |      J-W Hou. Overview of sensitivity analysis and shape optimization for complex aerodynamic
              |      configurations. Journal of Aircraft, 36(1):87‚Äì96, 1999.
blank         | 
ref           | [139] Nathan Mortimore Newmark. A method of computation for structural dynamics. In Proc.
              |      ASCE, volume 85, pages 67‚Äì94, 1959.
blank         | 
ref           | [140] Eric J Nielsen, Boris Diskin, and Nail K Yamaleev. Discrete adjoint-based design optimization
              |      of unsteady turbulent flows on dynamic unstructured grids. AIAA Journal, 48(6):1195‚Äì1206,
              |      2010.
blank         | 
ref           | [141] Fabio Nobile, Raul Tempone, and CG Webster. The analysis of a sparse grid stochastic
              |      collocation method for partial differential equations with high-dimensional random input data.
              |      Technical report, Technical report, Sandia National Laboratories, 2007. SAND REPORT,
              |      2007.
blank         | 
ref           | [142] Fabio Nobile, RauÃÅl Tempone, and Clayton G Webster. A sparse grid stochastic collocation
              |      method for partial differential equations with random input data. SIAM Journal on Numerical
              |      Analysis, 46(5):2309‚Äì2345, 2008.
blank         | 
ref           | [143] Jorge Nocedal and Stephen Wright. Numerical Optimization. Springer Science & Business
              |      Media, 2006.
blank         | 
ref           | [144] Erich Novak and Klaus Ritter. High dimensional integration of smooth functions over cubes.
              |      Numerische Mathematik, 75(1):79‚Äì97, 1996.
blank         | 
ref           | [145] Erich Novak and Klaus Ritter. Simple cubature formulas with high polynomial exactness.
              |      Constructive Approximation, 15(4):499‚Äì522, 1999.
blank         | 
ref           | [146] Erich Novak and Henryk WozÃÅniakowski. Tractability of Multivariate Problems: Standard in-
              |      formation for Functionals, volume 12. European Mathematical Society, 2010.
blank         | 
ref           | [147] Carlos E Orozco and ON Ghattas. Massively parallel aerodynamic shape optimization. Com-
              |      puting Systems in Engineering, 3(1):311‚Äì320, 1992.
blank         | 
ref           | [148] Akira Oyama, Yoshiyuki Okabe, Koji Shimoyama, and Kozo Fujii. Aerodynamic multiobjec-
              |      tive design exploration of a flapping airfoil using a Navier-Stokes solver. Journal of Aerospace
              |      Computing, Information, and Communication, 6(3):256‚Äì270, 2009.
meta          | BIBLIOGRAPHY                                                                                  312
blank         | 
              | 
              | 
ref           | [149] Anthony T Patera and Gianluigi Rozza. Reduced basis approximation and a posteriori error
              |      estimation for parametrized partial differential equations. Technical report, (C) MIT, Mas-
              |      sachusetts Institute of Technology, 2007.
blank         | 
ref           | [150] Jaime Peraire and P-O Persson. The Compact Discontinuous Galerkin (CDG) method for
              |      elliptic problems. SIAM Journal on Scientific Computing, 30(4):1806‚Äì1824, 2008.
blank         | 
ref           | [151] Ruben E Perez, Peter W Jansen, and Joaquim RRA Martins. pyOpt: a Python-based object-
              |      oriented framework for nonlinear constrained optimization. Structural and Multidisciplinary
              |      Optimization, 45(1):101‚Äì118, 2012.
blank         | 
ref           | [152] P-O Persson, J Bonet, and J Peraire. Discontinuous Galerkin solution of the Navier‚ÄìStokes
              |      equations on deformable domains. Computer Methods in Applied Mechanics and Engineering,
              |      198(17):1585‚Äì1595, 2009.
blank         | 
ref           | [153] P-O Persson and Jaime Peraire. Newton-GMRES preconditioning for discontinuous Galerkin
              |      discretizations of the Navier-Stokes equations.    SIAM Journal on Scientific Computing,
              |      30(6):2709‚Äì2733, 2008.
blank         | 
ref           | [154] Per-Olof Persson. Scalable parallel Newton-Krylov solvers for discontinuous Galerkin dis-
              |      cretizations. AIAA Paper, 606:2009, 2009.
blank         | 
ref           | [155] Per-Olof Persson and Jaime Peraire. Curved mesh generation and mesh refinement using
              |      Lagrangian solid mechanics. In Proceedings of the 47th AIAA Aerospace Sciences Meeting and
              |      Exhibit, volume 204, 2009.
blank         | 
ref           | [156] Knut Petras. On the smolyak cubature error for analytic functions. Advances in Computational
              |      Mathematics, 12(1):71‚Äì93, 2000.
blank         | 
ref           | [157] Knut Petras. Smolyak cubature of given polynomial degree with few nodes for increasing
              |      dimension. Numerische Mathematik, 93(4):729‚Äì753, 2003.
blank         | 
ref           | [158] Max F Platzer, Kevin D Jones, John Young, and JC S. Lai. Flapping wing aerodynamics:
              |      progress and challenges. AIAA Journal, 46(9):2136‚Äì2149, 2008.
blank         | 
ref           | [159] MJD Powell. Convergence properties of a class of minimization algorithms. Nonlinear Pro-
              |      gramming, 2(0):1‚Äì27, 1975.
blank         | 
ref           | [160] Alfio Quarteroni and Gianluigi Rozza. Optimal control and shape optimization of aorto-
              |      coronaric bypass anastomoses.     Mathematical Models and Methods in Applied Sciences,
              |      13(12):1801‚Äì1823, 2003.
blank         | 
ref           | [161] Louis B Rall. Automatic differentiation: Techniques and applications. 1981.
blank         | 
ref           | [162] Ravi Ramamurti and William Sandberg. Simulation of flow about flapping airfoils using finite
              |      element incompressible flow solver. AIAA Journal, 39(2):253‚Äì260, 2001.
meta          | BIBLIOGRAPHY                                                                                       313
blank         | 
              | 
              | 
ref           | [163] James Reuther, Juan Jose Alonso, Mark J Rimlinger, and Antony Jameson. Aerodynamic
              |      shape optimization of supersonic aircraft configurations via an adjoint formulation on dis-
              |      tributed memory parallel computers. Computers & Fluids, 28(4):675‚Äì700, 1999.
blank         | 
ref           | [164] James Reuther, Antony Jameson, James Farmer, Luigi Martinelli, and David Saunders. Aero-
              |      dynamic shape optimization of complex aircraft configurations via an adjoint formulation.
              |      AIAA paper, 94, 1996.
blank         | 
ref           | [165] Michal Rewienski and Jacob White. A trajectory piecewise-linear approach to model order
              |      reduction and fast simulation of nonlinear circuits and micromachined devices. IEEE Trans-
              |      actions on computer-aided design of integrated circuits and systems, 22(2):155‚Äì170, 2003.
blank         | 
ref           | [166] Denis Ridzal. Trust-region SQP methods with inexact linear system solves for large-scale
              |      optimization. PhD thesis, Citeseer, 2006.
blank         | 
ref           | [167] TD Robinson, MS Eldred, KE Willcox, and R Haimes. Surrogate-based optimization us-
              |      ing multifidelity models with variable parameterization and corrected space mapping. AIAA
              |      Journal, 46(11):2814‚Äì2822, 2008.
blank         | 
ref           | [168] Theresa Dawn Robinson. Surrogate-based optimization using multifidelity models with variable
              |      parameterization. PhD thesis, Massachusetts Institute of Technology, 2007.
blank         | 
ref           | [169] Philip L Roe. Approximate Riemann solvers, parameter vectors, and difference schemes.
              |      Journal of Computational Physics, 43(2):357‚Äì372, 1981.
blank         | 
ref           | [170] Sabrina Rogg. Trust Region POD for Optimal Boundary Control of a Semilinear Heat Equa-
              |      tion. PhD thesis, University of Trier, 2014.
blank         | 
ref           | [171] Gianluigi Rozza. On optimization, control and shape design of an arterial bypass. International
              |      Journal for Numerical Methods in Fluids, 47(10-11):1411‚Äì1419, 2005.
blank         | 
ref           | [172] Gianluigi Rozza. Shape design by optimal flow control and reduced basis techniques. PhD
              |      thesis, EPFL, 2005.
blank         | 
ref           | [173] Gianluigi Rozza, DBP Huynh, and Anthony T Patera. Reduced basis approximation and a
              |      posteriori error estimation for affinely parametrized elliptic coercive partial differential equa-
              |      tions. Archives of Computational Methods in Engineering, 15(3):229‚Äì275, 2008.
blank         | 
ref           | [174] Gianluigi Rozza and Andrea Manzoni. Model order reduction by geometrical parametrization
              |      for shape optimization in computational fluid dynamics. In Proceedings of the ECCOMAS
              |      CFD 2010, V European Conference on Computational Fluid Dynamics, number EPFL-CONF-
              |      148535, 2010.
blank         | 
ref           | [175] D. Ryckelynck. A priori hyperreduction method: an adaptive approach. Journal of Computa-
              |      tional Physics, 202(1):346‚Äì366, 2005.
meta          | BIBLIOGRAPHY                                                                                    314
blank         | 
              | 
              | 
ref           | [176] Chris H Rycroft and Jon Wilkening. Computation of three-dimensional standing water waves.
              |      Journal of Computational Physics, 255:612‚Äì638, 2013.
blank         | 
ref           | [177] Jamshid A Samareh. A survey of shape parameterization techniques. In NASA Conference
              |      Publication, pages 333‚Äì344. Citeseer, 1999.
blank         | 
ref           | [178] Claudia Schillings. Optimal aerodynamic design under uncertainties. PhD thesis, PhD thesis,
              |      Fb‚ÄìIV, Mathematik, UniversitaÃàt Trier, D‚Äì54286 Trier, Germany, 2010.
blank         | 
ref           | [179] Thomas W Sederberg and Scott R Parry. Free-form deformation of solid geometric models.
              |      ACM SIGGRAPH computer graphics, 20(4):151‚Äì160, 1986.
blank         | 
ref           | [180] Ole Sigmund. Design of multiphysics actuators using topology optimization‚Äìpart I: One-
              |      material structures. Computer Methods in Applied Mechanics and Engineering, 190(49):6577‚Äì
              |      6604, 2001.
blank         | 
ref           | [181] Ole Sigmund and Kurt Maute. Topology optimization approaches. Structural and Multidisci-
              |      plinary Optimization, 48(6):1031‚Äì1055, 2013.
blank         | 
ref           | [182] Valeria Simoncini and Efstratios Gallopoulos. An iterative method for nonsymmetric systems
              |      with multiple right-hand sides. SIAM Journal on Scientific Computing, 16(4):917‚Äì933, 1995.
blank         | 
ref           | [183] L. Sirovich. Turbulence and the dynamics of coherent structures. I-coherent structures. II-
              |      symmetries and transformations. III-dynamics and scaling. Quarterly of Applied Mathematics,
              |      45:561‚Äì571, 1987.
blank         | 
ref           | [184] Sergey A Smolyak. Quadrature and interpolation formulas for tensor products of certain classes
              |      of functions. In Dokl. Akad. Nauk SSSR, volume 4, page 123, 1963.
blank         | 
ref           | [185] Roman Srzednicki.     Periodic and bounded solutions in blocks for time-periodic nonau-
              |      tonomous ordinary differential equations. Nonlinear Analysis: Theory, Methods & Appli-
              |      cations, 22(6):707‚Äì737, 1994.
blank         | 
ref           | [186] Eka Suwartadi, Stein Krogstad, and Bjarne Foss. Adjoint-based surrogate optimization of oil
              |      reservoir water flooding. Optimization and Engineering, 16(2):441‚Äì481, 2015.
blank         | 
ref           | [187] Jeffrey P Thomas, Kenneth C Hall, and Earl H Dowell. Discrete adjoint approach for modeling
              |      unsteady aerodynamic design sensitivities. AIAA Journal, 43(9):1931‚Äì1936, 2005.
blank         | 
ref           | [188] Hanne Tiesler, Robert M Kirby, Dongbin Xiu, and Tobias Preusser. Stochastic collocation
              |      for optimal control problems with stochastic PDE constraints. SIAM Journal on Control and
              |      Optimization, 50(5):2659‚Äì2682, 2012.
blank         | 
ref           | [189] Ph L Toint. Global convergence of a class of trust-region methods for nonconvex minimization
              |      in Hilbert space. IMA Journal of Numerical Analysis, 8(2):231‚Äì252, 1988.
meta          | BIBLIOGRAPHY                                                                                     315
blank         | 
              | 
              | 
ref           | [190] Fredi TroÃàltzsch. Optimal control of partial differential equations. Graduate studies in mathe-
              |      matics, 112, 2010.
blank         | 
ref           | [191] Ismail H Tuncer and Mustafa Kaya. Optimization of flapping airfoils for maximum thrust and
              |      propulsive efficiency. AIAA Journal, 43(11):2329‚Äì2336, 2005.
blank         | 
ref           | [192] Nico P van Dijk, K Maute, M Langelaar, and F Van Keulen. Level-set methods for structural
              |      topology optimization: a review. Structural and Multidisciplinary Optimization, 48(3):437‚Äì
              |      472, 2013.
blank         | 
ref           | [193] Marnix P van Schrojenstein Lantman and Krzysztof Fidkowski. Adjoint-based optimization of
              |      flapping kinematics in viscous flows. In 21st AIAA Computaional Fluid Dynamics Conference,
              |      2013.
blank         | 
ref           | [194] Stefan Vandewalle and Robert Piessens.       Efficient parallel algorithms for solving initial-
              |      boundary value and time-periodic parabolic partial differential equations. SIAM Journal on
              |      Scientific and Statistical Computing, 13(6):1330‚Äì1346, 1992.
blank         | 
ref           | [195] Jean Virieux and SteÃÅphane Operto. An overview of full-waveform inversion in exploration
              |      geophysics. Geophysics, 74(6):WCC1‚ÄìWCC26, 2009.
blank         | 
ref           | [196] Divakar Viswanath. Recurrent motions within plane Couette turbulence. Journal of Fluid
              |      Mechanics, 580:339‚Äì358, 2007.
blank         | 
ref           | [197] Zhi Wang, IM Navon, FX Le Dimet, and X Zou. The second order adjoint analysis: theory
              |      and applications. Meteorology and Atmospheric Physics, 50(1-3):3‚Äì20, 1992.
blank         | 
ref           | [198] Kyle Washabaugh. Faster Fidelity For Better Design: A Scalable Model Order Reduction
              |      Framework For Steady Aerodynamic Design Applications. PhD thesis, Stanford University,
              |      2016.
blank         | 
ref           | [199] Clayton G Webster. Sparse grid stochastic collocation techniques for the numerical solution of
              |      partial differential equations with random input data. Flordia State University, 2007.
blank         | 
ref           | [200] Jon Wilkening. Breakdown of self-similarity at the crests of large-amplitude standing water
              |      waves. Physical Review Letters, 107(18):184501, 2011.
blank         | 
ref           | [201] Jon Wilkening and Jia Yu. Overdetermined shooting methods for computing standing water
              |      waves with spectral accuracy. Computational Science & Discovery, 5(1):014017, 2012.
blank         | 
ref           | [202] Matthew O Williams, Jon Wilkening, Eli Shlizerman, and J Nathan Kutz. Continuation of
              |      periodic solutions in the waveguide array mode-locked laser. Physica D: Nonlinear Phenomena,
              |      240(22):1791‚Äì1804, 2011.
blank         | 
ref           | [203] Kaufui V Wong and Aldo Hernandez. A review of additive manufacturing. ISRN Mechanical
              |      Engineering, 2012, 2012.
meta          | BIBLIOGRAPHY                                                                                 316
blank         | 
              | 
              | 
ref           | [204] Dongbin Xiu and Jan S Hesthaven. High-order collocation methods for differential equations
              |      with random inputs. SIAM Journal on Scientific Computing, 27(3):1118‚Äì1139, 2005.
blank         | 
ref           | [205] Nail K Yamaleev, Boris Diskin, and Eric J Nielsen. Adjoint-based methodology for time-
              |      dependent optimization. AIAA Paper, 5857:2008, 2008.
blank         | 
ref           | [206] Nail K Yamaleev, Boris Diskin, and Eric J Nielsen. Local-in-time adjoint-based method for
              |      design optimization of unsteady flows. Journal of Computational Physics, 229(14):5394‚Äì5407,
              |      2010.
blank         | 
ref           | [207] Ya-Xiang Yuan. Subspace methods for large scale nonlinear equations and nonlinear least
              |      squares. Optimization and Engineering, 10(2):207‚Äì218, 2009.
blank         | 
ref           | [208] Yao Yue and Karl Meerbergen. Accelerating optimization of parametric linear systems by
              |      model order reduction. SIAM Journal on Optimization, 23(2):1344‚Äì1370, 2013.
blank         | 
ref           | [209] Matthew J. Zahr, Kevin Carlberg, David Amsallem, and Charbel Farhat. Comparison of
              |      model reduction techniques on high-fidelity linear and nonlinear electrical, mechanical, and
              |      biological systems. Technical report, University of California, Berkeley, 2010.
blank         | 
ref           | [210] Matthew J. Zahr and Charbel Farhat. Progressive construction of a parametric reduced-order
              |      model for PDE-constrained optimization. International Journal for Numerical Methods in
              |      Engineering, 102(5):1111‚Äì1135, 2015.
blank         | 
ref           | [211] Matthew J. Zahr and Per-Olof Persson. An adjoint method for a high-order discretization of
              |      deforming domain conservation laws for optimization of flow problems. Journal of Computa-
              |      tional Physics, In review, 2016.
blank         | 
ref           | [212] Matthew J. Zahr, Per-Olof Persson, and John Wilkening. A fully discrete adjoint method
              |      for optimization of flow problems on deforming domains with time-periodicity constraints.
              |      Computers & Fluids, 2016.
blank         | 
ref           | [213] TomaÃÅs Zegard and Glaucio H Paulino. Bridging topology optimization and additive manufac-
              |      turing. Structural and Multidisciplinary Optimization, pages 1‚Äì18, 2015.
blank         | 
ref           | [214] Kemin Zhou, John Comstock Doyle, Keith Glover, et al. Robust and Optimal Control, vol-
              |      ume 40. Prentice Hall, New Jersey, 1996.
blank         | 
ref           | [215] Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-BFGS-B:
              |      Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on
              |      Mathematical Software (TOMS), 23(4):550‚Äì560, 1997.
blank         | 
ref           | [216] J Carsten Ziems and Stefan Ulbrich. Adaptive multilevel inexact SQP methods for PDE-
              |      constrained optimization. SIAM Journal on Optimization, 21(1):1‚Äì40, 2011.
blank         | 
